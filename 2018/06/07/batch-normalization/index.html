<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 在深度神经网络中，有效的参数初始化和输入特征归一化等方法能够很大程度上避免梯度消失，加速网络的训练过程。但是深度神经网络由很多层网络叠加，而每一层网络的参数更新会导致下一层网络的输入数据的分布发生变化，通过层层叠加，输入的分布变化会非常剧烈，这就使得网络需要不断重新适应不同分布的输入，而批归一化能够很出色地解决隐藏层间输入分布改变问题。"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="批归一化"><meta property="og:url" content="https://pengzhendong.cn/2018/06/07/batch-normalization/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 在深度神经网络中，有效的参数初始化和输入特征归一化等方法能够很大程度上避免梯度消失，加速网络的训练过程。但是深度神经网络由很多层网络叠加，而每一层网络的参数更新会导致下一层网络的输入数据的分布发生变化，通过层层叠加，输入的分布变化会非常剧烈，这就使得网络需要不断重新适应不同分布的输入，而批归一化能够很出色地解决隐藏层间输入分布改变问题。"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2018-06-07T04:34:23.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="批归一化"><meta name="twitter:description" content="前言 在深度神经网络中，有效的参数初始化和输入特征归一化等方法能够很大程度上避免梯度消失，加速网络的训练过程。但是深度神经网络由很多层网络叠加，而每一层网络的参数更新会导致下一层网络的输入数据的分布发生变化，通过层层叠加，输入的分布变化会非常剧烈，这就使得网络需要不断重新适应不同分布的输入，而批归一化能够很出色地解决隐藏层间输入分布改变问题。"><link rel="canonical" href="https://pengzhendong.cn/2018/06/07/batch-normalization/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>批归一化 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/06/07/batch-normalization/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 批归一化</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-06-07 10:22:09 / 修改时间：12:34:23" itemprop="dateCreated datePublished" datetime="2018-06-07T10:22:09+08:00">2018-06-07</time></span><span id="/2018/06/07/batch-normalization/" class="post-meta-item leancloud_visitors" data-flag-title="批归一化" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>在深度神经网络中，有效的参数初始化和输入特征归一化等方法能够很大程度上避免梯度消失，加速网络的训练过程。但是深度神经网络由很多层网络叠加，而每一层网络的参数更新会导致下一层网络的输入数据的分布发生变化，通过层层叠加，输入的分布变化会非常剧烈，这就使得网络需要不断重新适应不同分布的输入，而批归一化能够很出色地解决隐藏层间输入分布改变问题。</p><a id="more"></a><h2 id="批归一化">批归一化</h2><p>批归一化 (Batch normalization) 简称 BN，Google 2015 年提出。批归一化通过对<strong>每一层</strong>的输入进行归一化，将数据预处理的方法引入到每一个隐藏层，保持深度网络各层的输入分布不变。</p><p>在传统机器学习通常有个假设：“源空间 (source domain) 和目标空间 (target domain) 的数据分布 (distribution) 是一致的”，即训练集和测试集满足独立同分布。目的是希望训练集训练的模型可以<strong>合理</strong>用在测试集上，如果不太关心泛化性 (如在线学习算法) 就不需要这个假设。</p><p>迁移学习可以将训练好的模型参数迁移到新的模来帮助模型训练，虽然大部分数据存在相关性，但是它们不是独立同分布的。</p><h3 id="internal-covariate-shift">Internal Covariate Shift</h3><p>Covariate shift 指源空间和目标空间的条件概率一致，对边缘概率不同：</p><blockquote><p><span class="math inline">\(\forall x \in \mathcal{X}, P_s(Y|X=x)=P_t(Y|X=x), P_s(X) \neq P_t(X)\)</span></p></blockquote><p>神经网络的输入经过了一层网络后分布会发生改变，而且差异会随着网络深度增大而增大，但是训练集的真实标签不变，符合 Covariate Shift 的定义，Internal 表示神经网络内部。隐藏层需要重新适应新的分布的输入，因此会降低收敛速度，批归一化通过对每一层网络的输入进行缩放，保证了输入分布的统一。</p><h3 id="前向传播">前向传播</h3><p>在深度学习文献中有一些争论关于应该批归一化 <span class="math inline">\(Z\)</span> 还是 <span class="math inline">\(A\)</span>，吴恩达表示在实践中通常批归一化的是 <span class="math inline">\(Z\)</span>。给定第 <span class="math inline">\(l\)</span> 层隐藏单元的值 <span class="math inline">\(\mathcal{Z}=\lbrace z^{(1)}, z^{(2)}, …, z^{(m)}\rbrace\)</span> (为了方便表示，省略其层数的上标 <span class="math inline">\([l]\)</span>)，对其进行归一化，有： <span class="math display">\[ \mu_\mathcal{Z}=\frac{1}{m}\sum_{i=1}^{m}z^{(i)} \]</span></p><p><span class="math display">\[ \sigma_\mathcal{Z}^2=\frac{1}{m}\sum_{i=1}^{m}(z^{(i)}-\mu_\mathcal{Z})^2 \]</span></p><p><span class="math display">\[ z_{norm}^{(i)}=\frac{z^{(i)}-\mu_\mathcal{Z}}{\sqrt{\sigma_\mathcal{Z}^2+\varepsilon}} \]</span></p><p>归一化后的 <span class="math inline">\(z_{norm}^{(i)}\)</span> 具有 0 均值和标准方差，但是这样会降低模型的灵活度，导致新的分丧失从前层传递过来的特征与知识，对于 Sigmoid 激活函数也无法有效利用其非线性功能，所以需要再次对其进行缩放和平移： <span class="math display">\[ \hat z^{(i)}=\gamma z_{norm}^{(i)}+\beta \]</span> 其中 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 是需要学习的参数，在每个隐藏层中通过 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 可以随意设置 <span class="math inline">\(\hat z^{(i)}\)</span> 的均值和方差。当 <span class="math inline">\(\gamma=\sqrt{\sigma_\mathcal{Z}^2+\varepsilon}\)</span> 和 <span class="math inline">\(\beta=\mu_\mathcal{Z}\)</span> 时，有 <span class="math inline">\(\hat z^{(i)}=z^{(i)}\)</span>。因此在梯度下降算法中，第 <span class="math inline">\(l\)</span> 层神经网络需要更新的参数不止有 <span class="math inline">\(W^{[l]}\)</span> 和 <span class="math inline">\(b^{[l]}\)</span>，还有 <span class="math inline">\(\gamma^{[l]}\)</span> 和 <span class="math inline">\(\beta^{[l]}\)</span>。批归一化有轻微的正则化效果 (类似于 Dropout)，因为使用小批量训练数据给隐藏单元添加了噪声，使得后部的神经元不过分依赖任何一个隐层单元。</p><h3 id="反向传播">反向传播</h3><p>在反向传播计算 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 的梯度的时候，同样使用链式法则求导，对于最后一层神经网络，有： <span class="math display">\[ d\gamma=\frac{\partial{\mathscr{l}}}{\partial{\gamma}}=\frac{\partial{\mathscr{l}}}{\partial{\hat z^{(i)}}}\cdot z_{norm}^{(i)} \]</span></p><p><span class="math display">\[ d\beta=\frac{\partial{\mathscr{l}}}{\partial{\beta}}=\frac{\partial{\mathscr{l}}}{\partial{\hat z^{(i)}}} \]</span></p><p><span class="math display">\[ dz_{norm}^{(i)}=\frac{\partial{\mathscr{l}}}{\partial{z_{norm}^{(i)}}}=\frac{\partial{\mathscr{l}}}{\partial{\hat z^{(i)}}}\cdot \gamma \]</span></p><p><span class="math display">\[ d\sigma_\mathcal{Z}^2=\frac{\partial{\mathscr{l}}}{\partial{\sigma_\mathcal{Z}^2}}=\sum_{i=1}^{m}dz_{norm}^{(i)}\cdot (z^{(i)}-\mu_\mathcal{Z})\cdot \frac{-1}{2}(\sigma_\mathcal{Z}^2+\varepsilon)^{\frac{-3}{2}} \]</span></p><p><span class="math display">\[ d\mu_\mathcal{Z}=\frac{\partial{\mathscr{l}}}{\partial{\mu_\mathcal{Z}}}=\sum_{i=1}^{m}dz_{norm}^{(i)}\cdot \frac{-1}{\sqrt{\sigma_\mathcal{Z}^2+\varepsilon}} \]</span></p><h3 id="测试">测试</h3><p>在训练过程中，使用批归一化将数据以小批量的形式逐一处理，但是在测试的时候，每次测试只有一个数据，计算一个数据的 <span class="math inline">\(\mu_\mathcal{Z}\)</span> 和 <span class="math inline">\(\sigma_\mathcal{Z}^2\)</span> 没有意义，所以需要重新估计 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\sigma^2\)</span>。理论上可以对整个训练集求 <span class="math inline">\(\mu_\mathcal{D}\)</span> 和 <span class="math inline">\(\sigma_\mathcal{D}^2\)</span>，但在实际操作中通常使用指数加权平均来追踪训练过程中看到的所有 <span class="math inline">\(\mu_\mathcal{Z}\)</span> 和 <span class="math inline">\(\sigma_\mathcal{Z}^2\)</span>，在第 <span class="math inline">\(l\)</span> 层隐藏层第 <span class="math inline">\(t\)</span> 个小批量处，有： <span class="math display">\[ \mu_t=\beta_1\mu_{t-1}+(1-\beta_1)\mu^{\lbrace t\rbrace[l]} \]</span></p><p><span class="math display">\[ \sigma_t^2=\beta_2 \sigma_{t-1}^2+(1-\beta_2)\sigma^{2\lbrace t\rbrace[l]} \]</span></p><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li><li>lan Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. 人民邮电出版社. 2017.</li><li><a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="noopener">详解深度学习中的 Normalization，不只是 BN</a></li><li><a href="https://www.quora.com/Why-does-batch-normalization-help" target="_blank" rel="noopener">Why does batch normalization help?</a></li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2018/06/07/batch-normalization/" title="批归一化">https://pengzhendong.cn/2018/06/07/batch-normalization/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/06/06/optimization-algorithms/" rel="prev" title="优化算法"><i class="fa fa-chevron-left"></i> 优化算法</a></div><div class="post-nav-item"> <a href="/2018/06/20/recurrent-neural-network/" rel="next" title="循环神经网络">循环神经网络<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#批归一化"><span class="nav-number">2.</span> <span class="nav-text">批归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#internal-covariate-shift"><span class="nav-number">2.1.</span> <span class="nav-text">Internal Covariate Shift</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播"><span class="nav-number">2.2.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">2.3.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试"><span class="nav-number">2.4.</span> <span class="nav-text">测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">3.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>