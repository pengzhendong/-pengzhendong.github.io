<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 在介绍 RNN 的文章中，重点是学习 RNN 的结构，前向传播和反向传播的大致流程，所以在实现代码中并不是很全面，甚至没有关于损失函数的定义，这个作业基于字符级别，实现了一个语言模型。"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="字符级别的语言模型"><meta property="og:url" content="https://pengzhendong.cn/2018/06/27/character-level-language-model/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 在介绍 RNN 的文章中，重点是学习 RNN 的结构，前向传播和反向传播的大致流程，所以在实现代码中并不是很全面，甚至没有关于损失函数的定义，这个作业基于字符级别，实现了一个语言模型。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://pengzhendong.cn/2018/06/27/character-level-language-model/rnn.png"><meta property="og:image" content="https://pengzhendong.cn/2018/06/27/character-level-language-model/clip.png"><meta property="og:image" content="https://pengzhendong.cn/2018/06/27/character-level-language-model/dinos3.png"><meta property="og:updated_time" content="2018-06-27T03:32:46.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="字符级别的语言模型"><meta name="twitter:description" content="前言 在介绍 RNN 的文章中，重点是学习 RNN 的结构，前向传播和反向传播的大致流程，所以在实现代码中并不是很全面，甚至没有关于损失函数的定义，这个作业基于字符级别，实现了一个语言模型。"><meta name="twitter:image" content="https://pengzhendong.cn/2018/06/27/character-level-language-model/rnn.png"><link rel="canonical" href="https://pengzhendong.cn/2018/06/27/character-level-language-model/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>字符级别的语言模型 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/06/27/character-level-language-model/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 字符级别的语言模型</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-06-27 09:10:57 / 修改时间：11:32:46" itemprop="dateCreated datePublished" datetime="2018-06-27T09:10:57+08:00">2018-06-27</time></span><span id="/2018/06/27/character-level-language-model/" class="post-meta-item leancloud_visitors" data-flag-title="字符级别的语言模型" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>在介绍 RNN 的文章中，重点是学习 RNN 的结构，前向传播和反向传播的大致流程，所以在实现代码中并不是很全面，甚至没有关于损失函数的定义，这个作业基于字符级别，实现了一个语言模型。</p><a id="more"></a><p>作业背景：根据已有的<a href="https://nbviewer.jupyter.org/github/pengzhendong/DeepLearning/blob/master/5.%20Sequence%20Models/Week%201/Dinosaurus%20Island%20-%20Character%20level%20language%20model/dinos.txt" target="_blank" rel="noopener">恐龙的名字</a>，生成一些相似风格的恐龙名字。</p><h2 id="数据处理">数据处理</h2><p>首先需要读取所有名字，然后保存所有名字中出现过的不同字符：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = open(<span class="string">'dinos.txt'</span>, <span class="string">'r'</span>).read()</span><br><span class="line">data= data.lower()</span><br><span class="line">chars = list(set(data))</span><br><span class="line">data_size, vocab_size = len(data), len(chars)</span><br><span class="line">print(<span class="string">'There are %d total characters and %d unique characters in your data.'</span> % (data_size, vocab_size))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">There are 19909 total characters and 27 unique characters in your data.</span><br></pre></td></tr></table></figure><p>字符有 a-z 和 ""，换行符的作用类似 <code>&lt;EOS&gt;</code> (End Of Sentence)，在这里是恐龙名字的结束符。然后需要创建字典来保存这些字符，在 Softmax 输出的概率分布中，就能知道哪个索引对应哪个字符，也能将名字中的每个字符转化成向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(sorted(chars)) &#125;</span><br><span class="line">ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(sorted(chars)) &#125;</span><br><span class="line">print(ix_to_char)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;0: &apos;\n&apos;, 1: &apos;a&apos;, 2: &apos;b&apos;, 3: &apos;c&apos;, 4: &apos;d&apos;, 5: &apos;e&apos;, 6: &apos;f&apos;, 7: &apos;g&apos;, 8: &apos;h&apos;, 9: &apos;i&apos;, 10: &apos;j&apos;, 11: &apos;k&apos;, 12: &apos;l&apos;, 13: &apos;m&apos;, 14: &apos;n&apos;, 15: &apos;o&apos;, 16: &apos;p&apos;, 17: &apos;q&apos;, 18: &apos;r&apos;, 19: &apos;s&apos;, 20: &apos;t&apos;, 21: &apos;u&apos;, 22: &apos;v&apos;, 23: &apos;w&apos;, 24: &apos;x&apos;, 25: &apos;y&apos;, 26: &apos;z&apos;&#125;</span><br></pre></td></tr></table></figure><h2 id="模型">模型</h2><p>模型的结构如下所示：</p><ul><li>初始化参数</li><li>运行优化循环<ul><li>前向传播计算损失函数</li><li>反向传播计算关于损失函数的梯度</li><li>梯度裁剪避免梯度爆炸</li><li>使用梯度下降更新规则更新参数</li></ul></li><li>返回学习好的参数</li></ul><p><img src="/2018/06/27/character-level-language-model/rnn.png"></p><p>在每一个时间步给定前一个字符，RNN 就会预测出下一个字符，所以对于每一个时间步有 <span class="math inline">\(y^{\langle t \rangle} = x^{\langle t+1 \rangle}\)</span>。</p><h3 id="初始化参数">初始化参数</h3><p>初始化三个权值参数 <span class="math inline">\(W_{ax}, W_{aa}, W_{ya}\)</span> 和两个偏置参数 <span class="math inline">\(b_a, b_y\)</span>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_a, n_x, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    Wax = np.random.randn(n_a, n_x)*<span class="number">0.01</span> <span class="comment"># input to hidden</span></span><br><span class="line">    Waa = np.random.randn(n_a, n_a)*<span class="number">0.01</span> <span class="comment"># hidden to hidden</span></span><br><span class="line">    Wya = np.random.randn(n_y, n_a)*<span class="number">0.01</span> <span class="comment"># hidden to output</span></span><br><span class="line">    ba = np.zeros((n_a, <span class="number">1</span>)) <span class="comment"># hidden bias</span></span><br><span class="line">    by = np.zeros((n_y, <span class="number">1</span>)) <span class="comment"># output bias</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"Wax"</span>: Wax, <span class="string">"Waa"</span>: Waa, <span class="string">"Wya"</span>: Wya, <span class="string">"ba"</span>: ba,<span class="string">"by"</span>: by&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="前向传播">前向传播</h3><p><span class="math display">\[ a^{\langle t\rangle}=tanh(W_{ax}x^{\langle t\rangle}+W_{aa}^{\langle t-1\rangle}+b_a) \]</span></p><p><span class="math display">\[ \hat y^{\langle t\rangle}=softmax(W_{ya}a^{\langle t\rangle}+b_y) \]</span></p><p>前向传播的代码和<a href="2018/06/20/Recurrent-neural-network">循环神经网络</a>稍有区别，输入 <code>rnn_step_foward</code> 中的参数不需要缓存返回。输入一个字符，通过 RNN 细胞得到下一个字符的概率分布，RNN 细胞的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(parameters, a_prev, x)</span>:</span></span><br><span class="line">    Waa, Wax, Wya, by, ba = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'ba'</span>]</span><br><span class="line">    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba) <span class="comment"># hidden state</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya, a_next) + by) <span class="comment"># unnormalized log probabilities for next chars # probabilities for next chars </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred</span><br></pre></td></tr></table></figure><p>在完整的 RNN 前向传播中，需要定义一个字典 <code>a</code> 来存储每个时间步 <span class="math inline">\(\langle t\rangle\)</span> 与其对应的隐藏状态 <span class="math inline">\(a^{\langle t\rangle}\)</span>；输入一个序列：0 + <code>X</code> (一个恐龙的名字)，然后遍历序列的每个字符 <code>X[t]</code>，将其转化成一个 27 维的 ont-hot 向量 <code>x[t]</code>；计算每个时间步的损失函数 <span class="math inline">\(loss=-log\hat y^{\langle t\rangle}_{y^{\langle t\rangle}}\)</span> (<span class="math inline">\(\hat y^{\langle t\rangle}\)</span> 为 Softmax 函数输出的概率分布，<span class="math inline">\(y^{\langle t\rangle}\)</span> 为真实的下一个字符，即 <span class="math inline">\(x^{\langle t+1\rangle}\)</span>，损失函数可参考 Softmax 回归的损失函数)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(X, Y, a0, parameters, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize x, a and y_hat as empty dictionaries</span></span><br><span class="line">    x, a, y_hat = &#123;&#125;, &#123;&#125;, &#123;&#125;</span><br><span class="line">    a[<span class="number">-1</span>] = np.copy(a0)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize your loss to 0</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        <span class="comment"># Set x[t] to be the one-hot vector representation of the t'th character in X.</span></span><br><span class="line">        <span class="comment"># if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. </span></span><br><span class="line">        x[t] = np.zeros((vocab_size,<span class="number">1</span>)) </span><br><span class="line">        <span class="keyword">if</span> (X[t] != <span class="literal">None</span>):</span><br><span class="line">            x[t][X[t]] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Run one step forward of the RNN</span></span><br><span class="line">        a[t], y_hat[t] = rnn_step_forward(parameters, a[t<span class="number">-1</span>], x[t])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the loss by substracting the cross-entropy term of this time-step from it.</span></span><br><span class="line">        loss -= np.log(y_hat[t][Y[t],<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">    cache = (y_hat, a, x)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> loss, cache</span><br></pre></td></tr></table></figure><h3 id="反向传播">反向传播</h3><p><span class="math display">\[ dW_{ya}=\sum_{t=1}^{T_x}dy^{\langle t\rangle}*a^T \]</span></p><p><span class="math display">\[ db_y=\sum_{t=1}^{T_x}dy^{\langle t\rangle} \]</span></p><p><span class="math display">\[ da=W_{ya}^Tdy+W_{aa}^Tda^{\langle t+1\rangle}diag(1-a^{\langle t+1\rangle2}) \]</span></p><p><span class="math display">\[ db_a=\sum_{t=1}^{T_x}diag(1-a^{\langle t\rangle2})a^{\langle t\rangle} \]</span></p><p><span class="math display">\[ dW_{ax}=\sum_{t=1}^{T_x}diag(1-a^{\langle t\rangle2})a^{\langle t\rangle}x^{\langle t\rangle T} \]</span></p><p>在反向传播中需要实现以上公式，在反向传播中需要注意代码的顺序，代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dy, gradients, parameters, x, a, a_prev)</span>:</span></span><br><span class="line">    gradients[<span class="string">'dWya'</span>] += np.dot(dy, a.T)</span><br><span class="line">    gradients[<span class="string">'dby'</span>] += dy</span><br><span class="line">    da = np.dot(parameters[<span class="string">'Wya'</span>].T, dy) + gradients[<span class="string">'da_next'</span>] <span class="comment"># backprop into h</span></span><br><span class="line">    daraw = (<span class="number">1</span> - a * a) * da <span class="comment"># backprop through tanh nonlinearity</span></span><br><span class="line">    gradients[<span class="string">'dba'</span>] += daraw</span><br><span class="line">    gradients[<span class="string">'dWax'</span>] += np.dot(daraw, x.T)</span><br><span class="line">    gradients[<span class="string">'dWaa'</span>] += np.dot(daraw, a_prev.T)</span><br><span class="line">    gradients[<span class="string">'da_next'</span>] = np.dot(parameters[<span class="string">'Waa'</span>].T, daraw)</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p></p><p>在完整的 RNN 反向传播中，需要定义参数的梯度且形状应该和该参数一样，例如 <code>gradients['dWax'] = np.zeros_like(Wax)</code>；遍历所有时间步，计算当前时间步的损失函数对输出的梯度 <span class="math inline">\(dy^{\langle t\rangle}[\hat y^{\langle t\rangle }]-=1\)</span> (可参考 Softmax 回归损失函数关于输出的梯度)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(X, Y, parameters, cache)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize gradients as an empty dictionary</span></span><br><span class="line">    gradients = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve from cache and parameters</span></span><br><span class="line">    (y_hat, a, x) = cache</span><br><span class="line">    Waa, Wax, Wya, by, ba = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'ba'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># each one should be initialized to zeros of the same dimension as its corresponding parameter</span></span><br><span class="line">    gradients[<span class="string">'dWax'</span>], gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWya'</span>] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)</span><br><span class="line">    gradients[<span class="string">'dba'</span>], gradients[<span class="string">'dby'</span>] = np.zeros_like(ba), np.zeros_like(by)</span><br><span class="line">    gradients[<span class="string">'da_next'</span>] = np.zeros_like(a[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagate through time</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(len(X))):</span><br><span class="line">        dy = np.copy(y_hat[t])</span><br><span class="line">        dy[Y[t]] -= <span class="number">1</span></span><br><span class="line">        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t<span class="number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients, a</span><br></pre></td></tr></table></figure><h3 id="梯度裁剪">梯度裁剪</h3><p>在反向传播中，我们需要对参数求梯度，然后根据参数梯度更新参数。在更新参数之前，需要对参数梯度进行裁剪，保证梯度不会爆炸，即梯度的取值不会太大。</p><p><img src="/2018/06/27/character-level-language-model/clip.png"></p><p>梯度裁剪的实现有许多不同方法，例如对梯度的 L2 范数进行裁剪和对梯度值进行裁剪，这里实现的是对梯度值进行裁剪，确保梯度在 <span class="math inline">\([-maxValue, maxValue]\)</span> 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients, maxValue)</span>:</span></span><br><span class="line">    dWaa, dWax, dWya, dba, dby = gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWax'</span>], gradients[<span class="string">'dWya'</span>], gradients[<span class="string">'dba'</span>], gradients[<span class="string">'dby'</span>]</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, dba, dby].</span></span><br><span class="line">    <span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax, dWaa, dWya, dba, dby]:</span><br><span class="line">        np.clip(gradient, a_min=-maxValue, a_max=maxValue, out=gradient)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dWaa"</span>: dWaa, <span class="string">"dWax"</span>: dWax, <span class="string">"dWya"</span>: dWya, <span class="string">"dba"</span>: dba, <span class="string">"dby"</span>: dby&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="更新参数">更新参数</h3><p>更新参数部分的代码比较简单，就是减去学习率 (<strong>l</strong>earning <strong>r</strong>ate) 乘以梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, gradients, lr)</span>:</span></span><br><span class="line">    parameters[<span class="string">'Wax'</span>] += -lr * gradients[<span class="string">'dWax'</span>]</span><br><span class="line">    parameters[<span class="string">'Waa'</span>] += -lr * gradients[<span class="string">'dWaa'</span>]</span><br><span class="line">    parameters[<span class="string">'Wya'</span>] += -lr * gradients[<span class="string">'dWya'</span>]</span><br><span class="line">    parameters[<span class="string">'ba'</span>]  += -lr * gradients[<span class="string">'dba'</span>]</span><br><span class="line">    parameters[<span class="string">'by'</span>]  += -lr * gradients[<span class="string">'dby'</span>]</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="构建语言模型">构建语言模型</h3><p>将上面步骤结合在一起，实现模型，最后需要返回最后一个时间步的隐藏状态，用做<strong>下一个序列</strong>的第 0 个时间步的隐藏状态：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Forward propagate through time</span></span><br><span class="line">    loss, cache = rnn_forward(X, Y, a_prev, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagate through time (≈1 line)</span></span><br><span class="line">    gradients, a = rnn_backward(X, Y, parameters, cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Clip your gradients between -5 (min) and 5 (max)</span></span><br><span class="line">    gradients = clip(gradients, maxValue = <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update parameters</span></span><br><span class="line">    parameters = update_parameters(parameters, gradients, learning_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, gradients, a[len(X)<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><h2 id="采样">采样</h2><p>训练出参数后，我们可能想让模型生成一些恐龙的名字，看看效果怎么样，生成流程如下图所示：</p><p><img src="/2018/06/27/character-level-language-model/dinos3.png"></p><ol type="1"><li><p><span class="math inline">\(a^{\langle 0\rangle}\)</span> 和 <span class="math inline">\(x^{\langle 1\rangle}\)</span> 为 0 向量</p></li><li><p>向前传播一个时间步得到隐藏状态 <span class="math inline">\(a^{\langle 1\rangle}\)</span> 输出 <span class="math inline">\(\hat y^{\langle 1\rangle}\)</span> (即各个字符的概率分布)： <span class="math display">\[ a^{\langle t+1 \rangle} = \tanh(W_{ax} x^{\langle t \rangle } + W_{aa} a^{\langle t \rangle } + ba) \]</span></p><p><span class="math display">\[ \hat y^{\langle t + 1 \rangle } = softmax(W_{ya} a^{\langle t + 1 \rangle } + b_y) \]</span></p></li><li><p>进行采样：假设 <span class="math inline">\(\hat{y}^{\langle t+1 \rangle }_i = 0.16\)</span>，则以 16% 的概率选取索引 i 所对应的字符，可以使用 <code>np.random.choice</code> 实现。这也正是 <code>softmax</code> 名字的由来，没有强硬地输出一个最大值，而是输出每个值为最大的概率，虽然大部分情况下用的就是概率最大的那个，但是采样的时候就可以按概率分布随机采样。</p></li><li><p>用上一个时间步的输出作为输入，重复采样，直到遇到结束符(或者名字长度为 50 个字符，避免停不下来)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters, char_to_ix, seed)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve parameters and relevant shapes from "parameters" dictionary</span></span><br><span class="line">    Waa, Wax, Wya, by, ba = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'ba'</span>]</span><br><span class="line">    vocab_size = by.shape[<span class="number">0</span>]</span><br><span class="line">    n_a = Waa.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Create the one-hot vector x for the first character (initializing the sequence generation).</span></span><br><span class="line">    x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Step 1': Initialize a_prev as zeros</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate</span></span><br><span class="line">    indices = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Idx is a flag to detect a newline character, we initialize it to -1</span></span><br><span class="line">    idx = <span class="number">-1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over time-steps t. At each time-step, sample a character from a probability distribution and append </span></span><br><span class="line">    <span class="comment"># its index to "indices". We'll stop if we reach 50 characters (which should be very unlikely with a well </span></span><br><span class="line">    <span class="comment"># trained model), which helps debugging and prevents entering an infinite loop. </span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    newline_character = char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (idx != newline_character <span class="keyword">and</span> counter != <span class="number">50</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2: Forward propagate x using the equations (1), (2) and (3)</span></span><br><span class="line">        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)</span><br><span class="line">        z = np.dot(Wya, a) + by</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        np.random.seed(counter+seed)  </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3: Sample the index of a character within the vocabulary from the probability distribution y</span></span><br><span class="line">        idx = np.random.choice(list(range(vocab_size)), p = y[:,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append the index to "indices"</span></span><br><span class="line">        indices.append(idx)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 4: Overwrite the input character as the one corresponding to the sampled index.</span></span><br><span class="line">        x = np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">        x[idx] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update "a_prev" to be "a"</span></span><br><span class="line">        a_prev = a</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        seed += <span class="number">1</span></span><br><span class="line">        counter +=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> (counter == <span class="number">50</span>):</span><br><span class="line">        indices.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> indices</span><br></pre></td></tr></table></figure><p></p></li></ol><h2 id="训练模型">训练模型</h2><p>对于数据集中的每一行数据(随机打乱)，随机梯度下降 100 次后则随机采样生成 10 个名字。首先需要生成数据的标签，即每个字符对于的标签是它的下一个字符：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = j % len(examples)</span><br><span class="line">X = [<span class="literal">None</span>] + [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]] </span><br><span class="line">Y = X[<span class="number">1</span>:] + [char_to_ix[<span class="string">"\n"</span>]]</span><br></pre></td></tr></table></figure><p>由于使用的梯度下降法是随机梯度下降，会存在振荡现象，需要用带修正的指数加权平均的方法来减小噪声：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_initial_loss</span><span class="params">(vocab_size, seq_length)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -np.log(<span class="number">1.0</span>/vocab_size)*seq_length</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth</span><span class="params">(loss, cur_loss)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> loss * <span class="number">0.999</span> + cur_loss * <span class="number">0.001</span></span><br></pre></td></tr></table></figure><p>模型的完整代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data, ix_to_char, char_to_ix, num_iterations = <span class="number">35000</span>, n_a = <span class="number">50</span>, dino_names = <span class="number">7</span>, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve n_x and n_y from vocab_size</span></span><br><span class="line">    n_x, n_y = vocab_size, vocab_size</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(n_a, n_x, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize loss (this is required because we want to smooth our loss, don't worry about it)</span></span><br><span class="line">    loss = get_initial_loss(vocab_size, dino_names)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build list of all dinosaur names (training examples).</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"dinos.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        examples = f.readlines()</span><br><span class="line">    examples = [x.lower().strip() <span class="keyword">for</span> x <span class="keyword">in</span> examples]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle list of all dinosaur names</span></span><br><span class="line">    shuffle(examples)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the hidden state of your LSTM</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use the hint above to define one training example (X,Y) (≈ 2 lines)</span></span><br><span class="line">        index = j % len(examples)</span><br><span class="line">        X = [<span class="literal">None</span>] + [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]] </span><br><span class="line">        Y = X[<span class="number">1</span>:] + [char_to_ix[<span class="string">"\n"</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters</span></span><br><span class="line">        <span class="comment"># Choose a learning rate of 0.01</span></span><br><span class="line">        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use a latency trick to keep the loss smooth. It happens here to accelerate the training.</span></span><br><span class="line">        loss = smooth(loss, curr_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Every 2000 Iteration, generate "n" characters thanks to sample() to check if the model is learning properly</span></span><br><span class="line">        <span class="keyword">if</span> j % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Iteration: %d, Loss: %f'</span> % (j, loss) + <span class="string">'\n'</span>)</span><br><span class="line">            <span class="comment"># The number of dinosaur names to print</span></span><br><span class="line">            seed = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> range(dino_names):</span><br><span class="line">                <span class="comment"># Sample indices and print them</span></span><br><span class="line">                sampled_indices = sample(parameters, char_to_ix, seed)</span><br><span class="line">                print_sample(sampled_indices, ix_to_char)</span><br><span class="line">                seed += <span class="number">1</span>  <span class="comment"># To get the same result for grading purposed, increment the seed by one. </span></span><br><span class="line">            print(<span class="string">'\n'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="莎士比亚风格">* 莎士比亚风格</h2><p>作业最后还展示了如何使用 LSTM 生成莎士比亚风格的诗词，由于恐龙的名字很短，所以长期依赖问题不明显。生成莎士比亚风格诗词的时候就很明显，所以需要使用 LSTM 来解决长期以来问题。</p><p>基于字符的语言模型有优点也有缺点，优点是不必担心出现未知的标识，例如 <code>Mau</code> 这样的序列。而基于词汇的语言模型，如果 <code>Mau</code> 不在字典中就只能把它当成 UNK。缺点是序列太长，即时间步太多，很难捕捉长期依赖关系，计算成本高。除非需要处理大量未知文本和未知词汇的应用，大多数都是使用基于词汇的语言模型。</p><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2018/06/27/character-level-language-model/" title="字符级别的语言模型">https://pengzhendong.cn/2018/06/27/character-level-language-model/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/06/20/recurrent-neural-network/" rel="prev" title="循环神经网络"><i class="fa fa-chevron-left"></i> 循环神经网络</a></div><div class="post-nav-item"> <a href="/2018/08/28/jazz-with-an-lstm-network/" rel="next" title="用 LSTM 网络创作爵士独奏">用 LSTM 网络创作爵士独奏<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据处理"><span class="nav-number">2.</span> <span class="nav-text">数据处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型"><span class="nav-number">3.</span> <span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化参数"><span class="nav-number">3.1.</span> <span class="nav-text">初始化参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播"><span class="nav-number">3.2.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">3.3.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度裁剪"><span class="nav-number">3.4.</span> <span class="nav-text">梯度裁剪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#更新参数"><span class="nav-number">3.5.</span> <span class="nav-text">更新参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#构建语言模型"><span class="nav-number">3.6.</span> <span class="nav-text">构建语言模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#采样"><span class="nav-number">4.</span> <span class="nav-text">采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练模型"><span class="nav-number">5.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#莎士比亚风格"><span class="nav-number">6.</span> <span class="nav-text">* 莎士比亚风格</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">7.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>