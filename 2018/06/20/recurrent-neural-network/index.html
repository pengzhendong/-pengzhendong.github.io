<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 按照吴恩达 Deeplearning 系列课程，应该是先学卷积神经网络，但是自己的实验中要用到递归神经网络，感觉不能再拖了，就先学习一下序列模型这一章节。在普通的神经网络中，一般都是输入一个向量，然后输出一个向量或者通过 Sigmoid 函数后输出一个值。"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="循环神经网络"><meta property="og:url" content="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 按照吴恩达 Deeplearning 系列课程，应该是先学卷积神经网络，但是自己的实验中要用到递归神经网络，感觉不能再拖了，就先学习一下序列模型这一章节。在普通的神经网络中，一般都是输入一个向量，然后输出一个向量或者通过 Sigmoid 函数后输出一个值。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/rnn.png"><meta property="og:image" content="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/rnn_step_forward.png"><meta property="og:image" content="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/cell_rnn.png"><meta property="og:image" content="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/LSTM.png"><meta property="og:image" content="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/LSTM_rnn.png"><meta property="og:image" content="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/rnn_cell_backprop.png"><meta property="og:updated_time" content="2018-06-20T06:33:32.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="循环神经网络"><meta name="twitter:description" content="前言 按照吴恩达 Deeplearning 系列课程，应该是先学卷积神经网络，但是自己的实验中要用到递归神经网络，感觉不能再拖了，就先学习一下序列模型这一章节。在普通的神经网络中，一般都是输入一个向量，然后输出一个向量或者通过 Sigmoid 函数后输出一个值。"><meta name="twitter:image" content="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/rnn.png"><link rel="canonical" href="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>循环神经网络 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 循环神经网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-06-20 10:29:57 / 修改时间：14:33:32" itemprop="dateCreated datePublished" datetime="2018-06-20T10:29:57+08:00">2018-06-20</time></span><span id="/2018/06/20/recurrent-neural-network/" class="post-meta-item leancloud_visitors" data-flag-title="循环神经网络" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>按照吴恩达 Deeplearning 系列课程，应该是先学卷积神经网络，但是自己的实验中要用到递归神经网络，感觉不能再拖了，就先学习一下序列模型这一章节。在普通的神经网络中，一般都是输入一个向量，然后输出一个向量或者通过 Sigmoid 函数后输出一个值。</p><a id="more"></a><h2 id="循环神经网络">循环神经网络</h2><p>循环神经网络 (Recurrent neural network) 是一类用于处理<strong>序列数据</strong>的神经网络。在普通的神经网络中，前一个输入和后一个输入没有关系；而在有时候需要网络在处理当前输入的时候，记住之前的输入的相关信息。如果将整个序列当成一个整体输入普通神经网络的话，则会遇到输入的长度不同(可通过填充解决)和参数量大的问题。</p><p>根据处理的问题，网络按结构可以分为以下几种：</p><ul><li>一对一：非 RNN 结构，例如图片分类</li><li>一对多：序列输出，例如生成图片的描述</li><li>多对一：序列输入，例如评论的情感分类</li><li>多对多：<ul><li>序列输入和序列输出，例如机器翻译</li><li>同步序列输入和输出，例如视频的帧分类</li></ul></li><li>递归神经网络 (Recursive nerual network) 是空间上的展开，处理的是树状结构信息(例如语法树)；循环神经网络是时间上的展开(也叫时间递归神经网络)，处理的是序列结构信息； RNN 一般指循环神经网络。</li></ul><h2 id="前向传播">前向传播</h2><p>在同步序列输入和输出结构中，有 <span class="math inline">\(T_x=T_y\)</span> ，其结构如下图所示：</p><p><img src="/2018/06/20/recurrent-neural-network/rnn.png"></p><p>一共有 <span class="math inline">\(T_x\)</span> 个时间步，所以只需要实现一个时间步，然后循环 <span class="math inline">\(T_x\)</span> 次则可以实现 RNN 的前向传播。</p><h3 id="rnn-细胞">RNN 细胞</h3><p>一个循环神经网络可以看成是单个细胞(即时间步)的循环，所有细胞共享参数。细胞内部结构如下图所示：</p><p><img src="/2018/06/20/recurrent-neural-network/rnn_step_forward.png"></p><p>细胞的输入有当前(第 <span class="math inline">\(t\)</span> 个时间步)的输入 <span class="math inline">\(x^{\langle t\rangle}\)</span> 和之前的隐藏状态 <span class="math inline">\(a^{\langle t-1\rangle}\)</span> (包含了以前的信息)，输出有 <span class="math inline">\(a^{\langle t\rangle}\)</span> 和 <span class="math inline">\(\hat y^{\langle t\rangle}\)</span> 。在前向传播过程中，需要缓存各种值用于反向传播计算参数梯度，实现 RNN 细胞代码主要分为以下几个步骤：</p><ol type="1"><li>用 <span class="math inline">\(tanh\)</span> 激活函数计算隐藏状态：<span class="math inline">\(a^{\langle t\rangle}=tanh(W_{aa}a^{\langle t-1\rangle}+W_{ax}x^{\langle t\rangle}+b_a)\)</span></li><li>用新的隐藏状态 <span class="math inline">\(a^{\langle t \rangle}\)</span> 计算预测值 <span class="math inline">\(\hat y^{\langle t\rangle}=softmax(W_{ya}a^{\langle t\rangle}+b_y)\)</span></li><li>缓存 <span class="math inline">\(a^{\langle t\rangle}, a^{\langle t-1\rangle}, x^{\langle t\rangle}\)</span></li><li>返回 <span class="math inline">\(a^{\langle t\rangle}, \hat y^{\langle t\rangle}\)</span> 和缓存</li></ol><p>一共有 <span class="math inline">\(m\)</span> 个样本数据，其中 <span class="math inline">\(x^{\langle t\rangle}\)</span> 的维度为 <span class="math inline">\((n_x, m)\)</span>，<span class="math inline">\(a^{\langle t\rangle}\)</span> 的维度为 <span class="math inline">\((n_a, m)\)</span>。代码中使用 <code>_prev</code> 表示上一个时间步 <span class="math inline">\(\langle t-1\rangle\)</span> ，<code>_next</code> 和 <code>t</code> 表示当前时间步 <span class="math inline">\(\langle t\rangle\)</span>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya, a_next) + by)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure><h3 id="rnn-前向传播">RNN 前向传播</h3><p><img src="/2018/06/20/recurrent-neural-network/cell_rnn.png"></p><p>RNN 的前向传播主要分为以下几个步骤：</p><ol type="1"><li>创建零向量 <span class="math inline">\(\boldsymbol{a}\)</span> 用于存储<strong>所有</strong>隐藏状态</li><li>初始化隐藏状态 <span class="math inline">\(a_0\)</span></li><li>循环所有时间步，当前时间步为 <span class="math inline">\(t\)</span><ul><li>使用 <code>run_cell_forward</code> 函数更新隐藏状态 <span class="math inline">\(a^{\langle t\rangle}\)</span> 和缓存</li><li>存储 <span class="math inline">\(a^{\langle t\rangle}\)</span> 到 <span class="math inline">\(\boldsymbol{a}\)</span> 中的第 <span class="math inline">\(t\)</span> 个位置</li><li>存储预测值 <span class="math inline">\(\hat y^{\langle t\rangle}\)</span> 到 <span class="math inline">\(\boldsymbol{\hat y}\)</span> 中</li><li>添加缓存到缓存列表中</li></ul></li><li>返回 <span class="math inline">\(\boldsymbol{a}, \boldsymbol{\hat y}\)</span> 和缓存列表</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and Wy</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a" and "y" with zeros</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    y_pred = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, compute the prediction, get the cache</span></span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y</span></span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Append "cache" to "caches"</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure><h3 id="长期依赖">长期依赖</h3><p>循环神经网络具有长期依赖的问题，在经过许多阶段传播后的梯度倾向于消失(大部分情况)或爆炸(很少而且容易发现，但对优化过程影响很大，可以使用梯度截断的方法解决)，因此只能学习到短期的依赖关系。以一个简单的、缺少非线性激活函数和输入 <span class="math inline">\(x\)</span> 的循环神经网络为例： <span class="math display">\[ a^{\langle t\rangle}=W_{aa}a^{\langle t-1\rangle} \]</span></p><p><span class="math display">\[ a^{\langle t\rangle}=W_{aa}^ta^{\langle 0\rangle} \]</span> 类似于深度神经网络，当 <span class="math inline">\(W_{aa}\)</span> 的特征值小于 1 时就会导致隐藏状态约等于 0。即 RNN 会忘了很久以前的信息，如果不需要用很久以前的信息(有意义的信息都在前<strong>几个</strong>时间步)就能估计输出 <span class="math inline">\(\hat y^{\langle t\rangle}\)</span> ，那么 RNN 效果也不错。而 LSTM 则可以很好得解决这个问题，可以记住更多时间步以前的信息。目前实际应用中最有效的序列模型称为门控 RNN (gated RNN)。包括基于长短期记忆 (Long Short-Term Memory, LSTM) 和基于门控循环单元 (gated recurrent unit, GRU) 网络。</p><h3 id="lstm-细胞">LSTM 细胞</h3><p>基于长短期记忆 (LSTM) 的网络的细胞结构如下图所示：</p><p><img src="/2018/06/20/recurrent-neural-network/LSTM.png"></p><p><strong>LSTM 最关键的地方就在于细胞的状态 <span class="math inline">\(c^{\langle t\rangle}\)</span>，即上图中上面横穿的水平线，这种结构能够很轻松地实现信息从整个细胞中穿过而不做改变(没有经过 <span class="math inline">\(tanh\)</span> 激活函数)，从而实现了长时期的记忆保留</strong>。可以参考反向传播时的分析，LSTM 通过门 (gates) 的结构来实现给细胞的状态添加或者删除信息。</p><h4 id="遗忘门">遗忘门</h4><p>假如我们希望用 LSTM 来跟踪主语是单数还是复数，如果主语从单数变成复数，我们需要忘记之前存储的状态。在 LSTM 中使用遗忘门 (<strong>F</strong>orget gate) 实现这一点： <span class="math display">\[ \Gamma_f^{\langle t\rangle}=\sigma(W_f[a^{\langle t-1\rangle}, x^{\langle t\rangle}]+b_f)\tag{1} \]</span> 其中 <span class="math inline">\(W_f\)</span> 是控制遗忘门行为的权重，遗忘门的输出 <span class="math inline">\(\Gamma_f^{\langle t \rangle}\)</span> 最后要作用于细胞的状态 (<span class="math inline">\(\Gamma_f^{\langle t\rangle}*c^{\langle t-1\rangle}\)</span>)，因此使用 <span class="math inline">\(sigmoid\)</span> 激活函数保证输出是一个 0，1 之间的向量，表示让 <span class="math inline">\(c^{\langle t-1\rangle}\)</span> 各部分信息通过的比例，0 表示不让任何信息通过，1 表示让所有信息通过。</p><h4 id="更新门">更新门</h4><p>类似于遗忘门，更新门 (<strong>U</strong>pdate gate) 也可以叫输入门 (<strong>I</strong>nput gate)，决定让多少新的信息加入到细胞状态中： <span class="math display">\[ \Gamma_u^{\langle t\rangle}=\sigma(W_u[a^{\langle t-1\rangle}, x^{\langle t\rangle}]+b_u)\tag{2} \]</span> 更新门的输出 <span class="math inline">\(\Gamma_u^{\langle t\rangle}\)</span> 要作用于新的信息 <span class="math inline">\(\tilde{c}^{\langle t\rangle}\)</span>，生成更新内容 <span class="math inline">\(\Gamma_u^{\langle t\rangle}*\tilde{c}^{\langle t\rangle}\)</span>，然后再添加到细胞的状态上： <span class="math display">\[ \tilde{c}^{\langle t\rangle}=\tanh(W_c[a^{\langle t-1\rangle}, x^{\langle t\rangle}]+b_c)\tag{3} \]</span></p><p><span class="math display">\[ c^{\langle t\rangle}=\Gamma_f^{\langle t\rangle}*c^{\langle t-1\rangle}+\Gamma_u^{\langle t\rangle}*\tilde{c}^{\langle t\rangle}\tag{4} \]</span></p><h4 id="输出门">输出门</h4><p>输出门 (<strong>O</strong>utput gate) 的输出如下所示： <span class="math display">\[ \Gamma_o^{\langle t\rangle}=\sigma(W_o[a^{\langle t-1\rangle}, x^{\langle t\rangle}]+b_o)\tag{5} \]</span> 最后细胞的隐藏状态为： <span class="math display">\[ a^{\langle t \rangle}=\Gamma_o^{\langle t\rangle}*\tanh(c^{\langle t\rangle})\tag{6} \]</span> 遗忘门、更新门和输出门的输入只取决于 <span class="math inline">\(a^{\langle t-1\rangle}\)</span> 和 <span class="math inline">\(x^{\langle t\rangle}\)</span>，如果还取决与上一个细胞的状态 <span class="math inline">\(c^{\langle t-1\rangle}\)</span> 则称为<strong>窥孔连接</strong>。类似于 RNN 细胞需要缓存各种值用于反向传播计算参数梯度，实现 LSTM 细胞代码主要分为以下几个步骤：</p><ol type="1"><li>连接 <span class="math inline">\(a^{\langle t-1\rangle}\)</span> 和 <span class="math inline">\(x^{\langle t \rangle}\)</span> 到一个矩阵中：<span class="math inline">\(concat = \begin{bmatrix} a^{\langle t-1 \rangle} \\\ x^{\langle t \rangle} \end{bmatrix}\)</span></li><li>实现公式 <span class="math inline">\((1)-(6)\)</span></li><li>计算预测值 <span class="math inline">\(y^{\langle t \rangle}\)</span></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>]</span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wu = parameters[<span class="string">"Wu"</span>]</span><br><span class="line">    bu = parameters[<span class="string">"bu"</span>]</span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>]</span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>]</span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt</span></span><br><span class="line">    concat = np.zeros([n_a + n_x, m])</span><br><span class="line">    concat[:n_a,:] = a_prev</span><br><span class="line">    concat[n_a:,:] = xt</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute values for ft, ut, cct, c_next, ot, a_next using the formulas</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    ut = sigmoid(np.dot(Wu, concat) + bu)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft * c_prev + ut * cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wy, a_next) + by)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, ut, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure><h3 id="lstm-前向传播">LSTM 前向传播</h3><p><img src="/2018/06/20/recurrent-neural-network/LSTM_rnn.png"></p><p>类似于 RNN 前向传播，只不过多了一个细胞的状态，所以需要初始化为 0 向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">'Wy'</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros</span></span><br><span class="line">    a = np.zeros([n_a, m, T_x])</span><br><span class="line">    c = np.zeros([n_a, m, T_x])</span><br><span class="line">    y = np.zeros([n_y, m, T_x])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros([n_a, m])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache</span></span><br><span class="line">        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y</span></span><br><span class="line">        y[:,:,t] = yt</span><br><span class="line">        <span class="comment"># Save the value of the next cell state</span></span><br><span class="line">        c[:,:,t]  = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure><h2 id="反向传播">反向传播</h2><p>在 DeepLearning 课程作业中，RNN 反向传播直接忽略了细胞的输出，没有考虑细胞的输出的误差对参数的梯度，降低了作业的难度，在 LSTM 反向传播中考虑了细胞的输出的误差对参数的梯度。</p><p>在预测输出的时候，RNN 使用了 Softmax 函数，关于 Softmax 函数的求导过程可以参考 <a href="/2018/04/26/Logistic-regression">Logistic 回归和 Softmax 回归</a>。RNN 在时间步上反向传播，因此也叫做 BackPropagation Through Time(BPTT) 算法。</p><h3 id="简单版-rnn-细胞">简单版 RNN 细胞</h3><p>没有输出只有隐藏状态的 RNN 细胞的反向传播过程如下图所示：</p><p><img src="/2018/06/20/recurrent-neural-network/rnn_cell_backprop.png"></p><p>由链式求导公式、复合求导公式和矩阵的求导公式或者参考<a href="/2018/05/19/Neuron-network">单隐层神经网络</a>可以推导出右边的表达式，其代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next, cache)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve values from cache</span></span><br><span class="line">    (a_next, a_prev, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from parameters</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of tanh with respect to a_next</span></span><br><span class="line">    dtanh = (<span class="number">1</span>-a_next * a_next) * da_next  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of the loss with respect to Wax</span></span><br><span class="line">    dxt = np.dot(Wax.T,dtanh)</span><br><span class="line">    dWax = np.dot(dtanh, xt.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to Waa</span></span><br><span class="line">    da_prev = np.dot(Waa.T,dtanh)</span><br><span class="line">    dWaa = np.dot(dtanh, a_prev.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to b</span></span><br><span class="line">    dba = np.sum(dtanh, keepdims=<span class="literal">True</span>, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa, <span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="rnn-反向传播">RNN 反向传播</h3><p>在 RNN 反向传播中不但要计算参数的梯度，也要计算 <span class="math inline">\(a^{\langle t\rangle}\)</span> 的梯度，这样才能将梯度反向传播到前一个 RNN 细胞，代码中还保存了输入的梯度到 <span class="math inline">\(dx\)</span> 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, a0, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes</span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes</span></span><br><span class="line">    dx = np.zeros([n_x, m, T_x])</span><br><span class="line">    dWax = np.zeros([n_a, n_x])</span><br><span class="line">    dWaa = np.zeros([n_a, n_a])</span><br><span class="line">    dba = np.zeros([n_a, <span class="number">1</span>])</span><br><span class="line">    da0 = np.zeros([n_a, m])</span><br><span class="line">    da_prevt = np.zeros([n_a, m])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop through all the time steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step.</span></span><br><span class="line">        gradients = rnn_cell_backward(da[:,:,t] + da_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Retrieve derivatives from gradients</span></span><br><span class="line">        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[<span class="string">"dxt"</span>], gradients[<span class="string">"da_prev"</span>], gradients[<span class="string">"dWax"</span>], gradients[<span class="string">"dWaa"</span>], gradients[<span class="string">"dba"</span>]</span><br><span class="line">        <span class="comment"># Increment global derivatives w.r.t parameters by adding their derivative at time-step t</span></span><br><span class="line">        dx[:,:,t] = dxt</span><br><span class="line">        dWax += dWaxt</span><br><span class="line">        dWaa += dWaat</span><br><span class="line">        dba += dbat</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Set da0 to the gradient of a which has been backpropagated through all time-steps</span></span><br><span class="line">    da0 = da_prevt</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa,<span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="完整版-rnn-细胞">完整版 RNN 细胞</h3><p>完整的 RNN 细胞有输出，代价函数是所有时间步的输出的损失函数的和，对参数 <span class="math inline">\(W_{ya}\)</span> 和 <span class="math inline">\(b_y\)</span> 的求导比较简单，因为它们当前梯度只和当前时间步的损失函数相关： <span class="math display">\[ J=\sum_{t=1}^{T_x}J^{\langle t\rangle} \]</span></p><p><span class="math display">\[ \frac{\partial J}{\partial W_{ya}}=\sum_{t=1}^{T_x}(\hat y^{\langle t\rangle}-y^{\langle t\rangle})a^{\langle t\rangle T} \]</span></p><p><span class="math display">\[ \frac{\partial J}{\partial b_y}=\sum_{t=1}^{T_x}\hat y^{\langle t\rangle}-y^{\langle t\rangle} \]</span></p><p>参数 <span class="math inline">\(W_{aa}, b_a\)</span> 和 <span class="math inline">\(W_{ax}\)</span> 的梯度就比较复杂，因为它们的当前梯度不仅和当前时间步的损失函数相关，还和后面的时间步的损失函数相关。首先定义当前时间步的隐藏状态的梯度 <span class="math inline">\(\delta^{\langle t\rangle}\)</span>，其递推公式如下所示： <span class="math display">\[ \begin{align} \delta^{\langle t\rangle}&amp;=\frac{\partial J}{\partial a^{\langle t\rangle}}=\frac{\sum_{i=t}^{T_x}\partial J^{\langle i\rangle}}{\partial a^{\langle t\rangle}} \\\ &amp;=\frac{\partial J^{\langle t\rangle}}{\partial a^{\langle t\rangle}}+\frac{\sum_{i=t+1}^{T_x}\partial J^{\langle i\rangle}}{\partial a^{\langle t+1\rangle}}\frac{\partial a^{\langle t+1\rangle}}{\partial a^{\langle t\rangle}} \\\ &amp;=W_{ya}^T(\hat y^{\langle t\rangle}-y^{\langle t\rangle})+W_{aa}^T\delta^{\langle t+1\rangle}diag(1-a^{\langle t+1\rangle2})\tag{1} \end{align} \]</span> 隐藏状态在时间步方向上的代价函数的梯度(即不考虑当前时间步的输出) 为 <span class="math inline">\(\delta^{\langle T_x\rangle}\prod_{i=t+1}^{T_x}W_{aa}^Tdiag(1-a^{\langle i\rangle2})\)</span>，当参数 <span class="math inline">\(W_{aa}^T\)</span> 小于 1 时就产生了梯度消失，即使使用 <span class="math inline">\(ReLU\)</span> 函数作为激活函数，梯度为 <span class="math inline">\(\delta^{\langle T_x\rangle}\prod_{i=t+1}^{T_x}W_{aa}^T\)</span>，也不能解决长期依赖问题。隐藏状态在最后一个时间步 <span class="math inline">\(\langle T_{x}\rangle\)</span> 梯度只由该时间步的损失函数相关，因为后面不再有损失函数，所以有： <span class="math display">\[ \delta^{\langle T_x\rangle}=\frac{\partial J}{\partial a^{\langle T_x\rangle}}=\frac{\partial J^{\langle T_x\rangle}}{\partial a^{\langle T_x\rangle}}=W_{ya}^T(\hat y^{\langle t\rangle}-y^{\langle t\rangle})\tag{2} \]</span> 根据 <span class="math inline">\((1)\)</span> 和 <span class="math inline">\((2)\)</span> 递推公式可以求得 <span class="math inline">\(\delta^{\langle t\rangle}\)</span>，有了 <span class="math inline">\(\delta^{\langle t\rangle}\)</span> 就可以很轻松地求解参数 <span class="math inline">\(W_{aa}, b_a\)</span> 和 <span class="math inline">\(W_{ax}\)</span> 的梯度： <span class="math display">\[ \frac{\partial J}{\partial W_{aa}}=\sum_{t=1}^{T_x}\frac{\partial J}{\partial a^{\langle t\rangle}}\frac{\partial a^{\langle t\rangle}}{\partial W_{aa}}=\sum_{t=1}^{T_x}diag(1-a^{\langle t\rangle2})\delta^{\langle t\rangle}a^{\langle t-1\rangle T} \]</span></p><p><span class="math display">\[ \frac{\partial J}{\partial b_a}=\sum_{t=1}^{T_x}\frac{\partial J}{\partial a^{\langle t\rangle}}\frac{\partial a^{\langle t\rangle}}{\partial b_a}=\sum_{t=1}^{T_x}diag(1-a^{\langle t\rangle2})\delta^{\langle t\rangle} \]</span></p><p><span class="math display">\[ \frac{\partial J}{\partial W_{ax}}=\sum_{t=1}^{T_x}\frac{\partial J}{\partial a^{\langle t\rangle}}\frac{\partial a^{\langle t\rangle}}{\partial W_{ax}}=\sum_{t=1}^{T_x}diag(1-a^{\langle t\rangle2})\delta^{\langle t\rangle}x^{\langle t\rangle T} \]</span></p><h3 id="lstm-细胞-1">LSTM 细胞</h3><p>LSTM 的细胞结构比较复杂，反向传播的公式也比较难，关于隐藏状态的梯度公式和普通 RNN 类似。参数的梯度不仅和当前时间步的损失函数相关(通过隐藏状态 <span class="math inline">\(a^{\langle t\rangle}\)</span>)，还和后面的时间步的损失函数相关(通过细胞的状态 <span class="math inline">\(c^{\langle t\rangle}\)</span>)。定义当前时间步的细胞状态的梯度 <span class="math inline">\(\delta^{\langle t\rangle}\)</span>，其递推公式如下所示： <span class="math display">\[ \begin{align} \delta^{\langle t\rangle}&amp;=\frac{\partial J}{\partial c^{\langle t\rangle}}=\frac{\sum_{i=t}^{T_x}\partial J^{\langle i\rangle}}{\partial c^{\langle t\rangle}} \\\ &amp;=\frac{\partial J^{\langle t\rangle}}{\partial c^{\langle t\rangle}}+\frac{\sum_{i=t+1}^{T_x}\partial J^{\langle i\rangle}}{\partial c^{\langle t+1\rangle}}\frac{\partial c^{\langle t+1\rangle}}{\partial c^{\langle t\rangle}} \\\ &amp;=\frac{\partial J^{\langle t\rangle}}{\partial c^{\langle t\rangle}}+\delta^{\langle t+1\rangle}\Gamma_f^{\langle t\rangle} \end{align} \]</span> 细胞状态在时间步方向上的代价函数的梯度为 <span class="math inline">\(\delta^{\langle T_x\rangle}\prod_{i=t+1}^k\Gamma_f^{\langle i\rangle}\)</span>，因为最原始的 LSTM 没有遗忘门，即 <span class="math inline">\(\Gamma_f^{\langle t\rangle}=1\)</span>，所以不存在梯度消失问题。目前流行的深度学习框架中 <span class="math inline">\(b_f\)</span> 一般会设置的大一些，这样遗忘门的输出 <span class="math inline">\(\Gamma_f^{\langle t\rangle}=\sigma(W_f[a^{\langle t-1\rangle}, x^{\langle t\rangle}]+b_f)\)</span> 就会约等于 1，可以减缓梯度消失，所以即使遗忘门的输出很小，那也是当前时间步的输入导致的模型的选择，不是多层嵌套导致的梯度消失。</p><p>LSTM 细胞的梯度主要分为两部分：门的梯度和参数的梯度，参数的梯度公式如下：</p><ul><li><p>门的梯度 <span class="math display">\[ d\Gamma_o^{\langle t \rangle}=da^{\langle t\rangle}*\tanh(c^{\langle t\rangle}) \]</span></p><p><span class="math display">\[ d\tilde c^{\langle t\rangle}=dc^{\langle t\rangle}*\Gamma_u^{\langle t \rangle}+\Gamma_o^{\langle t\rangle}\big(1-\tanh(c^{\langle t\rangle})^2\big)*\Gamma_u^{\langle t \rangle}*da^{\langle t\rangle} \]</span></p><p><span class="math display">\[ d\Gamma_u^{\langle t \rangle}=dc^{\langle t\rangle}*\tilde c^{\langle t\rangle}+\Gamma_o^{\langle t\rangle}\big(1-\tanh(c^{\langle t\rangle})^2\big)*\tilde c^{\langle t\rangle}*da^{\langle t\rangle} \]</span></p><p><span class="math display">\[ d\Gamma_f^{\langle t\rangle}=dc^{\langle t\rangle}*c^{\langle t-1\rangle}+\Gamma_o^{\langle t\rangle}\big(1-\tanh(c^{\langle t\rangle})^2\big)*c^{\langle t-1\rangle}*da^{\langle t\rangle} \]</span></p></li><li><p>参数的梯度 <span class="math display">\[ dW_f = d\Gamma_f^{\langle t\rangle}*\Gamma_f^{\langle t\rangle}*(1-\Gamma_f^{\langle t\rangle})\begin{bmatrix} a^{\langle t-1\rangle} \\\ x^{\langle t\rangle}\end{bmatrix}^T \]</span></p><p><span class="math display">\[ dW_u=d\Gamma_u^{\langle t \rangle}*\Gamma_u^{\langle t\rangle}*(1-\Gamma_u^{\langle t\rangle})*\begin{bmatrix} a^{\langle t-1\rangle} \\\ x^{\langle t\rangle}\end{bmatrix}^T \]</span></p><p><span class="math display">\[ dW_c=d\tilde c^{\langle t \rangle}*(1-\tilde c^{\langle t\rangle 2})*\begin{bmatrix} a^{\langle t-1\rangle} \\\ x^{\langle t\rangle}\end{bmatrix}^T \]</span></p><p><span class="math display">\[ dW_o=d\Gamma_o^{\langle t\rangle}*\Gamma_o^{\langle t\rangle}*(1-\Gamma_o^{\langle t\rangle})*\begin{bmatrix} a^{\langle t-1\rangle} \\\ x^{\langle t\rangle}\end{bmatrix}^T \]</span></p></li></ul><p><span class="math inline">\(b_f, b_u, b_c, b_o\)</span> 的梯度只需要将 <span class="math inline">\(\Gamma_f^{\langle t\rangle}, \Gamma_u^{\langle t\rangle}, \tilde c^{\langle t\rangle}, \Gamma_o^{\langle t\rangle}\)</span> 的梯度沿水平方向 (axis=1) 累加即可，当前时间步的输入、上一个时间步的细胞状态和隐藏状态的梯度如下所示： <span class="math display">\[ \begin{align} da^{\langle t-1\rangle} &amp;= W_f^T*d\Gamma_f^{\langle t\rangle}*\Gamma_f^{\langle t\rangle}*(1-\Gamma_f^{\langle t\rangle}) \\\ &amp;+ W_u^T * d\Gamma_u^{\langle t \rangle}*\Gamma_u^{\langle t\rangle}*(1-\Gamma_u^{\langle t\rangle}) \\\ &amp;+ W_c^T * d\tilde c^{\langle t \rangle}*(1-\tilde c^{\langle t\rangle 2}) \\\ &amp;+ W_o^T * d\Gamma_o^{\langle t\rangle}*\Gamma_o^{\langle t\rangle}*(1-\Gamma_o^{\langle t\rangle}) \end{align} \]</span></p><p><span class="math display">\[ dc^{\langle t-1\rangle} = dc^{\langle t\rangle}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c^{\langle t-1\rangle})^2)*\Gamma_f^{\langle t \rangle}*da^{\langle t\rangle} \]</span></p><p><span class="math display">\[ \begin{align} dx^{\langle t \rangle} &amp;= W_f^T*d\Gamma_f^{\langle t\rangle}*\Gamma_f^{\langle t\rangle}*(1-\Gamma_f^{\langle t\rangle}) \\\ &amp;+ W_u^T * d\Gamma_u^{\langle t \rangle}*\Gamma_u^{\langle t\rangle}*(1-\Gamma_u^{\langle t\rangle}) \\\ &amp;+ W_c^T * d\tilde c^{\langle t \rangle}*(1-\tilde c^{\langle t\rangle 2}) \\\ &amp;+ W_o^T * d\Gamma_o^{\langle t\rangle}*\Gamma_o^{\langle t\rangle}*(1-\Gamma_o^{\langle t\rangle}) \end{align} \]</span></p><p>DeepLearning 的目前最新版本作业中的公式和代码的表示有些小问题，这里已经修正。其代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next, dc_next, cache)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (a_next, c_next, a_prev, c_prev, ft, ut, cct, ot, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from xt's and a_next's shape</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_a, m = a_next.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10)</span></span><br><span class="line">    dot = da_next * np.tanh(c_next)</span><br><span class="line">    dcct = (dc_next * ut + ot * (<span class="number">1</span> - np.square(np.tanh(c_next))) * ut * da_next)</span><br><span class="line">    dut = (dc_next * cct + ot * (<span class="number">1</span> - np.square(np.tanh(c_next))) * cct * da_next)</span><br><span class="line">    dft = (dc_next * c_prev + ot * (<span class="number">1</span> - np.square(np.tanh(c_next))) * c_prev * da_next)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute parameters related derivatives. Use equations (11)-(14)</span></span><br><span class="line">    concat = np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T</span><br><span class="line">    dWf = np.dot(dft * ft * (<span class="number">1</span> - ft), concat)</span><br><span class="line">    dWu = np.dot(dut * ut * (<span class="number">1</span> - ut), concat)</span><br><span class="line">    dWc = np.dot(dcct * (<span class="number">1</span> - np.square(cct)), concat)</span><br><span class="line">    dWo = np.dot(dot * ot * (<span class="number">1</span> - ot), concat)</span><br><span class="line">    dbf = np.sum(dft * ft * (<span class="number">1</span> - ft), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)  </span><br><span class="line">    dbu = np.sum(dut * ut * (<span class="number">1</span> - ut), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)  </span><br><span class="line">    dbc = np.sum(dcct * (<span class="number">1</span> - np.square(cct)), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)  </span><br><span class="line">    dbo = np.sum(dot * ot * (<span class="number">1</span> - ot),axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17).</span></span><br><span class="line">    da_prev = np.dot(parameters[<span class="string">"Wf"</span>][:, :n_a].T, dft * ft * (<span class="number">1</span> - ft)) + np.dot(parameters[<span class="string">"Wc"</span>][:, :n_a].T, dcct * (<span class="number">1</span> - np.square(cct))) + np.dot(parameters[<span class="string">"Wu"</span>][:, :n_a].T, dut * ut * (<span class="number">1</span> - ut)) + np.dot(parameters[<span class="string">"Wo"</span>][:, :n_a].T, dot * ot * (<span class="number">1</span> - ot))</span><br><span class="line">    dc_prev = dc_next*ft+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ft*da_next</span><br><span class="line">    dxt = np.dot(parameters[<span class="string">"Wf"</span>][:, n_a:].T, dft * ft * (<span class="number">1</span> - ft)) + np.dot(parameters[<span class="string">"Wc"</span>][:, n_a:].T, dcct * (<span class="number">1</span> - np.square(cct))) + np.dot(parameters[<span class="string">"Wu"</span>][:, n_a:].T, dut * ut * (<span class="number">1</span> - ut)) + np.dot(parameters[<span class="string">"Wo"</span>][:, n_a:].T, dot * ot * (<span class="number">1</span> - ot))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save gradients in dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dc_prev"</span>: dc_prev, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWu"</span>: dWu,<span class="string">"dbu"</span>: dbu,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="lstm-反向传播">LSTM 反向传播</h3><p>类似于 RNN 的反向传播，最后一个时间步的细胞状态和隐藏状态的梯度为 0，其代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches.</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈12 lines)</span></span><br><span class="line">    dx = np.zeros([n_x, m, T_x])</span><br><span class="line">    da0 = np.zeros([n_a, m])</span><br><span class="line">    da_prevt = np.zeros([n_a, m])</span><br><span class="line">    dc_prevt = np.zeros([n_a, m])</span><br><span class="line">    dWf = np.zeros([n_a, n_a + n_x])</span><br><span class="line">    dWu = np.zeros([n_a, n_a + n_x])</span><br><span class="line">    dWc = np.zeros([n_a, n_a + n_x])</span><br><span class="line">    dWo = np.zeros([n_a, n_a + n_x])</span><br><span class="line">    dbf = np.zeros([n_a, <span class="number">1</span>])</span><br><span class="line">    dbu = np.zeros([n_a, <span class="number">1</span>])</span><br><span class="line">    dbc = np.zeros([n_a, <span class="number">1</span>])</span><br><span class="line">    dbo = np.zeros([n_a, <span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop back over the whole sequence</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute all gradients using lstm_cell_backward</span></span><br><span class="line">        gradients = lstm_cell_backward(da[:,:,t],dc_prevt,caches[t])</span><br><span class="line">        <span class="comment"># Store or add the gradient to the parameters' previous step's gradient</span></span><br><span class="line">        dx[:,:,t] = gradients[<span class="string">'dxt'</span>]</span><br><span class="line">        dWf = dWf+gradients[<span class="string">'dWf'</span>]</span><br><span class="line">        dWu = dWu+gradients[<span class="string">'dWu'</span>]</span><br><span class="line">        dWc = dWc+gradients[<span class="string">'dWc'</span>]</span><br><span class="line">        dWo = dWo+gradients[<span class="string">'dWo'</span>]</span><br><span class="line">        dbf = dbf+gradients[<span class="string">'dbf'</span>]</span><br><span class="line">        dbu = dbu+gradients[<span class="string">'dbu'</span>]</span><br><span class="line">        dbc = dbc+gradients[<span class="string">'dbc'</span>]</span><br><span class="line">        dbo = dbo+gradients[<span class="string">'dbo'</span>]</span><br><span class="line">    <span class="comment"># Set the first activation's gradient to the backpropagated gradient da_prev.</span></span><br><span class="line">    da0 = gradients[<span class="string">'da_prev'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWu"</span>: dWu,<span class="string">"dbu"</span>: dbu,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h2 id="激活函数选择">激活函数选择</h2><ul><li>在 RNN 中，使用 <span class="math inline">\(tanh\)</span> 函数作为激活函数是因为 RNN 主要存在梯度消失问题。相比于 <span class="math inline">\(sigmoid\)</span> 激活函数，<span class="math inline">\(tanh\)</span> 函数的二阶导数在 0 之前持续很长的范围，更加有利于保持梯度在激活函数的线性区域。</li><li>在 RNN 中，参数 <span class="math inline">\(W_{ax}\)</span> 和 <span class="math inline">\(W_{aa}\)</span> 参与了每个时间步的运算，即使使用 <span class="math inline">\(ReLU\)</span> 函数作为激活函数，当参数 <span class="math inline">\(W\)</span> 小于 1 时也会产生梯度消失问题，而且 <span class="math inline">\(ReLU\)</span> 函数还会导致模型的输出过大，<span class="math inline">\(tanh\)</span> 函数则可以控制输出范围在 <span class="math inline">\((-1, 1)\)</span>。</li><li>在 LSTM 的门单元中，使用 <span class="math inline">\(sigmoid\)</span> 函数作为激活函数是因为要保证门的输出是一个 0，1 之间的向量，例如遗忘门 的输出表示让 <span class="math inline">\(c^{\langle t-1\rangle}\)</span> 各部分信息通过的比例，0 表示不让任何信息通过，1 表示让所有信息通过。</li></ul><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li><li>Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. 人民邮电出版社. 2017.</li><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" target="_blank" rel="noopener">Understanding LSTM Networks</a></li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2018/06/20/recurrent-neural-network/" title="循环神经网络">https://pengzhendong.cn/2018/06/20/recurrent-neural-network/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/06/07/batch-normalization/" rel="prev" title="批归一化"><i class="fa fa-chevron-left"></i> 批归一化</a></div><div class="post-nav-item"> <a href="/2018/06/27/character-level-language-model/" rel="next" title="字符级别的语言模型">字符级别的语言模型<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络"><span class="nav-number">2.</span> <span class="nav-text">循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播"><span class="nav-number">3.</span> <span class="nav-text">前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn-细胞"><span class="nav-number">3.1.</span> <span class="nav-text">RNN 细胞</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn-前向传播"><span class="nav-number">3.2.</span> <span class="nav-text">RNN 前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#长期依赖"><span class="nav-number">3.3.</span> <span class="nav-text">长期依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lstm-细胞"><span class="nav-number">3.4.</span> <span class="nav-text">LSTM 细胞</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#遗忘门"><span class="nav-number">3.4.1.</span> <span class="nav-text">遗忘门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#更新门"><span class="nav-number">3.4.2.</span> <span class="nav-text">更新门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#输出门"><span class="nav-number">3.4.3.</span> <span class="nav-text">输出门</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lstm-前向传播"><span class="nav-number">3.5.</span> <span class="nav-text">LSTM 前向传播</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">4.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简单版-rnn-细胞"><span class="nav-number">4.1.</span> <span class="nav-text">简单版 RNN 细胞</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn-反向传播"><span class="nav-number">4.2.</span> <span class="nav-text">RNN 反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#完整版-rnn-细胞"><span class="nav-number">4.3.</span> <span class="nav-text">完整版 RNN 细胞</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lstm-细胞-1"><span class="nav-number">4.4.</span> <span class="nav-text">LSTM 细胞</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lstm-反向传播"><span class="nav-number">4.5.</span> <span class="nav-text">LSTM 反向传播</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数选择"><span class="nav-number">5.</span> <span class="nav-text">激活函数选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>