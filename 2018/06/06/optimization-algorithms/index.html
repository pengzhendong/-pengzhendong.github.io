<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 单单通过特征缩放提高梯度下降的收敛速度并不够，有时候还需要改进梯度下降算法。例如动量梯度下降 (Grandient descent with Momentum)、RMSprop 算法和 Adam 优化算法(Adam optimization algorithm)。"><meta name="keywords" content="Machine Learning"><meta property="og:type" content="article"><meta property="og:title" content="优化算法"><meta property="og:url" content="https://pengzhendong.cn/2018/06/06/optimization-algorithms/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 单单通过特征缩放提高梯度下降的收敛速度并不够，有时候还需要改进梯度下降算法。例如动量梯度下降 (Grandient descent with Momentum)、RMSprop 算法和 Adam 优化算法(Adam optimization algorithm)。"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2018-06-06T04:02:28.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="优化算法"><meta name="twitter:description" content="前言 单单通过特征缩放提高梯度下降的收敛速度并不够，有时候还需要改进梯度下降算法。例如动量梯度下降 (Grandient descent with Momentum)、RMSprop 算法和 Adam 优化算法(Adam optimization algorithm)。"><link rel="canonical" href="https://pengzhendong.cn/2018/06/06/optimization-algorithms/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>优化算法 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/06/06/optimization-algorithms/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 优化算法</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-06-06 10:34:00 / 修改时间：12:02:28" itemprop="dateCreated datePublished" datetime="2018-06-06T10:34:00+08:00">2018-06-06</time></span><span id="/2018/06/06/optimization-algorithms/" class="post-meta-item leancloud_visitors" data-flag-title="优化算法" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>单单通过特征缩放提高梯度下降的收敛速度并不够，有时候还需要改进梯度下降算法。例如动量梯度下降 (Grandient descent with Momentum)、RMSprop 算法和 Adam 优化算法(Adam optimization algorithm)。</p><a id="more"></a><h2 id="动量梯度下降法">动量梯度下降法</h2><p>在大数据时代，使用批量梯度下降会非常耗时，而小批量梯度下降每次只使用小批量的数据，在梯度下降过程中并不是每次迭代都向着整体最优化的方向。动量梯度下降法 (Gradient descent with Momentum) 则可以帮助梯度下降尽可能保持向着整体最优化的方向，可以加速算法的收敛，动量梯度下降法使用指数加权平均，在计算当前梯度的同时也使用了之前迭代过程的梯度。</p><h3 id="指数加权平均">指数加权平均</h3><p><a href="https://zh.wikipedia.org/wiki/%E7%A7%BB%E5%8B%95%E5%B9%B3%E5%9D%87#%E6%8C%87%E6%95%B8%E7%A7%BB%E5%8B%95%E5%B9%B3%E5%9D%87" target="_blank" rel="noopener">指数加权平均</a>（Exponential weighted average，简称 EXWA 或者 EWA）也叫指数移动平均（Exponential moving average，简称 EXMA 或者 EMA），是以指数式递减加权的移动平均。各数值的加权影响力随时间而指数式递减，越近期的数据加权影响力越重，但较旧的数据也给予一定的加权值。 <span class="math display">\[ v_t=\beta v_{t-1}+(1-\beta)\theta_t \]</span> 其中 <span class="math inline">\(v_t\)</span> 表示指数加权平均值 (<span class="math inline">\(v_0=0\)</span>)，<span class="math inline">\(\theta_t\)</span> 表示当前数据，<span class="math inline">\(\beta\)</span> 为参数，其取值范围为 <span class="math inline">\([0, 1)\)</span>。例如 <span class="math inline">\(\beta=0.9\)</span>，有： <span class="math display">\[ \begin{align} v_{100} &amp;= (1-\beta)\theta_{100}+(1-\beta)\beta\theta_{99}+(1-\beta)\beta^2\theta_{98}+...+(1-\beta)\beta^{99}\theta_{1} \\\ &amp;= 0.1\theta_{100}+(0.1\times 0.9)\theta_{99}+(0.1\times 0.9^2)\theta_{98}+...+(0.1\times 0.9^{99})\theta_{1} \end{align} \]</span> 越旧的数据权值越小，如何给 <span class="math inline">\(\beta\)</span> 一个直观的感觉呢？当权值 <span class="math inline">\(\beta^n\)</span> 小于 <span class="math inline">\(\frac{1}{e}\)</span> 就可以说只关注了前 <span class="math inline">\(n\)</span> 个数据，因为更旧的数据权值只有不到 <span class="math inline">\(\frac{1}{e}\)</span>： <span class="math display">\[ \lim_{x \to 0}(1+x)^{-\frac{1}{x}}=\frac{1}{e} \]</span> 令 <span class="math inline">\(x=\beta-1\)</span>，得 <span class="math display">\[ \lim_{\beta \to 1}(\beta)^{\frac{1}{1-\beta}}=\frac{1}{e} \]</span></p><p>因此可以<strong>简单地</strong>认为 <span class="math inline">\(v_t\)</span> 是前 <span class="math inline">\(\frac{1}{1-\beta}\)</span> 个数据的指数加权平均(并不是严格的数学证明)。<span class="math inline">\(\beta\)</span> 越大表示当前数据所占的权值越小，即求出来的平均值对当前数据越不敏感，则曲线越平坦。</p><h4 id="偏差修正">偏差修正</h4><p>由于默认 <span class="math inline">\(v_0=0\)</span>，所以对一开始的数据计算移动平均数作为估计就不太准确。可以用 <span class="math inline">\(\frac{v_{t}}{1- \beta^{t}}\)</span> 作为估计值，当随着 <span class="math inline">\(t\)</span> 增加，<span class="math inline">\(\beta^{t}\)</span> 接近于 0。</p><h3 id="动量梯度下降">动量梯度下降</h3><p>动量梯度下降的基本想法就是计算梯度的指数加权平均数，并利用该梯度更新权重。在每次迭代中，参数的更新公式如下所示：</p><p><span class="math display">\[ v_{dW}=\beta v_{dW}+(1-\beta)dW \]</span></p><p><span class="math display">\[ W=W-\alpha v_{dW} \]</span></p><p>使用动量梯度下降法，由于每次都尽量朝着整体最优化的方向更新参数，所以算法的速度回比较快。</p><h2 id="rmsprop">RMSprop</h2><p>RMSprop(root mean square prop) 算法，也可以加速梯度下降。如上图 without momentum 中，<span class="math inline">\(w_2\)</span> 方向上的梯度要大于 <span class="math inline">\(w_1\)</span> 方向上的梯度(因为 <span class="math inline">\(w_2\)</span> 方向上一步跨得比较大，<span class="math inline">\(W=W-\alpha dW\)</span>)，RMSprop 算法通过让学习率除以一个衰减系数(历史梯度平方和的平方根)，使得每个参数的学习率不同。在参数空间更为平缓的方向(衰减系数较小)，获得更大的步伐，从而加快训练速度。 <span class="math display">\[ S_{dW}= \beta S_{dW} + (1 - \beta)({dW})^{2} \]</span></p><p><span class="math display">\[ W=W-\frac{\alpha}{\sqrt{S_{dw}}+\varepsilon}dW \]</span></p><p>其中 <span class="math inline">\(\varepsilon\)</span> 是为了避免分母为 0。</p><h2 id="adam-优化算法">Adam 优化算法</h2><p>Adam(Adaptive Moment Estimation) 优化算法基本上就是将 Momentum 和 RMSprop 结合在一起： <span class="math display">\[ v_{dW}= \beta_{1}v_{dW} + ( 1 - \beta_{1})dW \]</span></p><p><span class="math display">\[ S_{dW}=\beta_{2}S_{dW} + ( 1 - \beta_{2}){(dW)}^{2} \]</span></p><p>偏差修正： <span class="math display">\[ v_{dW}^{\text{corrected}}= \frac{v_{dW}}{1 - \beta_{1}^{t}} \]</span></p><p><span class="math display">\[ S_{dW}^{\text{corrected}} =\frac{S_{dW}}{1 - \beta_{2}^{t}} \]</span></p><p>权值更新： <span class="math display">\[ W= W - \frac{\alpha}{\sqrt{S_{dW}^{\text{corrected}}} +\varepsilon}v_{dW}^{\text{corrected}} \]</span> Adam 算法结合了 Momentum 和 RMSprop 梯度下降法，是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。其中超参数学习率 <span class="math inline">\(\alpha\)</span> 很重要，也经常需要调试；<span class="math inline">\(\beta_{1}\)</span>常用的缺省值为 0.9；Adam 论文作者推荐使用 0.999 作为超参数 <span class="math inline">\(\beta_{2}\)</span> 的默认值；<span class="math inline">\(\varepsilon\)</span> 建议为 <span class="math inline">\(10^{-8}\)</span>。但是在使用 Adam 的时候，人们往往使用缺省值即可。</p><h2 id="学习率衰减">学习率衰减</h2><p>随时间慢慢减少学习率也可以加快学习算法，我们将之称为学习率衰减。因为在学习初期，模型可以承受较大的步伐，当开始收敛的时候，则需要逐渐减小步伐，否则容易错过最优值。</p><p>使用小批量梯度下降进行训练，每遍历一次训练集称为一个 <code>epoch</code> (一代)，学习率可以随着 epoch 的变大而减小： <span class="math display">\[ \alpha=\frac{1}{1+\text{decay_rate}*\text{epoch_num}}\alpha_0 \]</span> 其中 decay_rate 是衰减率(需要调整的超参数)，epoch_num 是代数，<span class="math inline">\(\alpha_0\)</span> 是出事学习率。除了这个公式，还可以用其他的公式使学习率递减或者通过手动的方式调整学习率： <span class="math display">\[ \alpha=0.95^\text{epoch_num}\alpha_0 \]</span></p><p><span class="math display">\[ \alpha=\frac{k}{\sqrt{\text{epoch_num}}}\alpha_0 \]</span></p><p><span class="math display">\[ \alpha=\frac{k}{\sqrt{t}}\alpha_0 \]</span></p><h2 id="局部最优问题">局部最优问题</h2><p>在深度学习研究早期，人们总是担心优化算法会被困在一些局部最优点处。而事实上，在神经网络中上图所示的局部最优点出现的可能性很小，梯度为 0 时，通常是<strong>鞍点</strong>，因为代价函数梯度为 0 时，那么在每个方向(权值)上，它可能是凸函数，也可能是凹函数。在一个 <span class="math inline">\(n\)</span> 维的高维空间中，如果想要得到局部最优，那么 <span class="math inline">\(n\)</span> 个方向上都需要一样，发生这种情况的概率是 <span class="math inline">\(\frac{1}{2^n}\)</span>。而大部分情况却是一部分方向是凸函数，一部分方向是凹函数，即鞍点。但问题是在鞍点处，会有平稳段，即曲面很平坦，下降速度慢，而 Adam 算法正好可以加快速度，尽早走出平稳段。</p><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li><li><a href="https://jermwatt.github.io/mlrefined/blog_posts/13_Multilayer_perceptrons/13_5_Momentum_methods.html" target="_blank" rel="noopener">Momentum methods</a></li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2018/06/06/optimization-algorithms/" title="优化算法">https://pengzhendong.cn/2018/06/06/optimization-algorithms/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/05/31/feature-scaling/" rel="prev" title="特征缩放"><i class="fa fa-chevron-left"></i> 特征缩放</a></div><div class="post-nav-item"> <a href="/2018/06/07/batch-normalization/" rel="next" title="批归一化">批归一化<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#动量梯度下降法"><span class="nav-number">2.</span> <span class="nav-text">动量梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#指数加权平均"><span class="nav-number">2.1.</span> <span class="nav-text">指数加权平均</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差修正"><span class="nav-number">2.1.1.</span> <span class="nav-text">偏差修正</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#动量梯度下降"><span class="nav-number">2.2.</span> <span class="nav-text">动量梯度下降</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rmsprop"><span class="nav-number">3.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adam-优化算法"><span class="nav-number">4.</span> <span class="nav-text">Adam 优化算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习率衰减"><span class="nav-number">5.</span> <span class="nav-text">学习率衰减</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#局部最优问题"><span class="nav-number">6.</span> <span class="nav-text">局部最优问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">7.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>