<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 花了一些时间填补了 WAV 文件的基础知识和快速傅里叶变换算法的内容。终于可以继续学习深度学习啦！前段时间买了个小米的小爱同学，用来睡前关灯还是挺方便的，这次的实验就是研究小爱同学究竟是如何被唤醒的。"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="触发词检测"><meta property="og:url" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 花了一些时间填补了 WAV 文件的基础知识和快速傅里叶变换算法的内容。终于可以继续学习深度学习啦！前段时间买了个小米的小爱同学，用来睡前关灯还是挺方便的，这次的实验就是研究小爱同学究竟是如何被唤醒的。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/voice.png"><meta property="og:image" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/wav.png"><meta property="og:image" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/spectrogram.png"><meta property="og:image" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/label_diagram.png"><meta property="og:image" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/output1.png"><meta property="og:image" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/model.png"><meta property="og:image" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/output2.png"><meta property="og:updated_time" content="2018-11-15T13:22:49.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="触发词检测"><meta name="twitter:description" content="前言 花了一些时间填补了 WAV 文件的基础知识和快速傅里叶变换算法的内容。终于可以继续学习深度学习啦！前段时间买了个小米的小爱同学，用来睡前关灯还是挺方便的，这次的实验就是研究小爱同学究竟是如何被唤醒的。"><meta name="twitter:image" content="https://pengzhendong.cn/2018/11/15/trigger-word-detection/voice.png"><link rel="canonical" href="https://pengzhendong.cn/2018/11/15/trigger-word-detection/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>触发词检测 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/11/15/trigger-word-detection/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 触发词检测</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-11-15 20:16:00 / 修改时间：21:22:49" itemprop="dateCreated datePublished" datetime="2018-11-15T20:16:00+08:00">2018-11-15</time></span><span id="/2018/11/15/trigger-word-detection/" class="post-meta-item leancloud_visitors" data-flag-title="触发词检测" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>花了一些时间填补了 WAV 文件的基础知识和快速傅里叶变换算法的内容。终于可以继续学习深度学习啦！前段时间买了个小米的小爱同学，用来睡前关灯还是挺方便的，这次的实验就是研究小爱同学究竟是如何被唤醒的。</p><a id="more"></a><h2 id="触发词检测">触发词检测</h2><p>这次的任务是收集语音数据集并且实现触发词（也称关键字或者唤醒字）检测。举个例子：小米的小爱同学，在检测到触发词“小爱同学”后，就会被唤醒。这里的触发词是 “activate”。</p><h3 id="数据合成创建语音数据集">数据合成：创建语音数据集</h3><p>在真实场景中，还会有其他声音，例如负面词（一些其他的词）和环境背景噪声（会和触发词混合在一起出现）。很难收集大量的音频，通常都是单独下载背景噪声然后将触发词、负面词与背景噪声混合。</p><ul><li>正面词</li></ul><center><audio controls controlslist="nodownload"><source src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/activate.wav" type="audio/mpeg"> Your browser does not support the audio element.</audio></center><ul><li>负面词</li></ul><center><audio controls controlslist="nodownload"><source src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/negative.wav" type="audio/mpeg"> Your browser does not support the audio element.</audio></center><ul><li>背景噪声</li></ul><center><audio controls controlslist="nodownload"><source src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/background.wav" type="audio/mpeg"> Your browser does not support the audio element.</audio></center><h4 id="波形图">波形图</h4><p>声音是弹性介质中压力变化形式的机械能，这些压力变化来自振动源的波传播。声音在介质中传播时，会造成介质的压缩和稀疏，从而引起原有环境压强的变化。压缩是比环境压力更高的时段，稀疏是压力低于环境压力的时段。</p><p><img src="/2018/11/15/trigger-word-detection/voice.png"></p><p>波形图 (Waveform) 如上所示，总压强等于环境静态压强（即标准大气压 <span class="math inline">\(P\)</span>: <span class="math inline">\(10^5\)</span> Pa）加上声音扰动带来的动态压强（即声压 <span class="math inline">\(P_A\)</span>）。① 时段声压为 0 即为无声阶段；② 时段有声音扰动；③ 为大气压；④ 为瞬时声压。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> wavfile</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">fs = <span class="number">44100</span></span><br><span class="line">rate, data = wavfile.read(<span class="string">'audio_examples/example_train.wav'</span>)</span><br><span class="line">time = np.linspace(<span class="number">0</span>, len(data)/fs, num=len(data))</span><br><span class="line">plt.xlabel(<span class="string">'time/s'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$P_A$'</span>)</span><br><span class="line">plt.plot(time, data) </span><br><span class="line"> </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/11/15/trigger-word-detection/wav.png"></p><p>这里的 WAV 文件使用的是 16 位量化数字，因此声压的取值范围是 (-32768, 32767)，音频时长为 10 秒。波形图看不出声音的特征，因此需要对其使用傅里叶变换，得到频谱图再分析声音的特征。</p><h2 id="波形图-1">波形图</h2><p>通过傅立叶变换可以得到信号的频谱，傅立叶变换有一个假设就是信号是平稳的，即信号的统计特性不随时间变化。声音信号就不是平稳信号，在很长的一段时间内，有很多信号会出现，然后立即消失。如果将这信号全部进行傅立叶变换，就不能反映声音随时间的变化。</p><h3 id="短时傅里叶变换">短时傅里叶变换</h3><p>声音信号虽然不是平稳信号，但在较短的一段时间内可以看作是平稳的，所以解决方案是取一小段进行傅立叶变换，即短时傅立叶变换（Short-time Fourier transform）。</p><h4 id="窗函数">窗函数</h4><p>从一段长的信号截取一段信号（通常在0.02~0.05s，称为一帧），相当于将原始信号乘以一个方窗，而方窗的傅里叶变换并不是理想的冲击函数，所以用 <code>sinc</code> 函数，<code>sinc</code> 较高的副瓣意味着在真实频点以外，副瓣的位置上的频谱也会不为零。如果在副瓣的位置上恰好有一个幅度很小的信号，就会被完全淹没。解决方案是使用窗函数，代替简单地截取一段信号，在窗的边缘，信号会乘上一个很小的数。这又会导致边缘数据并没有充分被利用，两个相邻窗之间的信号没有完全反映到频谱当中。因此解决办法是两个相邻的窗有一定的重叠，同时如果需要恢复成为时间序列，也能弥补窗函数带来的影响。总之一句话就是取一小段信号进行短时傅里叶变换会导致数据丢失，因此两个相邻的窗口之间需要有一定的重叠，重叠取加可以选择为窗长度的 50% 或者 25%。</p><h4 id="声谱图">声谱图</h4><p>语音的时域分析和频域分析是两种语音分析方法，但是这两种分析方法都有局限性。时域分析对语音信号的频率没有直观的了解，而频域分析出的特征中又没有语音信号随时间变化的关系。<strong>声谱图</strong>（语谱图）是一种三位频谱，表示语音频谱随着时间变化的图形。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> wavfile</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">rate, data = wavfile.read(<span class="string">'audio_examples/example_train.wav'</span>)</span><br><span class="line">nfft = <span class="number">200</span>     <span class="comment"># 截取的信号长度，即窗长</span></span><br><span class="line">fs = <span class="number">44100</span>     <span class="comment"># 采样频率</span></span><br><span class="line">noverlap = <span class="number">120</span> <span class="comment"># 重叠长度</span></span><br><span class="line">pxx, freqs, bins, im = plt.specgram(data[:,<span class="number">0</span>], nfft, fs, noverlap = noverlap)</span><br></pre></td></tr></table></figure><p><img src="/2018/11/15/trigger-word-detection/spectrogram.png"></p><p>横坐标为时间，纵坐标为频率。任意给定频率成分，在给定时刻的强弱用相应点的色调的浓淡来表示，颜色越深表示语音能量越强（声音更加响亮）。10 秒音频输出 <code>pxx</code> 的时间步长度为：<span class="math inline">\(5511=\frac{10\times fs-nfft}{nfft-overlap}+1\)</span>，在原始音频中一共有 441000 个时间步，而在声谱图中一共有 5511 个时间步，因此前者每个时间步代表 0.000023 秒，后者代表 0.0018 秒。也就是说 10 秒的时间可以被离散成不同的数值，例如 GRU 的输出离散成 1375 个时间步，也就是每个时间步 0.0072 秒，模型的输出就表示这个 0.0072 秒内是否有人说过触发词。</p><h2 id="生成训练示例">生成训练示例</h2><p>为了合成一个训练样本，需要：</p><ul><li>随机选择一个 10 秒的背景音频剪辑</li><li>随机将 0-4 个正面音频片段插入此 10 秒剪辑中</li><li>随机将 0-2 个反面音频片段插入此 10 秒剪辑中</li></ul><p>通常使用 <code>pydub</code> 来处理音频。 Pydub 将原始音频文件转换为 Pydub 数据结构列表，使用 1ms 作为离散化间隔，因此 10 秒剪辑一共有 10,000 个时间步。我们希望在背景噪声中插入多个触发词和负面，同时不希望这些词重叠（声音合成而不是声音拼接，最终输出音频还是 10 秒）。</p><p>首先初始化背景噪声的标签，因为里面还没有触发词，所以对于所有的 <span class="math inline">\(t\)</span>，有 <span class="math inline">\(y^{\langle t \rangle}=0\)</span>。在插入触发词的时候，还需要更新标签 <span class="math inline">\(y^{\langle t \rangle}\)</span>。假设在第 5 秒的时候插入了触发词（即输出的第 <span class="math inline">\(687=int(1375\times\frac{5}{10})\)</span> 个时间步），那么我们希望模型在接下来一小段时间内能检测到就行，我们选择 50 个时间步，也就是 <span class="math inline">\(y^{\langle 688 \rangle} = y^{\langle 689 \rangle} = \cdots = y^{\langle 737 \rangle} = 1\)</span>。如下图所示，每个触发词后 50 个时间步的标签都是 1：</p><p><img src="/2018/11/15/trigger-word-detection/label_diagram.png"></p><p>合成训练数据还有一个好处就是容易生成标签，如果在录制声音的时候手动标记是非常耗时的。</p><h3 id="辅助函数">辅助函数</h3><p>为了实现训练集的合成，还需要以下辅助函数，这些函数都使用 1 毫秒离散化间隔，即 10 秒的音频总是被离散化成 10,000 步。</p><ol type="1"><li><p><code>get_random_time_segment(segment_ms)</code> 从背景音频中选择指定长度的随机时间片段；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_time_segment</span><span class="params">(segment_ms)</span>:</span></span><br><span class="line">    segment_start = np.random.randint(low=<span class="number">0</span>, high=<span class="number">10000</span>-segment_ms) <span class="comment"># 防止超出 10s</span></span><br><span class="line">    segment_end = segment_start + segment_ms - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (segment_start, segment_end)</span><br></pre></td></tr></table></figure></li><li><p><code>is_overlapping(segment_time, existing_segments)</code> 判断时间片是否与先前的时间片重叠；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_overlapping</span><span class="params">(segment_time, previous_segments)</span>:</span></span><br><span class="line">    segment_start, segment_end = segment_time</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Initialize overlap as a "False" flag.</span></span><br><span class="line">    overlap = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: loop over the previous_segments start and end times.</span></span><br><span class="line">    <span class="comment"># Compare start/end times and set the flag to True if there is an overlap.</span></span><br><span class="line">    <span class="keyword">for</span> previous_start, previous_end <span class="keyword">in</span> previous_segments:</span><br><span class="line">        <span class="keyword">if</span> segment_start &lt;= previous_end <span class="keyword">and</span> segment_end &gt;= previous_start:</span><br><span class="line">            overlap = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> overlap</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">overlap1 = is_overlapping((<span class="number">950</span>, <span class="number">1430</span>), [(<span class="number">2000</span>, <span class="number">2550</span>), (<span class="number">260</span>, <span class="number">949</span>)])</span><br><span class="line">overlap2 = is_overlapping((<span class="number">2305</span>, <span class="number">2950</span>), [(<span class="number">824</span>, <span class="number">1532</span>), (<span class="number">1900</span>, <span class="number">2305</span>), (<span class="number">3424</span>, <span class="number">3656</span>)])</span><br><span class="line">assertFalse(overlap1)</span><br><span class="line">assertTrue(overlap2)</span><br></pre></td></tr></table></figure></li><li><p><code>insert_audio_clip(background, audio_clip, existing_times)</code> 使用上述两个辅助函数在背景音频的随机时间处插入一个音频时间片，需要完成 4 步：</p><ol type="1"><li>以毫秒为单位随机选择时间片；</li><li>确保时间片与先前的时间片都不重叠，否则返回上一个步骤重新选择时间片；</li><li>将新时间片添加到现有时间片列表中，以跟踪插入的所有时间片；</li><li>使用 pydub 将音频重叠在背景噪声中（使用 overlay 函数）。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_audio_clip</span><span class="params">(background, audio_clip, previous_segments)</span>:</span></span><br><span class="line">    <span class="comment"># Get the duration of the audio clip in ms</span></span><br><span class="line">    segment_ms = len(audio_clip)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Use one of the helper functions to pick a random time segment onto which to insert </span></span><br><span class="line">    <span class="comment"># the new audio clip. (≈ 1 line)</span></span><br><span class="line">    segment_time = get_random_time_segment(segment_ms)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep </span></span><br><span class="line">    <span class="comment"># picking new segment_time at random until it doesn't overlap. (≈ 2 lines)</span></span><br><span class="line">    <span class="keyword">while</span> is_overlapping(segment_time, previous_segments):</span><br><span class="line">        segment_time = get_random_time_segment(segment_ms)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line)</span></span><br><span class="line">    previous_segments.append(segment_time)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 4: Superpose audio segment and background</span></span><br><span class="line">    new_background = background.overlay(audio_clip, position = segment_time[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> new_background, segment_time</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">5</span>)</span><br><span class="line">audio_clip, segment_time = insert_audio_clip(backgrounds[<span class="number">0</span>], activates[<span class="number">0</span>], [(<span class="number">3790</span>, <span class="number">4400</span>)])</span><br><span class="line">audio_clip.export(<span class="string">"insert_test.wav"</span>, format=<span class="string">"wav"</span>)</span><br><span class="line">print(<span class="string">"Segment Time: "</span>, segment_time)</span><br><span class="line">IPython.display.Audio(<span class="string">"insert_test.wav"</span>)</span><br></pre></td></tr></table></figure></li></ol><center><audio controls controlslist="nodownload"><source src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/insert_test.wav" type="audio/mpeg"> Your browser does not support the audio element.</audio></center><ol start="4" type="1"><li><p><code>insert_ones(y, segment_end_ms)</code> 在 ”activate” 之后插入 1 到标签向量 <span class="math inline">\(y\)</span> 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_ones</span><span class="params">(y, segment_end_ms)</span>:</span></span><br><span class="line">    <span class="comment"># duration of the background (in terms of spectrogram time-steps)</span></span><br><span class="line">    segment_end_y = int(segment_end_ms * Ty / <span class="number">10000.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add 1 to the correct index in the background label (y)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(segment_end_y + <span class="number">1</span>, segment_end_y + <span class="number">51</span>):</span><br><span class="line">        <span class="keyword">if</span> i &lt; Ty:</span><br><span class="line">            y[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>注意标签一共有 1375 个时间步，因此不能越界。测试一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr1 = insert_ones(np.zeros((<span class="number">1</span>, Ty)), <span class="number">9700</span>)</span><br><span class="line">plt.plot(insert_ones(arr1, <span class="number">4251</span>)[<span class="number">0</span>,:])</span><br><span class="line">print(<span class="string">"sanity checks:"</span>, arr1[<span class="number">0</span>][<span class="number">1333</span>], arr1[<span class="number">0</span>][<span class="number">634</span>], arr1[<span class="number">0</span>][<span class="number">635</span>])</span><br></pre></td></tr></table></figure><p>sanity checks: 0.0 1.0 0.0</p><p><img src="/2018/11/15/trigger-word-detection/output1.png"></p></li></ol><h3 id="生成训练样本">生成训练样本</h3><p>实现 <code>create_training_example()</code> 来生成所有训练样本：</p><ol type="1"><li>将标签向量 <span class="math inline">\(y\)</span> 初始化为零值的 <span class="math inline">\((1，T_y)\)</span> numpy 数组；</li><li>将已存在时间片集合初始化为空列表；</li><li>随机选择 0 至 4 个 “activate” 音频剪辑，并将其插入 10 秒剪辑，记着将标签插入标签向量 <span class="math inline">\(y\)</span> 中的正确位置；</li><li>随机选择 0 到 2 个负面音频片段，并将它们插入 10 秒片段。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_example</span><span class="params">(background, activates, negatives)</span>:</span></span><br><span class="line">    <span class="comment"># Set the random seed</span></span><br><span class="line">    np.random.seed(<span class="number">18</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make background quieter</span></span><br><span class="line">    background = background - <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Initialize y (label vector) of zeros (≈ 1 line)</span></span><br><span class="line">    y = np.zeros((<span class="number">1</span>, Ty))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Initialize segment times as empty list (≈ 1 line)</span></span><br><span class="line">    previous_segments = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Select 0-4 random "activate" audio clips from the entire list of "activates" recordings</span></span><br><span class="line">    number_of_activates = np.random.randint(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">    random_indices = np.random.randint(len(activates), size=number_of_activates)</span><br><span class="line">    random_activates = [activates[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indices]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Loop over randomly selected "activate" clips and insert in background</span></span><br><span class="line">    <span class="keyword">for</span> random_activate <span class="keyword">in</span> random_activates:</span><br><span class="line">        <span class="comment"># Insert the audio clip on the background</span></span><br><span class="line">        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)</span><br><span class="line">        <span class="comment"># Retrieve segment_start and segment_end from segment_time</span></span><br><span class="line">        segment_start, segment_end = segment_time</span><br><span class="line">        <span class="comment"># Insert labels in "y"</span></span><br><span class="line">        y = insert_ones(y, segment_end_ms=segment_end)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select 0-2 random negatives audio recordings from the entire list of "negatives" recordings</span></span><br><span class="line">    number_of_negatives = np.random.randint(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">    random_indices = np.random.randint(len(negatives), size=number_of_negatives)</span><br><span class="line">    random_negatives = [negatives[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indices]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Loop over randomly selected negative clips and insert in background</span></span><br><span class="line">    <span class="keyword">for</span> random_negative <span class="keyword">in</span> random_negatives:</span><br><span class="line">        <span class="comment"># Insert the audio clip on the background </span></span><br><span class="line">        background, _ = insert_audio_clip(background, random_negative, previous_segments)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Standardize the volume of the audio clip </span></span><br><span class="line">    background = match_target_amplitude(background, <span class="number">-20.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Export new training example </span></span><br><span class="line">    file_handle = background.export(<span class="string">"train"</span> + <span class="string">".wav"</span>, format=<span class="string">"wav"</span>)</span><br><span class="line">    print(<span class="string">"File (train.wav) was saved in your directory."</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><h2 id="开发测试集">开发测试集</h2><p>为了测试模型，实验记录了 25 个样本的开发集。虽然训练数据是合成的，但是开发集应该与实际输入具有相同的分布，因此实验手工标记了 25 个 10 秒钟的音频剪辑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load preprocessed training examples</span></span><br><span class="line">X = np.load(<span class="string">"./XY_train/X.npy"</span>)</span><br><span class="line">Y = np.load(<span class="string">"./XY_train/Y.npy"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load preprocessed dev set examples</span></span><br><span class="line">X_dev = np.load(<span class="string">"./XY_dev/X_dev.npy"</span>)</span><br><span class="line">Y_dev = np.load(<span class="string">"./XY_dev/Y_dev.npy"</span>)</span><br></pre></td></tr></table></figure><h2 id="模型">模型</h2><p>实验模型使用一维的卷积层、GRU 层和全连接层，首先载入相关的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model, Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> GRU, Bidirectional, BatchNormalization, Reshape</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br></pre></td></tr></table></figure><h3 id="构建模型">构建模型</h3><p>模型结构如下图所示：</p><p><img src="/2018/11/15/trigger-word-detection/model.png"></p><p>该模型的一个关键步骤是一维卷积步骤，它的输入是 5511 个时间步的频谱，然后输出一个 1375 个时间步的输出。从计算的角度而言，卷积层有助于加速模型，经过卷积层后 GRU 仅处理 1375 个时间步而不是 5511 个时间步。两层 GRU 从左往右读入绪论，然后使用全连接神经网络加 Sigmoid 层对 <span class="math inline">\(y^{\langle t \rangle}\)</span> 进行预测，判断用户是否刚刚说过 “activate”。</p><p>注意：这里使用的是单向 RNN，因为如果想要使用双向 RNN，那么就必须等待整个 10 秒的音频被记录下来后从能确定音频片段是都具有 “activate”。</p><p>可以通过以下 4 个步骤来实现模型：</p><ol type="1"><li><p>使用 <code>Conv1D()</code> 来实现卷积层，有 196 个卷积核，每个卷积核的大小为 15(<code>kernel_size=15</code>)，并且步长为 4；</p></li><li><p>用 <code>X = GRU(units = 128, return_sequences = True)(X)</code> 实现 GRU 层，设置 <code>return_sequences=True</code> 确保所有时间步的隐藏状态都会喂给下一层，同时记得添加 Dropout 和 BatchNorm 层；</p></li><li><p>第二个 GRU 层，和上一个步骤类似，只不过多了一个 Dropout 层；</p></li><li><p>创建全连接层：</p><p><code>X = TimeDistributed(Dense(1, activation = "sigmoid"))(X)</code></p><p>这样全连接层后面就会跟一个 Sigmoid 层，TimeDistributed 可以让每个时间步的全连接层的参数一样。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    X_input = Input(shape = input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: CONV layer</span></span><br><span class="line">    X = Conv1D(<span class="number">196</span>, <span class="number">15</span>, strides=<span class="number">4</span>)(X_input)             <span class="comment"># CONV1D</span></span><br><span class="line">    X = BatchNormalization()(X)                         <span class="comment"># Batch normalization</span></span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)                           <span class="comment"># ReLu activation</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                 <span class="comment"># dropout (use 0.8)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: First GRU Laye</span></span><br><span class="line">    X = GRU(units = <span class="number">128</span>, return_sequences=<span class="literal">True</span>)(X)      <span class="comment"># GRU (use 128 units and return the sequences)</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                 <span class="comment"># dropout (use 0.8)</span></span><br><span class="line">    X = BatchNormalization()(X)                         <span class="comment"># Batch normalization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Second GRU Layer</span></span><br><span class="line">    X = GRU(units = <span class="number">128</span>, return_sequences=<span class="literal">True</span>)(X)      <span class="comment"># GRU (use 128 units and return the sequences)</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                 <span class="comment"># dropout (use 0.8)</span></span><br><span class="line">    X = BatchNormalization()(X)                         <span class="comment"># Batch normalization</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                 <span class="comment"># dropout (use 0.8)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Time-distributed dense layer</span></span><br><span class="line">    X = TimeDistributed(Dense(<span class="number">1</span>, activation = <span class="string">"sigmoid"</span>))(X) <span class="comment"># time distributed  (sigmoid)</span></span><br><span class="line"></span><br><span class="line">    model = Model(inputs = X_input, outputs = X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">model = model(input_shape = (Tx, n_freq))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h3 id="拟合测试模型">拟合&amp;测试模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">opt = Adam(lr=<span class="number">0.0001</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=opt, metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line">model.fit(X, Y, batch_size = <span class="number">5</span>, epochs=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss, acc = model.evaluate(X_dev, Y_dev)</span><br><span class="line">print(<span class="string">"Dev set accuracy = "</span>, acc)</span><br></pre></td></tr></table></figure><p>这个问题的样本不太均衡，因为神经网络很有可能就将所有的数据判断为 0，即不是触发词。因此需要定义更多有用的指标，例如 F1 分数或者 Precision/Recall。</p><h3 id="预测">预测</h3><p>模型训练完成后就可以用来对真实音频进行预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detect_triggerword</span><span class="params">(filename)</span>:</span></span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    x = graph_spectrogram(filename)</span><br><span class="line">    <span class="comment"># the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model</span></span><br><span class="line">    x  = x.swapaxes(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">    predictions = model.predict(x)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    plt.plot(predictions[<span class="number">0</span>,:,<span class="number">0</span>])</span><br><span class="line">    plt.ylabel(<span class="string">'probability'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><p>计算出在每个输出步骤检测到 “activate” 这个词的概率，当概率超过某个阈值时，就可以触发鸣响。此外，在检测到触发词后，对于后面连续的许多值，标签可能都接近于 1，但是我们只想响一次，因此可以设置每 75 个输出时间步最多响一次，类似于计算机视觉的非最大抑制作用。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">chime_file = <span class="string">"audio_examples/chime.wav"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chime_on_activate</span><span class="params">(filename, predictions, threshold)</span>:</span></span><br><span class="line">    audio_clip = AudioSegment.from_wav(filename)</span><br><span class="line">    chime = AudioSegment.from_wav(chime_file)</span><br><span class="line">    Ty = predictions.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># Step 1: Initialize the number of consecutive output steps to 0</span></span><br><span class="line">    consecutive_timesteps = <span class="number">0</span></span><br><span class="line">    <span class="comment"># Step 2: Loop over the output steps in the y</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Ty):</span><br><span class="line">        <span class="comment"># Step 3: Increment consecutive output steps</span></span><br><span class="line">        consecutive_timesteps += <span class="number">1</span></span><br><span class="line">        <span class="comment"># Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed</span></span><br><span class="line">        <span class="keyword">if</span> predictions[<span class="number">0</span>,i,<span class="number">0</span>] &gt; threshold <span class="keyword">and</span> consecutive_timesteps &gt; <span class="number">75</span>:</span><br><span class="line">            <span class="comment"># Step 5: Superpose audio and background using pydub</span></span><br><span class="line">            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*<span class="number">1000</span>)</span><br><span class="line">            <span class="comment"># Step 6: Reset consecutive output steps to 0</span></span><br><span class="line">            consecutive_timesteps = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    audio_clip.export(<span class="string">"chime_output.wav"</span>, format=<span class="string">'wav'</span>)</span><br></pre></td></tr></table></figure><p></p><h4 id="测试例子">测试例子</h4><p>原音频：</p><center><audio controls controlslist="nodownload"><source src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/my_audio.wav" type="audio/mpeg"> Your browser does not support the audio element.</audio></center><p>接下来对该音频进行预测，如果检测到 “activate”，就发出鸣响：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">filename  = <span class="string">"audio_examples/my_audio.wav"</span><span class="string">"</span></span><br><span class="line"><span class="string">prediction = detect_triggerword(filename)</span></span><br><span class="line"><span class="string">chime_on_activate(filename, prediction, 0.5)</span></span><br><span class="line"><span class="string">IPython.display.Audio("</span>./chime_output.wav<span class="string">")</span></span><br></pre></td></tr></table></figure><p><img src="/2018/11/15/trigger-word-detection/output2.png"></p><center><audio controls controlslist="nodownload"><source src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/chime_output.wav" type="audio/mpeg"> Your browser does not support the audio element.</audio></center><p>Sigmoid 的输出大于 0.5，表示检测到了触发词，因此添加了鸣响。</p><h2 id="总结">总结</h2><p>终于完成了 Sequence 系列的实验，对触发词检测的流程有了个大致的了解，但是还是感觉神经网络就像炼丹，知道模型的结构这么设置有什么好处，但是还是不知道为什么要这么设置，都是靠直觉？</p><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li><li>meelo. 短时傅里叶变换解析. https://www.cnblogs.com/meelo/p/5640009.html</li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2018/11/15/trigger-word-detection/" title="触发词检测">https://pengzhendong.cn/2018/11/15/trigger-word-detection/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/10/29/fast-fourier-transform/" rel="prev" title="快速傅里叶变换"><i class="fa fa-chevron-left"></i> 快速傅里叶变换</a></div><div class="post-nav-item"> <a href="/2018/11/20/convolution-and-filters/" rel="next" title="卷积与滤波器">卷积与滤波器<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#触发词检测"><span class="nav-number">2.</span> <span class="nav-text">触发词检测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据合成创建语音数据集"><span class="nav-number">2.1.</span> <span class="nav-text">数据合成：创建语音数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#波形图"><span class="nav-number">2.1.1.</span> <span class="nav-text">波形图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#波形图-1"><span class="nav-number">3.</span> <span class="nav-text">波形图</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#短时傅里叶变换"><span class="nav-number">3.1.</span> <span class="nav-text">短时傅里叶变换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#窗函数"><span class="nav-number">3.1.1.</span> <span class="nav-text">窗函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#声谱图"><span class="nav-number">3.1.2.</span> <span class="nav-text">声谱图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#生成训练示例"><span class="nav-number">4.</span> <span class="nav-text">生成训练示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#辅助函数"><span class="nav-number">4.1.</span> <span class="nav-text">辅助函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#生成训练样本"><span class="nav-number">4.2.</span> <span class="nav-text">生成训练样本</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#开发测试集"><span class="nav-number">5.</span> <span class="nav-text">开发测试集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型"><span class="nav-number">6.</span> <span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#构建模型"><span class="nav-number">6.1.</span> <span class="nav-text">构建模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#拟合测试模型"><span class="nav-number">6.2.</span> <span class="nav-text">拟合&amp;测试模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预测"><span class="nav-number">6.3.</span> <span class="nav-text">预测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#测试例子"><span class="nav-number">6.3.1.</span> <span class="nav-text">测试例子</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">8.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>