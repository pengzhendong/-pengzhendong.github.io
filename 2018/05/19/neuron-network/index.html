<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 Logistic 回归和 Softmax 回归解决的是线性分类问题，即不同类别之间可以被线性平面分隔开，所以相当于没有隐藏层的神经网络。对于线性不可分的数据，由于线性模型无法理解任何两个特征间的相互作用，所以就需要有隐藏层(使用了非线性激活函数)的神经网络提取特征，将线性不可分的数据变得线性可分。"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="单隐层神经网络"><meta property="og:url" content="https://pengzhendong.cn/2018/05/19/neuron-network/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 Logistic 回归和 Softmax 回归解决的是线性分类问题，即不同类别之间可以被线性平面分隔开，所以相当于没有隐藏层的神经网络。对于线性不可分的数据，由于线性模型无法理解任何两个特征间的相互作用，所以就需要有隐藏层(使用了非线性激活函数)的神经网络提取特征，将线性不可分的数据变得线性可分。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://randy-1251769892.cos.ap-beijing.myqcloud.com/hidden-layer.gif"><meta property="og:image" content="https://randy-1251769892.cos.ap-beijing.myqcloud.com/success.gif"><meta property="og:image" content="https://randy-1251769892.cos.ap-beijing.myqcloud.com/fail.gif"><meta property="og:image" content="https://pengzhendong.cn/2018/05/19/neuron-network/tanh.png"><meta property="og:image" content="https://randy-1251769892.cos.ap-beijing.myqcloud.com/sgd.gif"><meta property="og:image" content="https://randy-1251769892.cos.ap-beijing.myqcloud.com/sgd_bad.gif"><meta property="og:updated_time" content="2018-05-19T15:44:14.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="单隐层神经网络"><meta name="twitter:description" content="前言 Logistic 回归和 Softmax 回归解决的是线性分类问题，即不同类别之间可以被线性平面分隔开，所以相当于没有隐藏层的神经网络。对于线性不可分的数据，由于线性模型无法理解任何两个特征间的相互作用，所以就需要有隐藏层(使用了非线性激活函数)的神经网络提取特征，将线性不可分的数据变得线性可分。"><meta name="twitter:image" content="https://randy-1251769892.cos.ap-beijing.myqcloud.com/hidden-layer.gif"><link rel="canonical" href="https://pengzhendong.cn/2018/05/19/neuron-network/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>单隐层神经网络 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/05/19/neuron-network/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 单隐层神经网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-05-19 22:27:43 / 修改时间：23:44:14" itemprop="dateCreated datePublished" datetime="2018-05-19T22:27:43+08:00">2018-05-19</time></span><span id="/2018/05/19/neuron-network/" class="post-meta-item leancloud_visitors" data-flag-title="单隐层神经网络" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>Logistic 回归和 Softmax 回归解决的是线性分类问题，即不同类别之间可以被线性平面分隔开，所以相当于没有隐藏层的神经网络。对于线性不可分的数据，由于线性模型无法理解任何两个特征间的相互作用，所以就需要有隐藏层(使用了非线性激活函数)的神经网络提取特征，将线性不可分的数据变得线性可分。</p><a id="more"></a><p>从输入层到输出层，向前计算代价函数的过程称为前向传播。从输出层到输出层，向后使用链式法则计算梯度的过程称为反向传播，得到梯度后就可以使用梯度下降算法更新模型的参数。最后可以得到比 Logistic 回归复杂得多的模型，拟合能力强但是也容易过拟合，由于代价函数不是凸函数，所以会给优化带来一些困难。</p><h2 id="单隐层神经网络">单隐层神经网络</h2><p>单隐层神经网络相当于由多个对率回归模型组成。中间的隐藏层可以看成特征提取的过程，由于对率回归使用了非线性激活函数，所以通过特征提取，就可以把原本线性不可分的数据变得线性可分，最后通过输出层进行线性分类。</p><h3 id="特征提取">特征提取</h3><blockquote><p>隐藏层 <span class="math inline">\(\vec z=\sigma(W\vec x+\vec b)\)</span>，其中 <span class="math inline">\(\vec x\)</span> 是输入向量，<span class="math inline">\(\vec z\)</span> 是输出向量，<span class="math inline">\(W\)</span> 是权重矩阵，<span class="math inline">\(\vec b\)</span> 是偏移向量，<span class="math inline">\(\sigma()\)</span> 是激活函数。每一层仅仅是把输入 <span class="math inline">\(\vec x\)</span> 经过简单的操作得到 <span class="math inline">\(\vec y\)</span>。</p></blockquote><p>在线性代数或者计算机图形学中学过，空间中的物体乘以一个矩阵就可以对物体进行放大/缩小、升维/降维或者旋转；加上一个向量就可以进行平移；这里的非线性激活函数还可以让物体变弯曲，如果使用线性函数作为激活函数，那么无论神经网络有多少层，输出都是输入的线性组合。</p><p><img src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/hidden-layer.gif"></p><p>因此每层神经网络的作用就是对输入使用线性变换和非线性变换，通过最小化代价函数使得在输出空间中尽量线性可分；</p><p><img src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/success.gif"></p><p>如果神经网络学习的效果不好，就会导致在输出空间中不能线性可分。</p><p><img src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/fail.gif"></p><p>可以在斯坦福大学的网站中体验单隐层神经网络的运行过程 <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html" target="_blank" rel="noopener">ConvNetJS demo: Classify toy 2D data</a></p><h2 id="单隐层分类平面数据">单隐层分类平面数据</h2><p>以下内容基于 Deeplearning.ai 的 Neural Networks and DeepLearning 第三周的课程实验 <code>Planar data classification with one hidden layer</code>。</p><h3 id="数据集">数据集</h3><p>实验中通过添加噪声生成了一些非线性可分的二维数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># planar_utils.py 生成数据部分代码</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    ix = range(N*j,N*(j+<span class="number">1</span>))</span><br><span class="line">    t = np.linspace(j*<span class="number">3.12</span>,(j+<span class="number">1</span>)*<span class="number">3.12</span>,N) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># theta</span></span><br><span class="line">    r = a*np.sin(<span class="number">4</span>*t) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># radius</span></span><br><span class="line">    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]</span><br><span class="line">    Y[ix] = j</span><br></pre></td></tr></table></figure><p>实验中使用了 <code>sklearn.linear_model.LogisticRegressionCV()</code> 数据进行分类，由于数据线性不可分，因此测试集的准确率只有 47%，因此需要使用多层神经网络进行分类。</p><h3 id="神经网络模型">神经网络模型</h3><p>实验生成的数据是平面(二维)数据，因此输入是一个二维的向量，隐藏层具有四个神经元并且隐藏层使用的激活函数是 <code>Tanh</code> 函数，最后一层需要输出属于哪一类的概率，所以只能使用 Sigmoid 激活函数。</p><ul><li><p>Tanh 函数是双曲正切函数</p><p><img src="/2018/05/19/neuron-network/tanh.png"></p><p><span class="math display">\[ tanh(x)=\frac{sinhx}{coshx}=\frac{e^x-e^{-x}}{e^x+e^{-x}} \]</span></p></li></ul><p><span class="math display">\[ tanh&#39;(x)=sech^2x=1-tanh^2x \]</span></p><p>Tanh 和 Sigmoid 函数可以通过缩放平移重合，为什么 Tanh 函数表现更好？Deep Learning 中给出的解释是：因为 Tanh 函数经过原点，且在原点附近梯度比 Sigmoid 函数的梯度大，所以在训练过程中优化会比较容易。</p><p>Tanh 和 Sigmoid 函数能否拟合任意函数呢？不能！首先来看一下什么叫 <code>Squashing</code> (压扁；压制)函数：</p><blockquote><p>A function <span class="math inline">\(\Psi: R\to[0, 1]\)</span> is a squashing function if it is non-decreasing, <span class="math inline">\(\lim\limits_{\lambda \to \infty }{\Psi(\lambda)}=1\)</span> and <span class="math inline">\(\lim\limits_{\lambda \to -\infty }{\Psi(\lambda)}=0\)</span>.</p></blockquote><p>显然 Tanh 函数通过缩放平移和 Sigmoid 函数满足这个定义，那么 Squashing 函数有什么性质呢？Hornik 等人在 1989 年中的一篇文章中说道：即使单隐层神经网络，用任意的 Squashing 函数作为激活函数，当神经元数量足够多时，可以拟合任意的博雷尔可测(Borel measurable)函数。那么博雷尔不可测函数又是什么意思呢？</p><p>定义集合 <span class="math inline">\(S\)</span> 为一个博雷尔不可测集合，有 <span class="math display">\[ f(x) = \begin{cases} 0 &amp; x \not\in S \\\ 1 &amp; x \in S \end{cases} \]</span> 则函数 <span class="math inline">\(f(x)\)</span> 就是博雷尔不可测函数。那么博雷尔不可测集合又是什么呢？这就触及到我的知识盲区了，总之用 Squashing 函数拟合实际问题中的函数是绰绰有余的，也不难想象足够多的 Squashing 函数的线性组合确实能拟合很多函数了。</p><ul><li>模型结构：</li></ul><p>对于一个样本数据 <span class="math inline">\(x^{(i)}\)</span>，有： <span class="math display">\[ z^{[1] (i)} = W^{[1]} x^{(i)} + b^{[1]}\tag{1} \]</span></p><p><span class="math display">\[ a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2} \]</span></p><p><span class="math display">\[ z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\tag{3} \]</span></p><p><span class="math display">\[ \hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4} \]</span></p><p><span class="math display">\[ y^{(i)}\_{prediction} = \begin{cases} 1 &amp; \mbox{if}\quad a^{[2]\(i\)} &gt; 0.5 \\\ 0 &amp; \mbox{otherwise} \end{cases}\tag{5} \]</span></p><p>给定所有样本数据，代价函数为： <span class="math display">\[ J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \left(y^{(i)}\log(a^{[2] (i)}) + (1-y^{(i)})\log(1- a^{[2] (i)})\right)\tag{6} \]</span> 这里的 <span class="math inline">\(a^{[2] (i)}\)</span> 就是最后一层(不算输入层即第二层)神经网络的输出，类似于对率回归中的 <span class="math inline">\(h_\theta(x^{(i)})\)</span>。</p><p>构建神经网络模型主要分为以下几部分：</p><ol type="1"><li>定义神经网络结构(神经元的个数、隐藏层的层数等)</li><li>初始化模型参数</li><li>循环<ul><li>实现前向传播(实现公式 1~4，得到预测值 <span class="math inline">\(\hat y\)</span>，即 <span class="math inline">\(a^{[2]}\)</span>)</li><li>计算代价(根据前向传播得到的预测值和测试集的标签，实现公式 6，得到代价)</li><li>实现反向传播，计算梯度</li><li>梯度下降更新模型参数</li></ul></li></ol><h4 id="定义神经网络结构">定义神经网络结构</h4><p>在生成的实验数据中，<code>X.shape = (2, 400)</code>、<code>y.shape = (1, 400)</code>，因此输入层的神经元个数 <span class="math inline">\(n^{[x]}=2\)</span>；隐藏层的神经元个数定义为 <span class="math inline">\(n^{[h]}=4\)</span>；输出层神经元的个数 <span class="math inline">\(n^{[y]}=1\)</span>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h4 id="初始化模型参数">初始化模型参数</h4><p>使用 <code>np.random.randn(a, b)</code> 初始化一个形状为 <code>(a, b)</code> 的矩阵，使其元素为标准正态分布中的样本。使用 <code>np.zeros((a, b))</code> 初始化一个形状为 <code>(a, b)</code> 的矩阵，使其各元素值为 0。</p><ul><li>如果权值全部初始化为相同的数，那么隐藏层中神经元的输出就都是一样的，通过归纳法可以归纳出这些隐藏层的神经元一直在计算完全一样的函数，所以需要随机初始化打破对称性；</li><li>如果权值全部初始化为 0，更加糟糕的是不管输入是什么，隐藏层中神经元的输出就都是 0；</li><li>如果初始化为比较大的数，那么就会导致激活函数输出的值比较大，梯度较小，梯度下降的速度较慢，所以需要初始化为 0 附近的随机数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br></pre></td></tr></table></figure><h4 id="循环">循环</h4><h5 id="前向传播">前向传播</h5><p>在前向传播计算预测值时需要缓存中间变量 <span class="math inline">\(A^{[1]}\)</span>，用于反向传播计算梯度 <span class="math inline">\(dW^{[2]}\)</span>。在实验中也缓存了所有中间变量包括 <span class="math inline">\(Z^{[1]}\)</span> 和 <span class="math inline">\(Z^{[2]}\)</span>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.Tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = Sigmoid(Z2)</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><h5 id="计算代价">计算代价</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line"></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="number">1</span> - A2), <span class="number">1</span> - Y)</span><br><span class="line">    cost = - (<span class="number">1.0</span> / m) * np.sum(logprobs)</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h5 id="反向传播">反向传播</h5><p>为了快速计算，实验对所有样本数据使用向量化编程。同时为了表示简单，以下求导公式省去上标 <code>(i)</code>。所以对于一个样本数据的输入、隐藏层输入、隐藏层输出和标签分别用小写字母 <code>x</code>、<code>z</code>、<code>a</code> 和 <code>y</code> 表示；所有样本数据则对应大写字母 <code>X</code>、<code>Z</code>、<code>A</code> 和 <code>Y</code>。</p><ul><li>对于一个样本数据 <span class="math inline">\(x\)</span> (随机梯度下降)，有：</li></ul><p><span class="math display">\[ \mathscr{l}(\hat y, y)=-\left(y^{(i)}\log(a^{[2] (i)}) + (1-y^{(i)})\log(1- a^{[2] (i)})\right) \]</span></p><p><span class="math display">\[ \begin{align} dz^{[2]}=\frac{\partial{\mathscr{l}(\hat y, y)}}{\partial{z^{[2]}}} &amp; = -\left(\frac{y}{a^{[2]}}\sigma&#39;(z^{[2]})+\frac{1-y^{(i)}}{1-a^{[2]}}\sigma&#39;(z^{[2]})\right) \\\ &amp; = a^{[2]}-y \end{align} \]</span></p><p><span class="math display">\[ \begin{align} dW^{[2]}=\frac{\partial{\mathscr{l}(\hat y, y)}}{\partial{z^{[2]}}}\frac{\partial{z^{[2]}}}{\partial{W^{[2]}}} &amp; = dz^{[2]}a^{[1]\mathrm{T}} \end{align} \]</span></p><p><span class="math display">\[ db^{[2]}=\frac{\partial{\mathscr{l}(\hat y, y)}}{\partial{z^{[2]}}}\frac{\partial{z^{[2]}}}{\partial{b^{[2]}}}=dz^{[2]} \]</span></p><p><span class="math display">\[ \begin{align} dz^{[1]} = W^{[2]\mathrm{T}}dz^{[2]}*\left(1-tanh^2(z)\right) = W^{[2]\mathrm{T}}dz^{[2]}*(1-a^{[1]2}) \end{align} \]</span></p><p><span class="math display">\[ dW^{[1]}=dz^{[1]}x^{\mathrm{T}} \]</span></p><p><span class="math display">\[ db^{[1]}=dz^{[1]} \]</span></p><p>导数写在左边还是右边？是否需要转置？点乘还是叉乘？在矩阵求导中有<a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">两种布局</a>：分子布局和分母布局，不同布局求导规则不一样 。但是在实验中，我们已知各个变量和导数的维度，所以只需要根据数据的维度计算选择布局即可(<span class="math inline">\(dfoo.shape=foo.shape\)</span>)。例如： <span class="math display">\[ dz^{[1]}=da^{[1]}*\frac{\partial{a^{[1]}}}{\partial{z^{[1]}}}=W^{[2]\mathrm{T}}dz^{[2]}*(1-a^{[1]2}) \]</span> <span class="math inline">\(m\)</span> 表示样本数量(<span class="math inline">\(m = 1\)</span>)，由于 <span class="math inline">\(a^{[1]}\)</span> 和 <span class="math inline">\(z^{[1]}\)</span> 只是进行了一个非线性变换，具有相同的维度，所以用点乘；所以只需要求 <span class="math inline">\(da^{[1]}\)</span> 且满足 <span class="math inline">\(da^{[1]}.shape=(n^{[h]}, m)\)</span>： <span class="math display">\[ (n^{[h]}, m) = (n^{[y]}, n^{[h]})^{\mathrm{T}}(n^{[y]}, m) = (n^{[h]}, n^{[y]})(n^{[y]}, m) \]</span></p><ul><li>对于所有样本数据，有：</li></ul><p><span class="math display">\[ dZ^{[2]}=A^{[2]}-Y \]</span></p><p><span class="math display">\[ dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]\mathrm{T}} \]</span></p><p><span class="math display">\[ db^{[2]}=\frac{1}{m}\sum\limits_{i = 0}^{m}dZ^{[2]} \]</span></p><p><span class="math display">\[ dZ^{[1]}=W^{[2]\mathrm{T}}dZ^{[2]}*(1-A^{[1]2}) \]</span> <span class="math display">\[ dW^{[1]}=\frac{1}{m}dZ^{[1]}X^{\mathrm{T}} \]</span></p><p><span class="math display">\[ db^{[1]}=\frac{1}{m}\sum\limits_{i = 0}^{m}dZ^{[1]} \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">'A1'</span>]</span><br><span class="line">    A2 = cache[<span class="string">'A2'</span>]</span><br><span class="line"></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = (<span class="number">1.0</span> / m) * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = (<span class="number">1.0</span> / m) * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) * (<span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = (<span class="number">1.0</span> / m) * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = (<span class="number">1.0</span> / m) * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h5 id="梯度下降更新模型参数">梯度下降更新模型参数</h5><blockquote><p>梯度下降规则：<span class="math inline">\(\theta = \theta - \alpha \frac{\partial J }{ \partial \theta }\)</span></p></blockquote><p>在<a href="/2018/03/10/Linear-regression/">线性回归</a>中总结过，在梯度下降中好的学习率可以快速收敛，不好的学习率则会发散(实验中默认学习率为1.2)，如下图所示：</p><p><img src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/sgd.gif"></p><p><img src="https://randy-1251769892.cos.ap-beijing.myqcloud.com/sgd_bad.gif"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">    W1 = W1 - learning_rate * dW1</span><br><span class="line">    b1 = b1 - learning_rate * db1</span><br><span class="line">    W2 = W2 - learning_rate * dW2</span><br><span class="line">    b2 = b2 - learning_rate * db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="集成模型">集成模型</h3><p>集成单隐层神经网络的所有模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="预测">预测</h3><p>最后要根据神经网络的输出和阈值，预测输出，即实现公式 5：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = A2 &gt; <span class="number">0.5</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h3 id="评估分析">评估分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>对实验数据进行学习分类，最后输出分类准确率高达 90%，通过调节隐藏层神经元个数，可以发现模型越大(隐藏层神经元越多)，则模型的拟合能力越强，但是达到一定程度后就会对训练集产生过拟合(可以添加正则化项避免过拟合)。本次实验数据结果发现隐藏层神经元个数为 5 的时候拟合能力最好。</p><h2 id="参考文献">参考文献</h2><p>[1] 吴恩达. DeepLearning.</p><p>[2] Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. 人民邮电出版社. 2017.</p><p>[3] Hornik, K., Stinchcombe, M., &amp; White, H. (1989). Multilayer feedforward networks are universal approximators. <em>Neural networks</em>, <em>2</em>(5), 359-366.</p></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2018/05/19/neuron-network/" title="单隐层神经网络">https://pengzhendong.cn/2018/05/19/neuron-network/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/04/26/logistic-regression/" rel="prev" title="Logistic 回归和 Softmax 回归"><i class="fa fa-chevron-left"></i> Logistic 回归和 Softmax 回归</a></div><div class="post-nav-item"> <a href="/2018/05/21/deep-neuron-network/" rel="next" title="深度神经网络">深度神经网络<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#单隐层神经网络"><span class="nav-number">2.</span> <span class="nav-text">单隐层神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#特征提取"><span class="nav-number">2.1.</span> <span class="nav-text">特征提取</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#单隐层分类平面数据"><span class="nav-number">3.</span> <span class="nav-text">单隐层分类平面数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集"><span class="nav-number">3.1.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络模型"><span class="nav-number">3.2.</span> <span class="nav-text">神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义神经网络结构"><span class="nav-number">3.2.1.</span> <span class="nav-text">定义神经网络结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#初始化模型参数"><span class="nav-number">3.2.2.</span> <span class="nav-text">初始化模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#循环"><span class="nav-number">3.2.3.</span> <span class="nav-text">循环</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#前向传播"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#计算代价"><span class="nav-number">3.2.3.2.</span> <span class="nav-text">计算代价</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播"><span class="nav-number">3.2.3.3.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#梯度下降更新模型参数"><span class="nav-number">3.2.3.4.</span> <span class="nav-text">梯度下降更新模型参数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集成模型"><span class="nav-number">3.3.</span> <span class="nav-text">集成模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预测"><span class="nav-number">3.4.</span> <span class="nav-text">预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#评估分析"><span class="nav-number">3.5.</span> <span class="nav-text">评估分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">4.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>