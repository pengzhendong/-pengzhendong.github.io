<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="å‰è¨€ ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªéšè—å±‚ï¼Ÿéšè—å±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡è¶Šå¤šæ‹Ÿåˆèƒ½åŠ›ä¸å°±è¶Šå¼ºå—ï¼Ÿè¿™ä¸ªé—®é¢˜å›°æƒ‘äº†æˆ‘å¥½ä¹…ï¼Œè¯´ç™½äº†å°±æ˜¯ä¹¦è¯»çš„å¤ªå°‘ï¼Œæƒ³å¾—å¤ªå¤šã€‚å´æ©è¾¾ç”¨ç”µè·¯ç†è®ºå’ŒäºŒå‰æ ‘è§£å†³äº†æˆ‘è¿™ä¸ªå›°æƒ‘ï¼"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="æ·±åº¦ç¥ç»ç½‘ç»œ"><meta property="og:url" content="https://pengzhendong.cn/2018/05/21/deep-neuron-network/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="å‰è¨€ ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªéšè—å±‚ï¼Ÿéšè—å±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡è¶Šå¤šæ‹Ÿåˆèƒ½åŠ›ä¸å°±è¶Šå¼ºå—ï¼Ÿè¿™ä¸ªé—®é¢˜å›°æƒ‘äº†æˆ‘å¥½ä¹…ï¼Œè¯´ç™½äº†å°±æ˜¯ä¹¦è¯»çš„å¤ªå°‘ï¼Œæƒ³å¾—å¤ªå¤šã€‚å´æ©è¾¾ç”¨ç”µè·¯ç†è®ºå’ŒäºŒå‰æ ‘è§£å†³äº†æˆ‘è¿™ä¸ªå›°æƒ‘ï¼"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://pengzhendong.cn/2018/05/21/deep-neuron-network/ReLU.png"><meta property="og:image" content="https://pengzhendong.cn/2018/05/21/deep-neuron-network/squashing.png"><meta property="og:image" content="https://pengzhendong.cn/2018/05/21/deep-neuron-network/final%20outline.png"><meta property="og:updated_time" content="2018-05-21T07:54:16.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="æ·±åº¦ç¥ç»ç½‘ç»œ"><meta name="twitter:description" content="å‰è¨€ ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªéšè—å±‚ï¼Ÿéšè—å±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡è¶Šå¤šæ‹Ÿåˆèƒ½åŠ›ä¸å°±è¶Šå¼ºå—ï¼Ÿè¿™ä¸ªé—®é¢˜å›°æƒ‘äº†æˆ‘å¥½ä¹…ï¼Œè¯´ç™½äº†å°±æ˜¯ä¹¦è¯»çš„å¤ªå°‘ï¼Œæƒ³å¾—å¤ªå¤šã€‚å´æ©è¾¾ç”¨ç”µè·¯ç†è®ºå’ŒäºŒå‰æ ‘è§£å†³äº†æˆ‘è¿™ä¸ªå›°æƒ‘ï¼"><meta name="twitter:image" content="https://pengzhendong.cn/2018/05/21/deep-neuron-network/ReLU.png"><link rel="canonical" href="https://pengzhendong.cn/2018/05/21/deep-neuron-network/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>æ·±åº¦ç¥ç»ç½‘ç»œ | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ "><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> é¦–é¡µ</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> å½’æ¡£</a></li><li class="menu-item menu-item-å‹é“¾"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> å‹é“¾</a></li><li class="menu-item menu-item-ä¹¦å•"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> ä¹¦å•</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> æœç´¢</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/05/21/deep-neuron-network/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="è·¯æ¼«æ¼«å…¶ä¿®è¿œå…® å¾å°†ä¸Šä¸‹è€Œæ±‚ç´¢"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> æ·±åº¦ç¥ç»ç½‘ç»œ</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">å‘è¡¨äº</span> <time title="åˆ›å»ºæ—¶é—´ï¼š2018-05-21 13:56:22 / ä¿®æ”¹æ—¶é—´ï¼š15:54:16" itemprop="dateCreated datePublished" datetime="2018-05-21T13:56:22+08:00">2018-05-21</time></span><span id="/2018/05/21/deep-neuron-network/" class="post-meta-item leancloud_visitors" data-flag-title="æ·±åº¦ç¥ç»ç½‘ç»œ" title="é˜…è¯»æ¬¡æ•°"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="æœ¬æ–‡å­—æ•°"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span> <span>0</span></span><span class="post-meta-item" title="é˜…è¯»æ—¶é•¿"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span> <span>1 åˆ†é’Ÿ</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="å‰è¨€">å‰è¨€</h2><p>ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªéšè—å±‚ï¼Ÿéšè—å±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡è¶Šå¤šæ‹Ÿåˆèƒ½åŠ›ä¸å°±è¶Šå¼ºå—ï¼Ÿè¿™ä¸ªé—®é¢˜å›°æƒ‘äº†æˆ‘å¥½ä¹…ï¼Œè¯´ç™½äº†å°±æ˜¯ä¹¦è¯»çš„å¤ªå°‘ï¼Œæƒ³å¾—å¤ªå¤šã€‚å´æ©è¾¾ç”¨ç”µè·¯ç†è®ºå’ŒäºŒå‰æ ‘è§£å†³äº†æˆ‘è¿™ä¸ªå›°æƒ‘ï¼</p><a id="more"></a><h2 id="ç”µè·¯ç†è®ºå’Œæ·±åº¦å­¦ä¹ ">ç”µè·¯ç†è®ºå’Œæ·±åº¦å­¦ä¹ </h2><blockquote><p>There are functions you can compute with a "small" L-layer deep nerual network that shallower networks require exponentiall more hidden units to compute.</p></blockquote><p>ä¹Ÿå°±æ˜¯è¯´æœ‰ä¸€äº›å‡½æ•°ï¼Œä¸€ä¸ªå¾ˆå°çš„ L å±‚æ·±åº¦ç¥ç»ç½‘ç»œå°±èƒ½å®ç°ï¼Œè€Œæµ…å±‚ç¥ç»ç½‘ç»œéœ€è¦çš„ç¥ç»å…ƒçš„æ•°é‡æ˜¯æŒ‡æ•°çº§åˆ«çš„ã€‚ä¾‹å¦‚å¼‚æˆ–æ“ä½œï¼Œå¯¹äºä¸‰ç»´æ•°æ®ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆä¸ºï¼š<span class="math inline">\(x_1\oplus x_2\oplus x_3=(x_1\oplus x_2)\oplus x_3\)</span>ï¼Œæµ…å±‚ç¥ç»ç½‘ç»œæ‹Ÿåˆä¸ºï¼š<span class="math inline">\(x_1\oplus x_2\oplus x_3=x_1\cdot x_2\cdot x_3+x&#39;_1\cdot x&#39;_2\cdot x_3+x&#39;_1\cdot x_2\cdot x&#39;_3+x_1\cdot x&#39;_2\cdot x&#39;_3\)</span>ï¼›æ‰€ä»¥æ·±åº¦ç¥ç»ç½‘ç»œçš„å±‚æ•°ä¹Ÿå°±æ˜¯äºŒå‰æ ‘çš„é«˜åº¦ <span class="math inline">\(O(logn)\)</span>ï¼Œç¥ç»å…ƒçš„æ•°é‡ä¸ä¼šå¾ˆå¤§ï¼Œè€Œå•éšå±‚ç¥ç»ç½‘ç»œéœ€è¦çš„ç¥ç»å…ƒçš„ä¸ªæ•°åˆ™æ˜¯ <span class="math inline">\(2^{n-1}\)</span> ä¸ªï¼ŒæŒ‡æ•°çˆ†ç‚¸ï¼</p><h2 id="æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹">æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹</h2><p>æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹å’Œå•éšå±‚ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ¨¡å—ä¸€æ ·ï¼Œåªä¸è¿‡æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„éšè—å±‚ä¸æ­¢ä¸€ä¸ªã€‚åœ¨å•éšå±‚ç¥ç»ç½‘ç»œçš„éšè—å±‚ä¸­ä½¿ç”¨äº† <code>Tanh</code> æ¿€æ´»å‡½æ•°ï¼Œè€Œç°åœ¨æ›´åŠ å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯ <code>ReLU</code> (çº¿æ€§æ•´æµ)å‡½æ•°ã€‚</p><h3 id="relu">ReLU</h3><p>ReLU å‡½æ•°æ˜¯ä¸€ä¸ªåˆ†æ®µå‡½æ•°ï¼Œå…¶å‡½æ•°å›¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="/2018/05/21/deep-neuron-network/ReLU.png"> <span class="math display">\[ ReLU(x)=max(0, x) \]</span> è¿™æ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œå½“ <span class="math inline">\(x&lt;0\)</span> æ—¶ï¼Œ<span class="math inline">\(ReLu(x)=0\)</span>ï¼Œæ¢¯åº¦ä¸º 0ï¼›å½“ <span class="math inline">\(x\geq 0\)</span> æ—¶ï¼Œ<span class="math inline">\(ReLu(x)=x\)</span>ï¼Œæ¢¯åº¦ä¸º 1ã€‚</p><h4 id="squashing-å‡½æ•°">Squashing å‡½æ•°</h4><p>ç¬¬ä¸€æ¬¡çœ‹åˆ° ReLU å‡½æ•°ï¼Œå°±è§‰å¾—å®ƒè™½ç„¶æ˜¯éçº¿æ€§çš„ï¼Œä½†æ˜¯å®ƒä¸æ˜¯ Squashing å‡½æ•°å•Šï¼å¯ä»¥é€šè¿‡ä¸¤ä¸ª ReLU ç¥ç»å…ƒçš„å åŠ ï¼Œæ„é€ ä¸€ä¸ª Squashing å‡½æ•°ï¼š</p><p><span class="math display">\[ \Psi(x)=ReLU(x)-ReLU(x-1)=max(0, x)-max(0, x-1) \]</span></p><p><img src="/2018/05/21/deep-neuron-network/squashing.png"></p><p>ä½¿ç”¨ ReLU å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°çš„æœ€å¤§å¥½å¤„æ˜¯æ¿€æ´»çŠ¶æ€çš„ç¥ç»å…ƒçš„æ¢¯åº¦ä¸ä¼šæ¶ˆå¤±ï¼Œä¸”æ¢¯åº¦å›ºå®šå¯ä»¥åŠ å¿«å­¦ä¹ é€Ÿåº¦ï¼›å…¶æ¬¡ï¼Œå¯¹äº<strong>æ¯ä¸ªæ ·æœ¬æ•°æ®</strong>ï¼Œä¸€éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºä¸º 0 é€ æˆäº†ç½‘ç»œçš„ç¨€ç–æ€§ï¼Œç¼“è§£äº†è¿‡æ‹Ÿåˆé—®é¢˜çš„å‘ç”Ÿã€‚è™½ç„¶<strong>æ¯ä¸ªæ ·æœ¬æ•°æ®</strong>ç»è¿‡ç¥ç»ç½‘ç»œåçš„è¾“å‡ºéƒ½æ˜¯è¾“å…¥çš„çº¿æ€§ç»„åˆï¼Œä½†æ˜¯ä¸åŒçš„è¾“å…¥æ¿€æ´»çš„ç¥ç»å…ƒæ˜¯ä¸åŒçš„ï¼Œæ­£æ˜¯å› ä¸ºè¿™ç§å˜æ¢å¼•å…¥äº†éçº¿æ€§ã€‚ä¾‹å¦‚å•éšå±‚ç¥ç»ç½‘ç»œæ‹Ÿåˆ <span class="math inline">\(f(x)=x^2\)</span>:</p><ul><li>ä¸¤ä¸ªç¥ç»å…ƒï¼š</li></ul><p><span class="math display">\[ h_1(x)=ReLU(x)+ReLU(-x)=|x| \]</span></p><ul><li>å››ä¸ªç¥ç»å…ƒï¼š <span class="math display">\[ h_2(x)=ReLU(x)+ReLU(-x)+2ReLU(x-1)+2ReLU(-x-1) \]</span></li></ul><p>å¤šä¸ª ReLU ç¥ç»å…ƒå åŠ ç¡®å®å¯ä»¥æ‹Ÿåˆå‡ºå„ç§å½¢çŠ¶ï¼Œæ‰€ä»¥åªè¦ç¥ç»å…ƒä¸ªæ•°è¶³å¤Ÿå¤šï¼Œæ‹Ÿåˆå®é™…é—®é¢˜ä¸­çš„å‡½æ•°å°±å“å“æœ‰ä½™ã€‚</p><h4 id="ç¥ç»å…ƒåæ­»">ç¥ç»å…ƒåæ­»</h4><p>ReLU å‡½æ•°ä¹Ÿæœ‰å…¶ç¼ºç‚¹ï¼Œé‚£å°±æ˜¯ç¥ç»å…ƒå®¹æ˜“â€œåæ­»â€ã€‚å¦‚æœ<strong>æ‰€æœ‰æ ·æœ¬æ•°æ®</strong>éƒ½ä¸èƒ½æ¿€æ´»æŸä¸ªç¥ç»å…ƒ(å³ä¸ç®¡è¾“å…¥æ˜¯ä»€ä¹ˆï¼Œè¾“å‡ºéƒ½ä¸€æ ·)ï¼Œé‚£ä¹ˆ <font color="red">ReLU å‡½æ•°çš„æ¢¯åº¦ <span class="math inline">\(g&#39;()\)</span> ä¸º 0</font>ï¼Œåœ¨åå‘ä¼ æ’­çš„æ—¶å€™å‚æ•°å°±ä¸ä¼šè¢«æ›´æ–°ï¼Œè¿­ä»£åè¿˜æ˜¯ä¸€æ ·ï¼š <span class="math display">\[ dW^{[l]} \propto g&#39;(Z^{[l]}) \]</span> å­¦ä¹ ç‡è¿‡å¤§æˆ–è€…å‚æ•° <span class="math inline">\(w_1\)</span> çš„æ¢¯åº¦è¿‡å¤§ï¼Œ<span class="math inline">\(w_1\)</span> çš„å˜åŒ–å°±ä¼šå¾ˆå¤§ï¼ŒåŸæ¥ <span class="math inline">\(w_1x_1+w_2x_2+b\)</span> å¯¹äºä¸åŒçš„æ ·æœ¬æ•°æ®ï¼Œè¾“å‡ºå¯èƒ½æœ‰æ­£æœ‰è´Ÿï¼Œç°åœ¨å¾ˆæœ‰å¯èƒ½å°±åªå‡ºç°è´Ÿæ•°ã€‚å¯¹äºç¬¬ä¸€å±‚éšè—å±‚çš„ç¥ç»å…ƒï¼Œä¸€æ—¦åæ­»å°±å†ä¹Ÿæ— æ³•è¢«æ¿€æ´»ï¼›å¯¹äºç¬¬äºŒå±‚åŠä»¥åçš„ç¥ç»å…ƒï¼Œç”±äºå®ƒçš„è¾“å…¥(ä¸Šä¸€å±‚çš„è¾“å‡º)ä¹Ÿæ˜¯åˆ«çš„ç¥ç»å…ƒçš„è¾“å…¥ï¼Œæ‰€ä»¥ä¸Šä¸€å±‚çš„è¾“å‡ºæ›´æ–°åæœ‰å¯èƒ½å†æ¬¡æ¿€æ´»è¿™ä¸ªåæ­»çš„ç¥ç»å…ƒã€‚</p><p>ReLU çš„å˜ç§ Leaky ReLU å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šå…‹æœç¥ç»å…ƒåæ­»çš„é—®é¢˜ã€‚ç”±äºåœ¨ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ—¶å€™ï¼Œè®­ç»ƒæ•°æ®çš„ç»´åº¦æ¯”è¾ƒå¤§ï¼Œå¯¹äºéƒ¨åˆ†ç¥ç»å…ƒåæ­»è¿˜æ˜¯å¯ä»¥æ¥å—çš„ã€‚</p><h4 id="å‚æ•°åˆå§‹åŒ–">å‚æ•°åˆå§‹åŒ–</h4><p>æ—¢ç„¶ ReLU å‡½æ•°é¿å…äº†è¿‡é¥±å’Œï¼Œé‚£ä¹ˆåœ¨åˆå§‹åŒ–å‚æ•°çš„æ—¶å€™ä¸ºä»€ä¹ˆè¿˜è¦ä» (0, 1) æ­£æ€åˆ†å¸ƒé‡ŒæŠ½æ ·å‘¢ï¼Ÿé¦–å…ˆåˆ†æä¸€ä¸‹ä»¥ä¸‹å‡ ç§æƒ…å†µï¼š</p><ul><li><p><span class="math inline">\(W\)</span> éƒ½åˆå§‹åŒ–æˆç»å¯¹å€¼å¤§äº 1 çš„æ•°ï¼šæœ€åè¾“å…¥ Sigmoid å‡½æ•°çš„å€¼å°±ä¼šæŒ‡æ•°çˆ†ç‚¸ğŸ’¥ï¼Œ<span class="math inline">\(sigmoid(x)=\frac{1}{1+e^{-x}}\)</span>ï¼Œè€Œ <code>np.exp(710)</code> æº¢å‡ºï¼›ä»£ä»·å‡½æ•°ä¸­åŒ…å« <span class="math inline">\(log(1-a)\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(a=sigmoid(x)\)</span>ï¼Œè€Œ <code>np.exp(-37)=1</code> å¯¼è‡´ä»£ä»·å‡½æ•°åŒ…å« <span class="math inline">\(log(0)\)</span> äº§ç”Ÿè¿è¡Œæ—¶è­¦å‘Šã€‚</p></li><li><p>å› æ­¤ <span class="math inline">\(W\)</span> éœ€è¦åœ¨ <span class="math inline">\((-1, 1)\)</span> ä¹‹é—´é‡‡æ ·ï¼Œå‰é¢åˆ†æè¿‡ä¸èƒ½éƒ½åˆå§‹åŒ–ä¸º 0ï¼›å¦‚æœå…¨éƒ¨åœ¨ <span class="math inline">\((-1, 0)\)</span> æˆ–è€… <span class="math inline">\((0, 1)\)</span> ä¹‹é—´é‡‡æ ·åˆ™ ReLU å‡½æ•°æ˜¯çº¿æ€§çš„ï¼Œå­¦ä¹ èƒ½åŠ›è¾ƒå·®ã€‚</p></li><li><p><span class="math inline">\(W\)</span> éƒ½åˆå§‹åŒ–æˆæ»¡è¶³ <span class="math inline">\((0, 1)\)</span> æ­£æ€åˆ†å¸ƒçš„ç»å¯¹å€¼<strong>ç‰¹åˆ«å°</strong>çš„æ•°ï¼š<span class="math inline">\(dW^{[l]} \propto W^{[l+1]}\)</span>ï¼Œåœ¨æ·±åº¦ç½‘ç»œä¸­æ¢¯åº¦ä¼šæŒ‡æ•°çº§é€’å‡å¼•å‘æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚</p></li><li><p><span class="math inline">\(Z=\sum\limits_{i = 0}^{n}w_ix_i\)</span>ï¼Œç¥ç»å…ƒçš„ä¸ªæ•° <span class="math inline">\(n\)</span> è¶Šå¤§ï¼Œä¸‹ä¸€å±‚ç¥ç»ç½‘ç»œçš„è¾“å…¥å’Œè¾“å…¥çš„æ–¹å·®ä¹Ÿå°±è¶Šå¤§(<span class="math inline">\(w_i\)</span> å’Œ <span class="math inline">\(x_i\)</span> åŒ 0 å‡å€¼åˆ†å¸ƒ)ï¼š <span class="math display">\[ \begin{align} Var(Z) &amp;= Var(\sum\limits_{i = 0}^{n}w_ix_i) \\\ &amp;= \sum\limits_{i = 0}^{n}Var(w_ix_i) \\\ &amp;= \sum\limits_{i = 0}^{n}[E(w_i)]^2Var(x_i)+[E(x_i)]^2Var(w_i)+Var(x_i)Var(w_i) \\\ &amp;= \sum\limits_{i = 0}^{n}Var(x_i)Var(w_i) \\\ &amp;= nVar(w_i)Var(x_i) \end{align} \]</span> æ‰€ä»¥ <span class="math inline">\(n\)</span> è¶Šå¤§æˆ‘ä»¬å¸Œæœ› <span class="math inline">\(w_i\)</span> è¶Šå°ï¼Œè¿™æ ·ä¸‹ä¸€å±‚ç¥ç»ç½‘ç»œçš„è¾“å…¥å’Œè¯¥è¾“å…¥çš„æ–¹å·®éƒ½ä¸ä¼šå¤ªå¤§ï¼Œè¾“å…¥å°±è¿˜æ˜¯ 0 é™„è¿‘æ¯”è¾ƒå°çš„æ•°ã€‚æ—¢ä¸ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ã€‚</p></li></ul><p>å› æ­¤å‚æ•°æ—¢ä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå°ï¼Œæ‰€ä»¥å‚æ•°çš„åˆå§‹åŒ–å¾ˆé‡è¦ï¼ä¸€ç§æ–¹æ³•æ˜¯è®©æ¯å±‚ç¥ç»ç½‘ç»œçš„è¾“å…¥çš„æ–¹å·®å’Œè¾“å…¥å±‚çš„æ–¹æ³•ä¸€è‡´ï¼Œè¿™ç§æ–¹æ³•è™½ç„¶ä¸èƒ½å½»åº•è§£å†³é—®é¢˜ï¼Œä½†æ˜¯å¾ˆæœ‰æ•ˆã€‚å³ <span class="math inline">\(Var(Z)=Var(x_i)\)</span>ï¼Œæ‰€ä»¥ <span class="math inline">\(Var(w_i)=\frac{1}{n}\)</span>ã€‚å› ä¸º <span class="math inline">\(Var(cw)=c^2Var(w)\)</span>ï¼Œæ‰€ä»¥åœ¨æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„åŸºç¡€ä¸Šä¹˜ä»¥ <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span> å³å¯ä¿è¯ <span class="math inline">\(w\)</span> çš„æ–¹å·®ä¸º <span class="math inline">\(\frac{1}{n}\)</span>ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params[<span class="string">'W'</span> + str(l)] = np.random.randn(laye_dims[l], laye_dims[l<span class="number">-1</span>]) * np.sqrt(<span class="number">1</span>/layer_dims[l<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><h5 id="xavier-åˆå§‹åŒ–">Xavier åˆå§‹åŒ–</h5><p>æ–¹æ³•åŒæ—¶è€ƒè™‘äº†åå‘ä¼ æ’­æ—¶çš„æƒ…å½¢ï¼Œæ­¤æ—¶çš„è¾“å…¥æ˜¯å‰å‘ä¼ æ’­çš„è¾“å‡ºï¼Œå› æ­¤ <span class="math inline">\(Var(w_i)=\frac{1}{n}=\frac{1}{n_{out}}\)</span>ï¼Œäºæ˜¯ç»“åˆä»¥ä¸Šä¸¤ç‚¹è¦æ±‚ï¼Œæœ‰ <span class="math inline">\(Var(w_i)=\frac{2}{n_{in}+n_{out}}\)</span>ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params[<span class="string">'W'</span> + str(l)] = np.random.randn(laye_dims[l], laye_dims[l<span class="number">-1</span>]) * np.sqrt(<span class="number">2</span>/(layer_dims[l<span class="number">-1</span>]+layer_dims[l<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure><p>åœ¨å´æ©è¾¾çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¸­å»ºè®®å¦‚æœä½¿ç”¨ Tanh æ¿€æ´»å‡½æ•°ï¼Œåˆ™åˆå§‹åŒ–å‚æ•°æ–¹å·®ä¸º <span class="math inline">\(\frac{1}{n}\)</span> æˆ–è€… <span class="math inline">\(\frac{2}{n_{in}+n_{out}}\)</span>ï¼›å¦‚æœä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ï¼Œä¼šå‘ç°æ•ˆæœå¹¶ä¸å¥½ï¼Œå› ä¸º ReLU æ¿€æ´»å‡½æ•°æœ‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºæ˜¯ 0(å³æ²¡æœ‰è¢«æ¿€æ´»)ï¼Œäºæ˜¯ä½•å‡¯æ˜ç­‰äººæå‡ºäº† MSRA åˆå§‹åŒ–çš„æ–¹æ³•ï¼Œä¹Ÿå« He åˆå§‹åŒ–ã€‚</p><h5 id="he-åˆå§‹åŒ–">He åˆå§‹åŒ–</h5><p>He åˆå§‹åŒ–çš„æ€æƒ³æ˜¯ï¼šåœ¨ ReLU ç½‘ç»œä¸­ï¼Œå‡è®¾æœ‰ä¸€èˆ¬çš„ç¥ç»å…ƒè¢«æ¿€æ´»ï¼Œå¦ä¸€åŠè¾“å‡ºä¸º 0ï¼Œæ‰€ä»¥è¦ä¿æŒæ–¹å·®ä¸å˜åˆ™éœ€è¦åˆå§‹åŒ–å‚æ•°æ–¹å·®ä¸º <span class="math inline">\(\frac{2}{n}\)</span>ã€‚è¿˜å¯ä»¥æŠŠåˆ†å­å½“æˆä¸€ä¸ªè¶…çº§å‚æ•°æ¥è°ƒèŠ‚ï¼Œä½†æ˜¯è¿™ä¸ªè¶…çº§å‚æ•°å¹¶ä¸æ˜¯å¾ˆé‡è¦ï¼Œæ‰€ä»¥ä¼˜å…ˆçº§å¯ä»¥æ”¾å¾—æ¯”è¾ƒä½ã€‚ç”±äºæ²¡æœ‰è€ƒè™‘åå‘ä¼ æ’­ï¼Œæ‰€ä»¥åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œè¿˜æ˜¯ä½¿ç”¨ Xavier åˆå§‹åŒ–æ–¹æ³•çš„æ¯”è¾ƒå¤šã€‚</p><h3 id="æ¨¡å‹ç»“æ„">æ¨¡å‹ç»“æ„</h3><p>æ„å»ºä¸€ä¸ª L å±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ éƒ¨åˆ†ï¼š</p><ol type="1"><li>åˆå§‹åŒ– L å±‚ç¥ç»ç½‘ç»œçš„å‚æ•°</li><li>å®ç°å‰å‘ä¼ æ’­æ¨¡å‹(å›¾ä¸­ç´«è‰²éƒ¨åˆ†)<ul><li>è®¡ç®—æ¯ä¸€å±‚å‰å‘ä¼ æ’­æ­¥éª¤çš„çº¿æ€§(LINEAR)éƒ¨åˆ†ï¼Œå³è®¡ç®— <span class="math inline">\(Z^{[l]}\)</span></li><li>ä½¿ç”¨æ¿€æ´»(ACTIVATION)å‡½æ•° <code>ReLU</code> æˆ–è€… <code>Sigmoid</code></li><li>å°†ä¸¤ä¸ªæ­¥éª¤ç»“åˆåˆ°ä¸€ä¸ªæ–°çš„å‰å‘å‡½æ•°ä¸­ï¼š<code>[LINEAR-&gt;ACTIVATION]</code></li><li>å‰ L-1 å±‚ï¼š <code>[LINEAR-&gt;ACTIVATION]</code>ï¼Œæœ€åä¸€å±‚ï¼š <code>[LINEAR-&gt;SIGMOID]</code></li></ul></li><li>è®¡ç®—æŸå¤±</li><li>å®ç°åå‘ä¼ æ’­æ¨¡å‹(å›¾ä¸­çº¢è‰²éƒ¨åˆ†)<ul><li>è®¡ç®—æ¯ä¸€å±‚åå‘ä¼ æ’­æ­¥éª¤çš„çº¿æ€§(LINEAR)éƒ¨åˆ†</li><li>ä½¿ç”¨æ¿€æ´»(ACTIVATION)å‡½æ•° <code>ReLU</code> æˆ–è€… <code>Sigmoid</code> çš„æ¢¯åº¦</li><li>å°†ä¸¤ä¸ªæ­¥éª¤ç»“åˆåˆ°ä¸€ä¸ªæ–°çš„åå‘å‡½æ•°ä¸­ï¼š<code>[LINEAR-&gt;ACTIVATION]</code></li><li>å‰ L-1 å±‚ï¼š <code>[LINEAR-&gt;ACTIVATION]</code>ï¼Œæœ€åä¸€å±‚ï¼š <code>[LINEAR-&gt;SIGMOID]</code></li></ul></li><li>æ›´æ–°å‚æ•°</li></ol><p><img src="/2018/05/21/deep-neuron-network/final%20outline.png"></p><h4 id="åˆå§‹åŒ–æ¨¡å‹å‚æ•°">åˆå§‹åŒ–æ¨¡å‹å‚æ•°</h4><p>å®éªŒä¸­çš„è®­ç»ƒæ•°æ®æ˜¯ 209 å¼  <code>64*64*3</code> çš„å›¾ç‰‡ï¼Œå˜æˆå‘é‡åçš„ X çš„ç»´åº¦æ˜¯ (12288, 209)ï¼Œå› æ­¤æ¨¡å‹å‚æ•°çš„ç»´åº¦å¦‚ä¸‹æ ‡æ‰€ç¤ºï¼š</p><table><colgroup><col style="width:7%"><col style="width:21%"><col style="width:14%"><col style="width:40%"><col style="width:16%"></colgroup><thead><tr class="header"><th></th><th><span class="math inline">\(W\)</span> çš„ç»´åº¦</th><th><span class="math inline">\(b\)</span> çš„ç»´åº¦</th><th>æ¿€æ´»å‡½æ•°çš„è¾“å…¥ <span class="math inline">\(Z^{l}\)</span></th><th>æ¿€æ´»å‡½æ•°çš„ç»´åº¦</th></tr></thead><tbody><tr class="odd"><td>Layer 1</td><td><span class="math inline">\((n^{[1]}, 12288)\)</span></td><td><span class="math inline">\((n^{[1]}, 1)\)</span></td><td><span class="math inline">\(Z^{[1]} = W^{[1]} X + b^{[1]}\)</span></td><td><span class="math inline">\((n^{[1]}, 209)\)</span></td></tr><tr class="even"><td>Layer 1</td><td><span class="math inline">\((n^{[2]}, n^{[1]})\)</span></td><td><span class="math inline">\((n^{[2]}, 1)\)</span></td><td><span class="math inline">\(Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\)</span></td><td><span class="math inline">\((n^{[2]}, 209)\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\vdots\)</span></td><td><span class="math inline">\(\vdots\)</span></td><td><span class="math inline">\(\vdots\)</span></td><td><span class="math inline">\(\vdots\)</span></td><td><span class="math inline">\(\vdots\)</span></td></tr><tr class="even"><td>Layer 1</td><td><span class="math inline">\((n^{[L-1]}, n^{[L-2]})\)</span></td><td><span class="math inline">\((n^{[L-1]}, 1)\)</span></td><td><span class="math inline">\(Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]}\)</span></td><td><span class="math inline">\((n^{[L-1]}, 209)\)</span></td></tr><tr class="odd"><td>Layer 1</td><td><span class="math inline">\((n^{[L]}, n^{[L-1]})\)</span></td><td><span class="math inline">\((n^{[L]}, 1)\)</span></td><td><span class="math inline">\(Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]}\)</span></td><td><span class="math inline">\((n^{[L]}, 209)\)</span></td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>]) * np.sqrt(<span class="number">2</span>/layer_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>å‚æ•° <code>layer_dims</code> æ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«äº†å®šä¹‰çš„æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚çš„ç¥ç»å…ƒçš„ä¸ªæ•°ã€‚</p><h4 id="å‰å‘ä¼ æ’­æ¨¡å—">å‰å‘ä¼ æ’­æ¨¡å—</h4><p>åœ¨çº¿æ€§éƒ¨åˆ†å’Œæ¿€æ´»å‡½æ•°éƒ¨åˆ†ï¼Œå‰å‘ä¼ æ’­éƒ½ä¼šç¼“å­˜æ‰€æœ‰è¾“å…¥ï¼Œç”¨äºåå‘ä¼ æ’­æ—¶è®¡ç®—æ¢¯åº¦ã€‚</p><ul><li><p>çº¿æ€§å‰å‘ <span class="math display">\[ Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}, å…¶ä¸­ A^{[0]}=X \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line"></span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure></li><li><p>çº¿æ€§-æ¿€æ´»å‰å‘</p><ul><li>Sigmoid: <span class="math inline">\(g(Z)=\sigma(WA+b)=\frac{1}{1+e^{-(WA+b)}}\)</span></li><li>ReLU: <span class="math inline">\(g(Z)=ReLU(Z)=max(0, Z)\)</span></li></ul><p><span class="math display">\[ A^{[l]}=g(W^{[l]}A^{[l-1]}+b^{[l]}) \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure></li><li><p>L å±‚å‰å‘æ¨¡å‹</p><p>å¾ªç¯ä½¿ç”¨æ¿€æ´»å‡½æ•°æ˜¯ ReLU çš„ <code>linear_activation_forward</code> L-1 æ¬¡ï¼Œå†ä½¿ç”¨æ¿€æ´»å‡½æ•°æ˜¯ Sigmoid çš„ <code>linear_activation_forward</code> 1 æ¬¡ï¼Œå°±å¯ä»¥æ„å»ºä¸€ä¸ª L å±‚ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚åœ¨å®éªŒè¿‡ç¨‹ä¸­ï¼Œéœ€è¦æŠŠæ¯å±‚çš„ç¼“å­˜éƒ½æ”¾åˆ°åŒä¸€ä¸ªç¼“å­˜åˆ—è¡¨ä¸­ï¼Œç„¶åè¿”å›è¾“å‡ºå’Œç¼“å­˜ï¼Œç”¨äºè®¡ç®—ä»£ä»·å‡½æ•°å’Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€‚</p><p><span class="math display">\[ \hat Y=A^{[L]}=\sigma(W^{[L]}A^{[L-1]}+b^{[L]}) \]</span></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                  <span class="comment"># number of layers in the neural network</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># [LINEAR -&gt; RELU]*(L-1)</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span> + str(l)], parameters[<span class="string">'b'</span> + str(l)], activation = <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; SIGMOID</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span> + str(L)], parameters[<span class="string">'b'</span> + str(L)], activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><h4 id="ä»£ä»·å‡½æ•°">ä»£ä»·å‡½æ•°</h4><p><span class="math display">\[ J=-\frac{1}{m}\sum\limits_{i=1}^{m}\left(y^{(i)}\log(a^{[L]\(i\)}) + (1-y^{(i)})\log(1- a^{[L]\(i\)})\right) \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    cost = -np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(<span class="number">1</span> - AL), <span class="number">1</span> - Y)) / m</span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h4 id="åå‘ä¼ æ’­æ¨¡å‹">åå‘ä¼ æ’­æ¨¡å‹</h4><p>åå‘ä¼ æ’­æ˜¯ç”¨æ¥è®¡ç®—ä»£ä»·å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•æ›´æ–°å‚æ•°åç»§ç»­å‰å‘ä¼ æ’­ï¼Œä½¿å¾—ä»£ä»·æ›´å°ã€‚åœ¨è®¡ç®—æ¢¯åº¦çš„æ—¶å€™éœ€è¦ç”¨åˆ°å‰å‘ä¼ æ’­ç¼“å­˜çš„è¾“å…¥ï¼š</p><ul><li><p>çº¿æ€§åå‘</p><p>å‡è®¾å·²ç»è®¡ç®—å‡ºå¯¼æ•° <span class="math inline">\(dZ^{[l]}=\frac{\partial \mathcal{L} }{\partial Z^{[l]}}\)</span>ï¼Œç°åœ¨éœ€è¦æ ¹æ® $dZ^{[l]} $ æ±‚ <span class="math inline">\(dW^{[l]}, db^{[l]}, dA^{[l-1]}\)</span>ã€‚</p><p><span class="math display">\[ dW^{[l]}=\frac{\partial \mathcal{L}}{\partial W^{[l]}} = \frac{1}{m}dZ^{[l]}A^{[l-1]\mathrm{T}} \]</span></p></li></ul><p><span class="math display">\[ db^{[l]}=\frac{\partial \mathcal{L} }{\partial b^{[l]}}=\frac{1}{m}\sum_{i = 1}^{m}dZ^{[l]\(i\)} \]</span></p><p><span class="math display">\[ dA^{[l-1]}=\frac{\partial \mathcal{L} }{\partial A^{[l-1]}}=W^{[l]\mathrm{T}}dZ^{[l]} \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = np.dot(dZ, A_prev.T) / m</span><br><span class="line">    db = np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><ul><li><p>çº¿æ€§-æ¿€æ´»åå‘ <span class="math display">\[ dZ^{[l]}= dA^{[l]} * g&#39;(Z^{[l]}) \]</span> ReLU å‡½æ•°çš„å¯¼æ•°å°±æ˜¯ä¸€ä¸ªç®€å•çš„åˆ†æ®µå‡½æ•°ï¼Œå®éªŒç›´æ¥åœ¨ <code>dnn_utils</code> æ¨¡å—ä¸­å®ç°ï¼Œåªè¦è°ƒç”¨ä»¥ä¸‹å‡½æ•°ï¼Œä¼ å…¥ <span class="math inline">\(dA^{[l]}\)</span> å’Œå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„ç¼“å­˜å°±å¯ä»¥ç›´æ¥è¿”å› <span class="math inline">\(dZ^{[l]}\)</span>:</p><ul><li>Sigmoid: <code>dZ = sigmoid_backward(dA, activation_cache)</code></li><li>ReLU: <code>dZ = relu_backward(dA, activation_cache)</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure></li><li><p>L å±‚åå‘æ¨¡å‹</p><p>åœ¨åå‘ä¼ æ’­çš„æ—¶å€™ï¼Œé¦–å…ˆéœ€è¦è®¡ç®—ä»£ä»·å‡½æ•°å¯¹æ¨¡å‹è¾“å‡º <span class="math inline">\(A^{[L]}\)</span>(å³ <span class="math inline">\(\hat Y\)</span>) çš„æ¢¯åº¦(è®¡ç®—å…¬å¼è§<a href="/2018/05/19/Neuron-network/">å•éšå±‚ç¥ç»ç½‘ç»œ</a>)ï¼Œç„¶åè°ƒç”¨ <code>linear_activation_backward</code> å‡½æ•°ï¼Œæœ€åè¿”å›è®¡ç®—å‡ºçš„æ¢¯åº¦åˆ—è¡¨ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">      grads = &#123;&#125;</span><br><span class="line">      L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">      m = AL.shape[<span class="number">1</span>]</span><br><span class="line">      Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">      dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">      current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">      grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># Loop from l=L-2 to l=0</span></span><br><span class="line">      <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">          <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">          <span class="comment"># Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">          current_cache = caches[l]</span><br><span class="line">          dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">2</span>)], current_cache, activation = <span class="string">"relu"</span>)</span><br><span class="line">          grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">          grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">          grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure></li></ul><h4 id="æ›´æ–°å‚æ•°">æ›´æ–°å‚æ•°</h4><p><span class="math display">\[ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \]</span></p><p><span class="math display">\[ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span> </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="l-å±‚ç¥ç»ç½‘ç»œ">L å±‚ç¥ç»ç½‘ç»œ</h3><p>åœ¨å®ç° L å±‚ç¥ç»ç½‘ç»œçš„å„ä¸ªæ¨¡å—åï¼Œç°åœ¨å°†å®ƒä»¬ç»„è£…æˆä¸€ä¸ª L å±‚ç½‘ç»œæ¨¡å‹ï¼Œé€šè¿‡ <code>layers_dims</code> æŒ‡å®šç½‘ç»œç»“æ„ï¼Œè®¾ç½®å­¦ä¹ ç‡ä¸º 0.0075 è¿­ä»£è®­ç»ƒæ•°æ® 3000 æ¬¡ï¼Œæœ€åè¿”å›è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span><span class="comment">#lr was 0.009</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []                         <span class="comment"># keep track of cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Parameters initialization. (â‰ˆ 1 line of code)</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost.</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="å‚è€ƒæ–‡çŒ®">å‚è€ƒæ–‡çŒ®</h2><p>[1] å´æ©è¾¾. DeepLearning.</p><p>[2] Christopher Olah. Neural Networks, Manifolds, and Topology. 2014</p><p>[3] X. Glorot, Y. Bengio, "Understanding the Difficulty of Training Deep Feedforward Neural Networks",Â <em>Proc. Conf. Artificial Intelligence and Statistics</em>, 2010.</p></div><div class="reward-container"><div>ç–å½±æ¨ªæ–œæ°´æ¸…æµ…ï¼Œæš—é¦™æµ®åŠ¨æœˆé»„æ˜</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> æ‰“èµ</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng å¾®ä¿¡æ”¯ä»˜"><p>å¾®ä¿¡æ”¯ä»˜</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng æ”¯ä»˜å®"><p>æ”¯ä»˜å®</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>æœ¬æ–‡ä½œè€…ï¼š</strong> Randy Peng</li><li class="post-copyright-link"> <strong>æœ¬æ–‡é“¾æ¥ï¼š</strong> <a href="https://pengzhendong.cn/2018/05/21/deep-neuron-network/" title="æ·±åº¦ç¥ç»ç½‘ç»œ">https://pengzhendong.cn/2018/05/21/deep-neuron-network/</a></li><li class="post-copyright-license"> <strong>ç‰ˆæƒå£°æ˜ï¼š</strong> æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ï¼</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/05/19/neuron-network/" rel="prev" title="å•éšå±‚ç¥ç»ç½‘ç»œ"><i class="fa fa-chevron-left"></i> å•éšå±‚ç¥ç»ç½‘ç»œ</a></div><div class="post-nav-item"> <a href="/2018/05/28/basic-concepts-in-machine-learning/" rel="next" title="æœºå™¨å­¦ä¹ åŸºç¡€">æœºå™¨å­¦ä¹ åŸºç¡€<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> æ–‡ç« ç›®å½•</li><li class="sidebar-nav-overview"> ç«™ç‚¹æ¦‚è§ˆ</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#å‰è¨€"><span class="nav-number">1.</span> <span class="nav-text">å‰è¨€</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ç”µè·¯ç†è®ºå’Œæ·±åº¦å­¦ä¹ "><span class="nav-number">2.</span> <span class="nav-text">ç”µè·¯ç†è®ºå’Œæ·±åº¦å­¦ä¹ </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹"><span class="nav-number">3.</span> <span class="nav-text">æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#relu"><span class="nav-number">3.1.</span> <span class="nav-text">ReLU</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#squashing-å‡½æ•°"><span class="nav-number">3.1.1.</span> <span class="nav-text">Squashing å‡½æ•°</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ç¥ç»å…ƒåæ­»"><span class="nav-number">3.1.2.</span> <span class="nav-text">ç¥ç»å…ƒåæ­»</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#å‚æ•°åˆå§‹åŒ–"><span class="nav-number">3.1.3.</span> <span class="nav-text">å‚æ•°åˆå§‹åŒ–</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#xavier-åˆå§‹åŒ–"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">Xavier åˆå§‹åŒ–</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#he-åˆå§‹åŒ–"><span class="nav-number">3.1.3.2.</span> <span class="nav-text">He åˆå§‹åŒ–</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#æ¨¡å‹ç»“æ„"><span class="nav-number">3.2.</span> <span class="nav-text">æ¨¡å‹ç»“æ„</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#åˆå§‹åŒ–æ¨¡å‹å‚æ•°"><span class="nav-number">3.2.1.</span> <span class="nav-text">åˆå§‹åŒ–æ¨¡å‹å‚æ•°</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#å‰å‘ä¼ æ’­æ¨¡å—"><span class="nav-number">3.2.2.</span> <span class="nav-text">å‰å‘ä¼ æ’­æ¨¡å—</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ä»£ä»·å‡½æ•°"><span class="nav-number">3.2.3.</span> <span class="nav-text">ä»£ä»·å‡½æ•°</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#åå‘ä¼ æ’­æ¨¡å‹"><span class="nav-number">3.2.4.</span> <span class="nav-text">åå‘ä¼ æ’­æ¨¡å‹</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#æ›´æ–°å‚æ•°"><span class="nav-number">3.2.5.</span> <span class="nav-text">æ›´æ–°å‚æ•°</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#l-å±‚ç¥ç»ç½‘ç»œ"><span class="nav-number">3.3.</span> <span class="nav-text">L å±‚ç¥ç»ç½‘ç»œ</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#å‚è€ƒæ–‡çŒ®"><span class="nav-number">4.</span> <span class="nav-text">å‚è€ƒæ–‡çŒ®</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">è·¯æ¼«æ¼«å…¶ä¿®è¿œå…® å¾å°†ä¸Šä¸‹è€Œæ±‚ç´¢</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">æ—¥å¿—</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">æ ‡ç­¾</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub â†’ https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter â†’ https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail â†’ mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook â†’ https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram â†’ https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="çŸ¥ä¹ â†’ https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> çŸ¥ä¹</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="å¾®åš â†’ https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> å¾®åš</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="å…³äº â†’ /about" target="_self"><i class="fa fa-user fa-fw"></i> å…³äº</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 â€“ <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="ç«™ç‚¹æ€»å­—æ•°">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="ç«™ç‚¹é˜…è¯»æ—¶é•¿">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>