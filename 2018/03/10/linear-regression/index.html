<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 不知不觉研一已经过去了一个学期，上学期真实忙得没有时间总结。天天忙着上课和做实验，随机过程和工程硕士数学确实有些收获，就是感觉上课的形式花的时间太多；模式识别和计算机网络体系结构做了几个实验，收获颇丰。说实话，在教学方面，清华的老师也强不到哪里去(听师兄说本部也差不多)，根本就不能吸引学生注意力，课堂气氛也不行。"><meta name="keywords" content="Machine Learning"><meta property="og:type" content="article"><meta property="og:title" content="线性回归"><meta property="og:url" content="https://pengzhendong.cn/2018/03/10/linear-regression/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 不知不觉研一已经过去了一个学期，上学期真实忙得没有时间总结。天天忙着上课和做实验，随机过程和工程硕士数学确实有些收获，就是感觉上课的形式花的时间太多；模式识别和计算机网络体系结构做了几个实验，收获颇丰。说实话，在教学方面，清华的老师也强不到哪里去(听师兄说本部也差不多)，根本就不能吸引学生注意力，课堂气氛也不行。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://pengzhendong.cn/2018/03/10/linear-regression/data.png"><meta property="og:image" content="https://pengzhendong.cn/2018/03/10/linear-regression/predict.png"><meta property="og:updated_time" content="2018-03-10T14:56:32.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="线性回归"><meta name="twitter:description" content="前言 不知不觉研一已经过去了一个学期，上学期真实忙得没有时间总结。天天忙着上课和做实验，随机过程和工程硕士数学确实有些收获，就是感觉上课的形式花的时间太多；模式识别和计算机网络体系结构做了几个实验，收获颇丰。说实话，在教学方面，清华的老师也强不到哪里去(听师兄说本部也差不多)，根本就不能吸引学生注意力，课堂气氛也不行。"><meta name="twitter:image" content="https://pengzhendong.cn/2018/03/10/linear-regression/data.png"><link rel="canonical" href="https://pengzhendong.cn/2018/03/10/linear-regression/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>线性回归 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/03/10/linear-regression/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 线性回归</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-03-10 22:00:12 / 修改时间：22:56:32" itemprop="dateCreated datePublished" datetime="2018-03-10T22:00:12+08:00">2018-03-10</time></span><span id="/2018/03/10/linear-regression/" class="post-meta-item leancloud_visitors" data-flag-title="线性回归" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>不知不觉研一已经过去了一个学期，上学期真实忙得没有时间总结。天天忙着上课和做实验，随机过程和工程硕士数学确实有些收获，就是感觉上课的形式花的时间太多；模式识别和计算机网络体系结构做了几个实验，收获颇丰。说实话，在教学方面，清华的老师也强不到哪里去(听师兄说本部也差不多)，根本就不能吸引学生注意力，课堂气氛也不行。</p><a id="more"></a><p>上学期也算是入门了机器学习吧，跟着吴恩达在 Coursera 上的 Machine Learning 学了半个学期，也看了一些书和论文。现在总结一下，不然就连最简单的线性回归还是一知半解，毕竟只有写出来，讲出来才是自己的。</p><h2 id="线性回归">线性回归</h2><blockquote><p>给定数据集 <span class="math inline">\(\boldsymbol{D}=\lbrace(\boldsymbol{x}^{(i)}, y^{(i)}); i=1, …, m\rbrace\)</span>，其中 <span class="math inline">\(\boldsymbol{x}^{(i)}=(x^{(i)}_1; x^{(i)}_2; …; x^{(i)}_d)\)</span>，<span class="math inline">\(y^{(i)}\in\mathbb{R}\)</span>，线性回归就是试图去学习线性模型以尽可能准确地根据输入 <span class="math inline">\(\boldsymbol{x}\)</span> 预测输出 <span class="math inline">\(y\)</span>。</p></blockquote><p>线性回归并不陌生，例如高中的时候学过的父母身高预测法，假设父母的身高 <span class="math inline">\(\boldsymbol{x}=(x_1; x_2)\)</span> 和子女的身高 <span class="math inline">\(y\)</span> 之间存在某种线性的关系(其实还和子女的性别有关，对于性别这种离散且不存在“序”的值，可以用一个 2 维向量表示：男(0; 1)、女(1; 0))，线性回归就是要根据统计的数据去学习这个假设(hypothesis)存在的关系 <span class="math inline">\(h_\boldsymbol{\theta}(\boldsymbol{x})=\theta_1x_1+\theta_2x_2+b=\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x}+b\)</span>。</p><p>假设存在 <span class="math inline">\(x_0=1\)</span> 且令 <span class="math inline">\(\theta_{0}=b\)</span>，则<span class="math inline">\(h_\boldsymbol{\theta}(\boldsymbol{x})=\theta_0x_0+\theta_1x_1+\theta_2x_2=\boldsymbol{\theta}^ \mathrm{T}\boldsymbol{x}\)</span>，线性回归就是要找到 <span class="math inline">\(\boldsymbol{\theta}\)</span>，使得预测结果尽可能准确。</p><p>以下实验数据来自于吴恩达在 Coursera 的 Machine Learning 课程的实验 ex1。</p><h3 id="ex-1">Ex 1</h3><p>一个餐厅的 CEO 考虑在不同的城市开一家新店，所以希望能根据城市人口的数量(先不考虑其他因素，即一元线性回归)预测商铺的利润 <span class="math inline">\(y\)</span> ，以决定在哪座城市开店。</p><p><img src="/2018/03/10/linear-regression/data.png"></p><p>商铺利润 <span class="math inline">\(y\)</span> 和城市人口数量 <span class="math inline">\(x_1\)</span> 之间大体上呈线性关系，所以可以使用线性回归的方法学习出这个关系，即找到 <span class="math inline">\(\boldsymbol{\theta}\)</span>，那么给定一个新的城市人口数时候，就可以根据 <span class="math inline">\(\boldsymbol{x}\)</span> 尽准确预测可能出商铺利润 <span class="math inline">\(h_\boldsymbol{\theta}(\boldsymbol{x})\)</span>。</p><h3 id="符号解释">符号解释</h3><ul><li><p><span class="math inline">\(\boldsymbol{x}^{(i)}\)</span>：“输入”变量，也叫做输入特征。例如 <span class="math inline">\(\boldsymbol{x}^{(i)}=(1; x^{(i)}_1)\)</span>, <span class="math inline">\(x^{(i)}_1\)</span> 就是第 <span class="math inline">\(i\)</span> 座城市的人口数量。 <span class="math display">\[ \boldsymbol{X}=\begin{bmatrix} - (\boldsymbol{x}^{(1)})^\mathrm{T} - \\\ . \\\ . \\\ . \\\ - (\boldsymbol{x}^{(m)})^\mathrm{T} - \end{bmatrix}=\begin{bmatrix} 1 &amp; x^{(1)}_1 \\\ . &amp; . \\\ . &amp; . \\\ . &amp; .\\\ 1 &amp; x^{(m)}_1 \end{bmatrix}\quad \]</span></p></li><li><p><span class="math inline">\(y^{(i)}\)</span>：“输出”，也叫做目标变量。例如第 <span class="math inline">\(i\)</span> 座城市的利润。 <span class="math display">\[ \boldsymbol{y}=\begin{bmatrix} y^{(1)} \\\ . \\\ . \\\ . \\\ y^{(m)} \end{bmatrix}\quad \]</span></p></li><li><p><span class="math inline">\((\boldsymbol{x}^{(i)}, y^{(i)})\)</span>：一个训练样本</p></li><li><p><span class="math inline">\(\boldsymbol{D}=\lbrace(\boldsymbol{x}^{(i)}, y^{(i)}); i=1,…,m)\rbrace\)</span>：<span class="math inline">\(m\)</span> 个训练样本组成的训练集，上标 <span class="math inline">\((i)\)</span> 表示样本在训练集中的索引，和指数没有关系</p></li></ul><p><span class="math display">\[ \boldsymbol{D}=\begin{bmatrix}\boldsymbol{X} &amp; \boldsymbol{y}\end{bmatrix}=\begin{bmatrix} 1 &amp; x^{(1)}_1 &amp; y^{(1)} \\\ . &amp; . &amp; . \\\ . &amp; . &amp; . \\\ . &amp; . &amp; . \\\ 1 &amp; x^{(m)}_1 &amp; y^{(m)} \end{bmatrix}\quad \]</span></p><h2 id="性能度量">性能度量</h2><p>如何找到 <span class="math inline">\(\boldsymbol{\theta}\)</span> 就需要了解什么样的 <span class="math inline">\(\boldsymbol{\theta}\)</span> 能使预测结果更准确。如果 <span class="math inline">\(h_\boldsymbol{\theta1}(\boldsymbol{x})\)</span> 与真实的结果 <span class="math inline">\(y\)</span> 之间的<font color="red" size="4">差别</font>比 <span class="math inline">\(h_\boldsymbol{\theta2}(\boldsymbol{x})\)</span> 与真实的结果 <span class="math inline">\(y\)</span> 之间的差别更小，那么 <span class="math inline">\(\boldsymbol{\theta1}\)</span> 就比 <span class="math inline">\(\boldsymbol{\theta2}\)</span> 更好，能使预测结果更准确。</p><p><img src="/2018/03/10/linear-regression/predict.png"></p><p>例如上图中明显 <span class="math inline">\(\boldsymbol{\theta1}\)</span> 就比 <span class="math inline">\(\boldsymbol{\theta2}\)</span> 更好，因为直观上使用 <span class="math inline">\(\boldsymbol{\theta1}\)</span> 预测出来的利润 <span class="math inline">\(h_\boldsymbol{\theta1}(\boldsymbol{x})\)</span> 与真实利润 <span class="math inline">\(y\)</span> 之间的差别更小，但是如何用数学语言衡量 <span class="math inline">\(h_\boldsymbol{\theta}(\boldsymbol{x})\)</span> 和 <span class="math inline">\(y\)</span> 之间的差别呢？</p><h3 id="损失函数代价函数和目标函数">损失函数、代价函数和目标函数</h3><blockquote><p><strong>损失函数</strong>是一种衡量<strong>损失</strong>和错误程度的<strong>函数</strong></p></blockquote><p>在机器学习领域，经常会出现损失函数、代价函数和目标函数，它们之间并没有严格的规定，然而它们的定义一般如下：</p><ol type="1"><li>损失函数(Loss function)：定义在单个训练样本上，衡量一个样本的输出与真实值差别，例如：<ul><li>平方损失(通常用于线性回归)：<span class="math inline">\(l_\boldsymbol{\theta}(i)=\left(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})-y^{(i)}\right)^2\)</span></li><li>铰链损失(用于 SVM，像铰链)：<span class="math inline">\(l_\boldsymbol{\theta}(i) = \max\left(0, 1-h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})y^{(i)}\right)\)</span></li><li>...</li></ul></li><li>代价函数(Cost function)：定义在整个训练集上，即选定参数 <span class="math inline">\(\boldsymbol{\theta}\)</span> 后对数据进行估计所要支付的代价加上一些惩罚函数(例如正则化项)，例如：<ul><li>均方误差(几何意义是“欧氏距离”)：<span class="math inline">\(MSE(\boldsymbol{\theta}) = \frac{1}{m} \sum_{i=1}^m\left(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})-y^{(i)}\right)^2\)</span></li><li>SVM 的代价函数：<span class="math inline">\(SVM(\boldsymbol{\theta}) = \|\boldsymbol{\theta}\|^2 + C \sum_{i=1}^m \xi^{(i)}\)</span></li><li>...</li></ul></li><li>目标函数(Objective function)：代价函数的推广，即需要优化的函数。可能是最大化，也可能是最小化(此时就是代价函数)，例如：<ul><li>似然函数：用最大似然估计评估模型参数(MLE)</li><li>后验：用最大后验估计模型参数</li><li>...</li></ul></li></ol><p>线性回归使用均方误差作为代价函数，因此可以算出中 <span class="math inline">\(MSE(\boldsymbol{\theta1})\)</span> 比 $MSE() $ 更小，即 <span class="math inline">\(\boldsymbol{\theta1}\)</span> 能使预测结果更准确。<span class="math inline">\(MSE(\boldsymbol{\theta1})\)</span> 为每个点到预测结果的距离（每个点与横坐标作垂线，与预测结果的交点）之和的平均：</p><p>但是在以均方误差作为性能度量的前提下，是不是还存在 <span class="math inline">\(\boldsymbol{\theta^{*}}\)</span> 能使预测结果 <span class="math inline">\(\boldsymbol{\theta1}\)</span> 的预测结果更准确？如何找到最准确的 <span class="math inline">\(\boldsymbol{\theta^{*}}\)</span> 是一个凸优化问题，更准确地说这是一个最小二乘问题。</p><p><span class="math display">\[\boldsymbol{\theta^{*}} = \arg \min_{\boldsymbol{\theta}}\frac{1}{m}\sum_{i=1}^m\left(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})-y^{(i)}\right)^{2}\]</span></p><h2 id="最小二乘问题">最小二乘问题</h2><p>线性回归的代价函数是一个二次函数且半正定，所以这是一个最小二乘问题。所以最小化 <span class="math inline">\(MSE(\boldsymbol{\theta})\)</span> 求解 <span class="math inline">\(\boldsymbol{\theta}\)</span> 的过程也叫做最小二乘“参数估计”。一元线性回归的代价函数图是一个“碗状”图。</p><h3 id="解析解">解析解</h3><p>求解线性最小二乘问题可以通过对代价函数求导，然后令导数为零可以得到 <span class="math inline">\(\boldsymbol{\theta}\)</span> 的解析解，但是由于在求解过程中会涉及到矩阵求逆的计算，对于 <span class="math inline">\(n\)</span> 维的的输入变量 <span class="math inline">\(\boldsymbol{x}=(x_0; x_1; …; x_{n-1})\)</span>，时间复杂度为 <span class="math inline">\(O(n^3)\)</span>，因此当 <span class="math inline">\(n&gt;10000\)</span> 时不推荐使用。其解析解为：</p><p><span class="math display">\[\boldsymbol{\theta}=(\boldsymbol{X}^ \mathrm{T}\boldsymbol{X})^{-1}\boldsymbol{X}^ \mathrm{T}\vec{y}\]</span></p><p>对于多元线性回归(输入变量维数大于 2)，如果数据的组数少于输入变量维数，那么矩阵 <span class="math inline">\(\boldsymbol{X}^ \mathrm{T}\boldsymbol{X}\)</span> 显然不满秩。很好理解，未知数的个数大于方程的个数，那么这个方程的解就不唯一，就会有多个解能使均方误差最小化，常见的解决方法就是引入正则化项(详情见《凸优化》第六章)。</p><p>求解过程涉及求一阶偏导数，一阶偏导数以一定方式排列成的矩阵又叫做<strong>雅可比(Jacobian)矩阵</strong>，所以在机器学习中一般使用 <span class="math inline">\(J(\boldsymbol{\theta})\)</span> 表示代价函数。</p><h3 id="梯度下降算法">梯度下降算法</h3><p>当 <span class="math inline">\(n&gt;10000\)</span> 或者最小二乘问题是非线性的，可以考虑使用梯度下降算法。梯度下降是迭代法的一种，常用于求解最小二乘问题。在最小化代价函数时，可以通过梯度下降法来一步步的迭代求解(通过一个已经找到的 <span class="math inline">\(\boldsymbol{\theta}\)</span> 和迭代公式去算更好的 <span class="math inline">\(\boldsymbol{\theta}\)</span>)，最后得到最小化的代价函数和模型参数值 <span class="math inline">\(\boldsymbol{\theta}\)</span>。</p><p>梯度下降算法涉及到求代价函数 <span class="math inline">\(J(\boldsymbol{\theta}) = \frac{1}{m} \sum_{i=1}^m\left(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})-y^{(i)}\right)^2\)</span> 的梯度(即<a href="https://www.zhihu.com/question/28684811/answer/159589897" target="_blank" rel="noopener">导数</a>)，为了计算方便，一般 <span class="math inline">\(J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^m\left(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})-y^{(i)}\right)^2\)</span>，根据链式求导法则得 <span class="math inline">\(J(\boldsymbol{\theta})\)</span> 在 <span class="math inline">\(\theta_j\)</span> 方向上的梯度的表达式为：</p><p><span class="math display">\[\nabla=\frac{\partial{J(\boldsymbol{\theta})}}{\partial{\theta_j}}=\frac{\partial{J(\boldsymbol{\theta})}}{\partial{h_\boldsymbol{\theta}(\boldsymbol{x})}}\frac{\partial{h_\boldsymbol{\theta}(\boldsymbol{x})}}{\partial{\theta_j}}=\frac{1}{m} \sum_{i=1}^m\left(h_\boldsymbol{\theta}(\boldsymbol{x}^{(i)})-y^{(i)}\right)x^{(i)}_j\]</span></p><p>所以梯度下降过程中，<span class="math inline">\(\theta_j\)</span> 的更新过程为：</p><p><span class="math display">\[\theta_j:=\theta_j-\alpha\frac{\partial{J(\boldsymbol{\theta})}}{\partial{\theta_j}}\]</span></p><p>其中 <span class="math inline">\(\alpha\)</span> 为学习率，即梯度下降的“步伐”大小，太大会错误最小值导致梯度上升，太小会导致下降速度太慢，一般开始的时候稍微大些然后逐渐变小，当一次迭代梯度下降小于 <span class="math inline">\(10^{-3}\)</span> 的时候就可以说是收敛了。其中 <span class="math inline">\(m\)</span> 为梯度下降时使用的样本个数，根据 <span class="math inline">\(m\)</span> 取值的不同梯度下降算法分为以下三种：</p><ol type="1"><li>批量梯度下降</li><li>随机梯度下降</li><li>小批量梯度下降</li></ol><h4 id="批量梯度下降batch-gradient-descent简称-bgd">批量梯度下降(Batch Gradient Descent，简称 BGD)</h4><p>梯度下降的时候使用所有样本来更新参数 <span class="math inline">\(\boldsymbol{\theta}\)</span>，最后会收敛到全局最优解，也易于并行实现。但是如果样本数目过多的时候，训练过程会很慢。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iteration</span><br><span class="line">    h = X * theta;</span><br><span class="line">    theta = theta - (alpha/m) * X' * (h - y);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="随机梯度下降stochastic-gradient-descent简称-sgd">随机梯度下降(Stochastic Gradient Descent，简称 SGD)</h4><p>梯度下降的时候使用一个样本来更新参数 <span class="math inline">\(\boldsymbol{\theta}\)</span>，不一定能收敛到全局最优解，也不易于并行实现，但是训练过程会很快。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iteration</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line">    	h = X(<span class="built_in">i</span>, :) * theta;</span><br><span class="line">    	theta = theta - alpha * X(<span class="built_in">i</span>, :)' * (h - y(<span class="built_in">i</span>));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>批量梯度下降和随机梯度下降的时间复杂度一样，但是对于迭代同样的次数，随机梯度下降中的参数更新的次数较多，所以收敛的速度就快。但是由于随机梯度下降计算的梯度是对于这一次所选取的这一个样本的平方损失的梯度，而不是全部样本的均方误差的梯度，所以计算的梯度可能不准确，所以最后不一定能收敛到全局最优点。</p><p>在数据量很大的情况下，单个样本的平方损失可能会很接近于全部样本的均方误差，那么随机梯度下降计算的梯度就会很准确，同时收敛的速度也很快。</p><h4 id="小批量梯度下降mini-batch-gradient-descent简称-mbgd">小批量梯度下降(Mini-batch Gradient Descent，简称 MBGD)</h4><p>结合了批量梯度下降和随机梯度下降，在梯度下降的时候使用一部分样本来更新参数 <span class="math inline">\(\boldsymbol{\theta}\)</span>。所以在数据集比较小的时候采用批量梯度下降算法，数据集比较大的时候采用随机梯度下降算法，一般情况下使用小批量梯度下降算法。</p><h2 id="总结">总结</h2><p>线性回归主要就是观察数据，发现它满足一定的线性关系，然后就去找出这个关系，让预测尽可能地准确。一般都是使用小批量梯度下降算法，通过最小化代价函数算出模型的参数，得到的模型就可以用来对新的数据进行预测。</p><h2 id="参考文献">参考文献</h2><p>[1] 周志华. 机器学习. 清华大学出版社. 2016.</p><p>[2] 吴恩达. 机器学习.</p><p>[4] Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. 人民邮电出版社. 2017.</p><p>[4] Stephen Boyd, Lieven Vandenberghe. 凸优化. 清华大学出版社. 2017.</p><p>[5] 关治, 陆金甫. 数值方法. 清华大学出版社. 2017.</p></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2018/03/10/linear-regression/" title="线性回归">https://pengzhendong.cn/2018/03/10/linear-regression/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2016/09/14/recursion/" rel="prev" title="递归"><i class="fa fa-chevron-left"></i> 递归</a></div><div class="post-nav-item"> <a href="/2018/04/26/logistic-regression/" rel="next" title="Logistic 回归和 Softmax 回归">Logistic 回归和 Softmax 回归<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归"><span class="nav-number">2.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ex-1"><span class="nav-number">2.1.</span> <span class="nav-text">Ex 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#符号解释"><span class="nav-number">2.2.</span> <span class="nav-text">符号解释</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#性能度量"><span class="nav-number">3.</span> <span class="nav-text">性能度量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数代价函数和目标函数"><span class="nav-number">3.1.</span> <span class="nav-text">损失函数、代价函数和目标函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最小二乘问题"><span class="nav-number">4.</span> <span class="nav-text">最小二乘问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解析解"><span class="nav-number">4.1.</span> <span class="nav-text">解析解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降算法"><span class="nav-number">4.2.</span> <span class="nav-text">梯度下降算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#批量梯度下降batch-gradient-descent简称-bgd"><span class="nav-number">4.2.1.</span> <span class="nav-text">批量梯度下降(Batch Gradient Descent，简称 BGD)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度下降stochastic-gradient-descent简称-sgd"><span class="nav-number">4.2.2.</span> <span class="nav-text">随机梯度下降(Stochastic Gradient Descent，简称 SGD)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#小批量梯度下降mini-batch-gradient-descent简称-mbgd"><span class="nav-number">4.2.3.</span> <span class="nav-text">小批量梯度下降(Mini-batch Gradient Descent，简称 MBGD)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">5.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>