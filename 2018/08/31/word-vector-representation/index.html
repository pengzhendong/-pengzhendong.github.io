<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 实验和上一篇博客都使用了 Embedding，这篇博客正好可以加深对词向量和嵌入矩阵的理解。发现吴恩达课程里面很多内容他说在实验里有，但是我却没找到，例如本节中的负采样，难道他也喜欢挖坑不喜欢填？"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="词向量表示"><meta property="og:url" content="https://pengzhendong.cn/2018/08/31/word-vector-representation/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 实验和上一篇博客都使用了 Embedding，这篇博客正好可以加深对词向量和嵌入矩阵的理解。发现吴恩达课程里面很多内容他说在实验里有，但是我却没找到，例如本节中的负采样，难道他也喜欢挖坑不喜欢填？"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://pengzhendong.cn/2018/08/31/word-vector-representation/cosine_sim.png"><meta property="og:image" content="https://pengzhendong.cn/2018/08/31/word-vector-representation/neutral.png"><meta property="og:image" content="https://pengzhendong.cn/2018/08/31/word-vector-representation/equalize.png"><meta property="og:updated_time" content="2018-08-31T12:18:30.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="词向量表示"><meta name="twitter:description" content="前言 实验和上一篇博客都使用了 Embedding，这篇博客正好可以加深对词向量和嵌入矩阵的理解。发现吴恩达课程里面很多内容他说在实验里有，但是我却没找到，例如本节中的负采样，难道他也喜欢挖坑不喜欢填？"><meta name="twitter:image" content="https://pengzhendong.cn/2018/08/31/word-vector-representation/cosine_sim.png"><link rel="canonical" href="https://pengzhendong.cn/2018/08/31/word-vector-representation/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>词向量表示 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2018/08/31/word-vector-representation/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 词向量表示</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2018-08-31 16:35:00 / 修改时间：20:18:30" itemprop="dateCreated datePublished" datetime="2018-08-31T16:35:00+08:00">2018-08-31</time></span><span id="/2018/08/31/word-vector-representation/" class="post-meta-item leancloud_visitors" data-flag-title="词向量表示" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>实验和上一篇博客都使用了 Embedding，这篇博客正好可以加深对词向量和嵌入矩阵的理解。发现吴恩达课程里面很多内容他说在实验里有，但是我却没找到，例如本节中的负采样，难道他也喜欢挖坑不喜欢填？</p><a id="more"></a><h2 id="词汇表征">词汇表征</h2><p>在自然语言处理领域一个很重要的概念就是词嵌入 (Word Embeddings)，这是语言表示的一种方式，可以让算法理解一些<strong>类似</strong>的词。在使用词嵌入以前，通常用的都是词汇表，也就是将一个个单词变成独热向量，但是这样的表示没有办法让模型理解相似的词，引用夏树涛老师的一句话就是：独热向量的欧氏距离是没有意义的！</p><p>因此很有必要使用特征化的表示来表示每个词，也就是学习给每个词进行分类，例如词汇量为 5 的词汇表对应的嵌入矩阵如下所示：</p><table><thead><tr class="header"><th></th><th>Man</th><th>Woman</th><th>King</th><th>Queen</th><th>Apple</th><th>Orange</th></tr></thead><tbody><tr class="odd"><td>Gender</td><td>-1</td><td>1</td><td>-0.95</td><td>0.97</td><td>0.00</td><td>0.01</td></tr><tr class="even"><td>Royal</td><td>0.01</td><td>0.02</td><td>0.93</td><td>0.95</td><td>-0.01</td><td>0.00</td></tr><tr class="odd"><td>Age</td><td>0.03</td><td>0.02</td><td>0.7</td><td>0.69</td><td>0.03</td><td>-0.02</td></tr><tr class="even"><td>Food</td><td>0.09</td><td>0.01</td><td>0.02</td><td>0.01</td><td>0.95</td><td>0.97</td></tr></tbody></table><p>对于词汇表中任意单词的独热向量 <span class="math inline">\(O_i\)</span>，它的大小是 <span class="math inline">\(5\times 1\)</span> ，嵌入矩阵 <span class="math inline">\(E\)</span> 大小为 <span class="math inline">\(4\times 5\)</span> ， 那么 <span class="math inline">\(EO_i\)</span> 就可以得到词汇表中单词 <span class="math inline">\(i\)</span> 的嵌入向量。 模型在遇到 Apple 和 Orange 的时候就可以计算两个向量的余弦相似度，从而知道它们都是食物。在实践中可以使用 t-SNE 算法可视化高维特征向量，这个算法会将高维向量映射到一个二维空间中。</p><h3 id="余弦相似度">余弦相似度</h3><p>为了衡量两个单词的相似性，我们需要一种衡量两个单词的嵌入向量的方法。给定两个向量 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(v\)</span>，其余弦相似度定义为： <span class="math display">\[ \text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta) \]</span> 其中分子是两个向量的点乘，分母是两个向量的二范数的乘积，<span class="math inline">\(\theta\)</span> 是两个向量形成的角度。两个向量越相似，余弦相似度就越接近于 1，不相似则取值会很小。图像如下图所示：</p><p><img src="/2018/08/31/word-vector-representation/cosine_sim.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    distance = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the dot product between u and v (≈1 line)</span></span><br><span class="line">    dot = np.dot(u, v)</span><br><span class="line">    <span class="comment"># Compute the L2 norm of u (≈1 line)</span></span><br><span class="line">    norm_u = np.sqrt(np.sum(np.power(u,<span class="number">2</span>)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the L2 norm of v (≈1 line)</span></span><br><span class="line">    norm_v = np.sqrt(np.sum(np.power(v,<span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># Compute the cosine similarity defined by formula (1) (≈1 line)</span></span><br><span class="line">    cosine_similarity = np.divide(dot, norm_u * norm_v)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure><h3 id="学习词嵌入">学习词嵌入</h3><p>通常在使用词嵌入的时候，可以针对自己的数据集训练（也就是学习上面表格中的嵌入矩阵 <span class="math inline">\(E\)</span>），如果数据集不是很充分也可以从网上下载训练好的词嵌入模型。在实践中通常是建立一个语言模型进行学习词嵌入（也就是说不是单独地去训练词嵌入），例如使用神经网络预测序列的下一个单词，I want a glass of orange __。 在实践中通常的做法是使用一个固定的历史窗口，例如超参数窗口大小为 4，那么就只用前面 4 个单词来预测下一个单词。嵌入矩阵也是一个参数，可以在训练过程中学习出来。如果训练集中的句子比较复杂还可以考虑上下文，即用前面四个词和后面四个词来预测中间的词。所以如果使用预训练的嵌入矩阵，那么在这个步骤就可以再训练一下，或者不训练（把它当成超参数）直接使用。</p><h4 id="word2vec">Word2Vec</h4><p>Word2Vec 算法有两种模型 Skip-grams 和 CBOW，视频中只介绍了 Skip-grams，因为它在大型语料库中表现更好。这个模型的做法是随机选择一个单词 <span class="math inline">\(O_c\)</span> 作为上下文，然后在一定的词距内随机选另一个词 <span class="math inline">\(O_t\)</span> 作为待预测的词，即目标词，然后进行监督学习。虽然不太容易预测，但是这个模型可以很好地学习出嵌入矩阵，其中 <span class="math inline">\(e_c=EO_c\)</span>，对于 10,000 个单词的词汇表，softmax 预测目标词 <span class="math inline">\(O_t\)</span> 的概率为： <span class="math display">\[ P(O_t|O_c) = \frac{e^{\theta_{t}^{T}e_{c}}}{\sum_{i=1}^{10,000}e^{\theta_{i}^{T}e_{c}}} \]</span> 其中 <span class="math inline">\(\theta_t\)</span> 是判断输出为 <span class="math inline">\(O_t\)</span> 这个类别的参数。损失函数为： <span class="math display">\[ L(\hat y,y)=-\sum_{i=1}^{10,000}{y_{i}\log\hat y_{i}} \]</span> 损失函数中的 <span class="math inline">\(\hat y\)</span> 和 <span class="math inline">\(y\)</span> 都是独热向量。在计算概率的时候，分母要累加所有词汇在给定词汇情况下的概率，所以词汇量比较大的时候计算量比较大，因此可以采用分级 softmax 分类器或者<strong>负采样</strong>。分级 softmax 分类器的思想是使用霍夫曼树，先判断词属于前 5000 个还是后 5000 个，然后继续分析，最后时间复杂度就从 N 变成 logN。 不过需要注意的是，在实践中使用的不是完全平衡的分类树，而且通常常用词会放在树根。详细的内容可以参考原文献[2]。</p><h4 id="负采样">负采样</h4><p>Skim-grams 其实就是学习从 <span class="math inline">\(x\)</span> 映射到 <span class="math inline">\(y\)</span> 的监督模型，只不过时间复杂度有点大。而负采样需要构造一个新的监督学习问题，即给定一对单词，例如 orange 和 juice，预测它们是否属于一对上下文-目标词。例如有一个句子：I want a glass of orange juice to go along with my cereal.</p><p>首先从句子中采样得到一个上下文词 orange 和一个目标词 juice，然后标记为 1；然后去字典中随机选 k （这里 k=4）个单词，标记为 0（即使 of 也出现在句子中）：</p><table><thead><tr class="header"><th>Context</th><th>Word</th><th>Target?</th></tr></thead><tbody><tr class="odd"><td>orange</td><td>juice</td><td>1</td></tr><tr class="even"><td>orange</td><td>king</td><td>0</td></tr><tr class="odd"><td>orange</td><td>book</td><td>0</td></tr><tr class="even"><td>orange</td><td>the</td><td>0</td></tr><tr class="odd"><td>orange</td><td>of</td><td>0</td></tr></tbody></table><p>给定输入的上下文词 <span class="math inline">\(O_c\)</span> 和可能的目标词 <span class="math inline">\(O_t\)</span> ，定义一个逻辑回归模型，判断输出： <span class="math display">\[ P(y=1|c,t)=\sigma(\theta_t^Te_c) \]</span> 即每个正样本都有 K 个对应的负样本来训练一个逻辑回归模型，相对而言每次迭代的成本更低，详细内容可以参考原文献[3]。在负采样的时候如果均匀采样，则学不到单词的分布，如果根据单词的频率采样又可能导致一些介词的频率很高，因此通常介于这两者之间： <span class="math display">\[ P(\omega_i)=\frac{f(\omega_i)^{\frac{3}{4}}}{\sum_{j=1}^{10,000}f(\omega_i)^{\frac{3}{4}}} \]</span> 其中 <span class="math inline">\(f(\omega_i)\)</span> 是语料库中某个单词的词频。</p><h4 id="glove-词向量">GloVe 词向量</h4><p>GloVe 表示<strong>用于词表示的全局变量</strong>（Global vectors for word representation），假设 <span class="math inline">\(X_{ij}\)</span> 为单词 <span class="math inline">\(i\)</span> 在上下文词 <span class="math inline">\(j\)</span> 中出现的次数（即两个词出现在同一个窗口中的次数）。如果上下文词和目标词的范围定义为左右各 10 各词的话，根据定义有 <span class="math inline">\(X_{ij}=X_{ji}\)</span>，矩阵 <span class="math inline">\(X\)</span> 也叫做语料库的共现矩阵。GloVe 就是要最小化： <span class="math display">\[ \text{minimize}\sum_{i=1}^{10,000}\sum_{j=1}^{10,000}f(X_{ij})(\theta_i^Te_j+b_i+\tilde{b_j}-logX_{ij})^2 \]</span> 其中 <span class="math inline">\(b_i\)</span> 和 <span class="math inline">\(\tilde{b_j}\)</span> 是两个词向量的偏置项， 权重函数 <span class="math inline">\(f(X_{ij})\)</span> 是一个截断函数： <span class="math display">\[ f(x) = \begin{cases} (x/x_{max})^\alpha &amp; \text{if $x&lt;x_{max}$ } \\\ 1 &amp; \text{otherwise} \end{cases} \]</span> 原文献中 <span class="math inline">\(\alpha\)</span> 的取值都是 0.75，而 <span class="math inline">\(x_{max}\)</span> 取值都是 100，损失函数的详细推导过程可以参考原文献[4]。</p><h2 id="单词类比任务">单词类比任务</h2><p><strong>man is to woman as king is to queen</strong>，即给定单词 a(man)、b(woman) 和 c(king)，需要找到一个单词 d 满足 <span class="math inline">\(e_b - e_a \approx e_d - e_c\)</span>。这里衡量 <span class="math inline">\(e_b - e_a\)</span> 就用余弦相似度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a, word_b, word_c, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="comment"># convert words to lower case</span></span><br><span class="line">    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the word embeddings v_a, v_b and v_c (≈1-3 lines)</span></span><br><span class="line">    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]</span><br><span class="line">    </span><br><span class="line">    words = word_to_vec_map.keys()</span><br><span class="line">    max_cosine_sim = <span class="number">-100</span>              <span class="comment"># Initialize max_cosine_sim to a large negative number</span></span><br><span class="line">    best_word = <span class="literal">None</span>                   <span class="comment"># Initialize best_word with None, it will help keep track of the word to output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over the whole word vector set</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:        </span><br><span class="line">        <span class="comment"># to avoid best_word being one of the input words, pass on them.</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> [word_a, word_b, word_c] :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)</span></span><br><span class="line">        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the cosine_sim is more than the max_cosine_sim seen so far,</span></span><br><span class="line">            <span class="comment"># then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> cosine_sim &gt; max_cosine_sim:</span><br><span class="line">            max_cosine_sim = cosine_sim</span><br><span class="line">            best_word = w</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_word</span><br></pre></td></tr></table></figure><h2 id="去偏词向量">去偏词向量</h2><p>首先计算一个向量 <span class="math inline">\(g = e_{woman}-e_{man}\)</span>，这个向量可以粗略地看成是性别 <strong>g</strong>ender。或者可以同时计算:</p><ul><li><p><span class="math inline">\(g_1 = e_{mother}-e_{father}\)</span></p></li><li><p><span class="math inline">\(g_2 = e_{girl}-e_{boy}\)</span></p></li></ul><p>最后取这三个向量的均值作为性别则会更加精确。可以通过以下代码验证我们的想法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name_list = [<span class="string">'john'</span>, <span class="string">'marie'</span>, <span class="string">'sophie'</span>, <span class="string">'ronaldo'</span>, <span class="string">'priya'</span>, <span class="string">'rahul'</span>, <span class="string">'danielle'</span>, <span class="string">'reza'</span>, <span class="string">'katy'</span>, <span class="string">'yasmin'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> name_list:</span><br><span class="line">    <span class="keyword">print</span> (w, cosine_similarity(word_to_vec_map[w], g))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">List of names and their similarities with constructed vector:</span><br><span class="line">john [-0.23163356]</span><br><span class="line">marie [0.31559794]</span><br><span class="line">sophie [0.3186879]</span><br><span class="line">ronaldo [-0.31244797]</span><br><span class="line">priya [0.17632042]</span><br><span class="line">rahul [-0.16915471]</span><br><span class="line">danielle [0.24393299]</span><br><span class="line">reza [-0.0793043]</span><br><span class="line">katy [0.28310687]</span><br><span class="line">yasmin [0.23313858]</span><br></pre></td></tr></table></figure><p>可以看出，一些比较女性化的名字和 <span class="math inline">\(g\)</span> 的相似性大于0，比较男性化的名字和 <span class="math inline">\(g\)</span> 的相似性则小于 0。</p><h3 id="中和无性别单词的偏差">中和无性别单词的偏差</h3><p>下面是一些词和性别的相似性，虽然大部分的工程师是男性，但是这有点性别歧视了，而且这些词本身是不应该有性别之分的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">receptionist [0.33077942]</span><br><span class="line">technology [-0.13193732]</span><br><span class="line">teacher [0.17920923]</span><br><span class="line">engineer [-0.0803928]</span><br></pre></td></tr></table></figure><p>假如词嵌入是 50 维，则可以分为两部分：偏置方向 <span class="math inline">\(g\)</span> 和其余的 49 维 <span class="math inline">\(g_{\perp}\)</span>。其余的 49 维与性别无关，所以是正交的。下面的任务就是把向量 <span class="math inline">\(e_{receptionist}\)</span> 的 <span class="math inline">\(g\)</span> 方向置 0，得到 <span class="math inline">\(e_{receptionist}^{debiased}\)</span>。如下图所示：</p><p><img src="/2018/08/31/word-vector-representation/neutral.png"> <span class="math display">\[ e^{bias\\_component} = \frac{e \cdot g}{||g||_2^2} * g \]</span></p><p><span class="math display">\[ e^{debiased} = e - e^{bias\\_component} \]</span></p><p><span class="math inline">\(e^{bias\\_component}\)</span> 也就是 <span class="math inline">\(e\)</span> 在方向 <span class="math inline">\(g\)</span> 上的投影。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word, g, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="comment"># Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line)</span></span><br><span class="line">    e = word_to_vec_map[word]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute e_biascomponent using the formula give above. (≈ 1 line)</span></span><br><span class="line">    e_biascomponent = np.divide(np.dot(e, g), np.linalg.norm(g)**<span class="number">2</span>) * g</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Neutralize e by substracting e_biascomponent from it </span></span><br><span class="line">    <span class="comment"># e_debiased should be equal to its orthogonal projection. (≈ 1 line)</span></span><br><span class="line">    e_debiased = e - e_biascomponent</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e_debiased</span><br></pre></td></tr></table></figure><h3 id="性别专用词均衡算法">性别专用词均衡算法</h3><p>均衡算法可以应用于两个只有性别之分的词。例如男演员 (actor) 和女演员 (actress)，可能女演员更接近保姆 (babysit)，通过对 babysit 的中和可以减少保姆和性别的关联性，但是还是不能保证这两种演员和其他词的关联性是否相同。均衡算法就可以处理这个问题，均衡算法的原理如下图所示：</p><p><img src="/2018/08/31/word-vector-representation/equalize.png"></p><p>原理就是保证这两个词到 49 维的 <span class="math inline">\(g_\perp\)</span> 的距离相等，公式参考 Bolukbasi et al., 2016： <span class="math display">\[ \mu = \frac{e_{w1} + e_{w2}}{2} \]</span></p><p><span class="math display">\[ \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis} \]</span></p><p><span class="math display">\[ \mu_{\perp} = \mu - \mu_{B} \]</span></p><p><span class="math display">\[ e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis} \]</span></p><p><span class="math display">\[ e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis} \]</span></p><p><span class="math display">\[ e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {|(e_{w1} - \mu_{\perp}) - \mu_B)|} \]</span></p><p><span class="math display">\[ e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {|(e_{w2} - \mu_{\perp}) - \mu_B)|} \]</span></p><p><span class="math display">\[ e_1 = e_{w1B}^{corrected} + \mu_{\perp} \]</span></p><p><span class="math display">\[ e_2 = e_{w2B}^{corrected} + \mu_{\perp} \]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pair, bias_axis, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="comment"># Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines)</span></span><br><span class="line">    w1, w2 = pair</span><br><span class="line">    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)</span></span><br><span class="line">    mu = (e_w1 + e_w2) / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)</span></span><br><span class="line">    mu_B = np.divide(np.dot(mu, bias_axis), np.linalg.norm(bias_axis)**<span class="number">2</span>) * bias_axis</span><br><span class="line">    mu_orth = mu - mu_B</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)</span></span><br><span class="line">    e_w1B = np.divide(np.dot(e_w1, bias_axis), np.linalg.norm(bias_axis)**<span class="number">2</span>) * bias_axis</span><br><span class="line">    e_w2B = np.divide(np.dot(e_w2, bias_axis), np.linalg.norm(bias_axis)**<span class="number">2</span>) * bias_axis</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)</span></span><br><span class="line">    corrected_e_w1B = np.sqrt(np.abs(<span class="number">1</span> - np.sum(mu_orth**<span class="number">2</span>))) * np.divide(e_w1B - mu_B, np.abs(e_w1 - mu_orth - mu_B))</span><br><span class="line">    corrected_e_w2B = np.sqrt(np.abs(<span class="number">1</span> - np.sum(mu_orth**<span class="number">2</span>))) * np.divide(e_w2B - mu_B, np.abs(e_w2 - mu_orth - mu_B))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)</span></span><br><span class="line">    e1 = corrected_e_w1B + mu_orth</span><br><span class="line">    e2 = corrected_e_w2B + mu_orth</span><br><span class="line">                                                                </span><br><span class="line">    <span class="keyword">return</span> e1, e2</span><br></pre></td></tr></table></figure><p>通过均衡算法，两个只有性别之分的词和性别的相似度应该大致成相反数的关系。</p><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li><li>Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013.</li><li>Mikolov T, Sutskever I, Chen K, et al. Distributed Representations of Words and Phrases and their Compositionality[J]. 2013, 26:3111-3119.</li><li>Pennington J, Socher R, Manning C. Glove: Global Vectors for Word Representation[C]// Conference on Empirical Methods in Natural Language Processing. 2014:1532-1543.</li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2018/08/31/word-vector-representation/" title="词向量表示">https://pengzhendong.cn/2018/08/31/word-vector-representation/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2018/08/31/emojify/" rel="prev" title="Emojify 文本情感分析"><i class="fa fa-chevron-left"></i> Emojify 文本情感分析</a></div><div class="post-nav-item"> <a href="/2018/09/10/neural-machine-translation/" rel="next" title="自然机器翻译">自然机器翻译<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词汇表征"><span class="nav-number">2.</span> <span class="nav-text">词汇表征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#余弦相似度"><span class="nav-number">2.1.</span> <span class="nav-text">余弦相似度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习词嵌入"><span class="nav-number">2.2.</span> <span class="nav-text">学习词嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#word2vec"><span class="nav-number">2.2.1.</span> <span class="nav-text">Word2Vec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#负采样"><span class="nav-number">2.2.2.</span> <span class="nav-text">负采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#glove-词向量"><span class="nav-number">2.2.3.</span> <span class="nav-text">GloVe 词向量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#单词类比任务"><span class="nav-number">3.</span> <span class="nav-text">单词类比任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#去偏词向量"><span class="nav-number">4.</span> <span class="nav-text">去偏词向量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#中和无性别单词的偏差"><span class="nav-number">4.1.</span> <span class="nav-text">中和无性别单词的偏差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#性别专用词均衡算法"><span class="nav-number">4.2.</span> <span class="nav-text">性别专用词均衡算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">5.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>