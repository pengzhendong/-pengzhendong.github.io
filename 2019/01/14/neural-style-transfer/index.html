<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"pengzhendong.cn",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"39IHYBUVGR",apiKey:"4287c8f8a629343c8d2212e108417ceb",indexName:"Notes",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="前言 我觉得卷积神经网络最神奇的应用就是风格迁移！大部分应用的思想都相差无几，重点就是如何构造损失函数，将我们的目标用损失函数的方式表示，让模型按照指定的方向去学习。"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="风格迁移"><meta property="og:url" content="https://pengzhendong.cn/2019/01/14/neural-style-transfer/index.html"><meta property="og:site_name" content="Randy&#39;s Notes"><meta property="og:description" content="前言 我觉得卷积神经网络最神奇的应用就是风格迁移！大部分应用的思想都相差无几，重点就是如何构造损失函数，将我们的目标用损失函数的方式表示，让模型按照指定的方向去学习。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://pengzhendong.cn/2019/01/14/neural-style-transfer/example_bvlc-googlenet_bus.png"><meta property="og:image" content="https://pengzhendong.cn/2019/01/14/neural-style-transfer/perspolis_vangogh.png"><meta property="og:image" content="https://pengzhendong.cn/2019/01/14/neural-style-transfer/reshape_loss.png"><meta property="og:image" content="https://pengzhendong.cn/2019/01/14/neural-style-transfer/NST_GM.png"><meta property="og:updated_time" content="2019-01-14T10:42:42.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="风格迁移"><meta name="twitter:description" content="前言 我觉得卷积神经网络最神奇的应用就是风格迁移！大部分应用的思想都相差无几，重点就是如何构造损失函数，将我们的目标用损失函数的方式表示，让模型按照指定的方向去学习。"><meta name="twitter:image" content="https://pengzhendong.cn/2019/01/14/neural-style-transfer/example_bvlc-googlenet_bus.png"><link rel="canonical" href="https://pengzhendong.cn/2019/01/14/neural-style-transfer/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><style type="text/css">body{background-image:url(/images/rockywall.png)}</style><title>风格迁移 | Randy's Notes</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-92548519-1"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-92548519-1")}</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?06c54470f22c395ef480d6fb358497d5";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Randy's Notes</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-友链"><a href="/friends/" rel="section"><i class="fa fa-users fa-fw"></i> 友链</a></li><li class="menu-item menu-item-书单"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i> 书单</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/pengzhendong" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://pengzhendong.cn/2019/01/14/neural-style-transfer/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Randy Peng"><meta itemprop="description" content="路漫漫其修远兮 吾将上下而求索"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Randy's Notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 风格迁移</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-01-14 16:16:19 / 修改时间：18:42:42" itemprop="dateCreated datePublished" datetime="2019-01-14T16:16:19+08:00">2019-01-14</time></span><span id="/2019/01/14/neural-style-transfer/" class="post-meta-item leancloud_visitors" data-flag-title="风格迁移" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>0</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2><p>我觉得卷积神经网络最神奇的应用就是风格迁移！大部分应用的思想都相差无几，重点就是如何构造损失函数，将我们的目标用损失函数的方式表示，让模型按照指定的方向去学习。</p><a id="more"></a><h2 id="可视化">可视化</h2><p>在学习风格迁移之前，首先了解一下卷积神经网络的可视化。训练好的 CNN 模型的隐藏层中的每一个滤波器对应一种特征，每一个滤波器与输入的图像进行卷积运算后经过激活层。如果输入的图像具有该滤波器对应的特征，那么经过激活层后就会被激活，即输出特征图对应的数值大于 0。可视化过程涉及到反卷积和反池化，具体过程可参考 Visualizing and Understanding Convolutional Networks [2]。<a href="https://github.com/yosinski/deep-visualization-toolbox" target="_blank" rel="noopener">DeepVis Toolbox</a> 是一个开源的可视化工具，可视化结果如下图所示：</p><p><img src="/2019/01/14/neural-style-transfer/example_bvlc-googlenet_bus.png"></p><p>图中可视化的是 GoogleNet，输入为一张公交车的图像，每个小方块表示一个滤波器。将滤波器反卷积和反池化回原图像，结果如左下角所示。</p><h2 id="风格迁移">风格迁移</h2><p>给定一张内容图像 C 和一张风格图像 S，风格迁移模型生成一张具有 C 的内容和 S 的风格图像 G。如下图所示：</p><p><img src="/2019/01/14/neural-style-transfer/perspolis_vangogh.png"></p><h3 id="迁移学习">迁移学习</h3><p>风格迁移任务中，需要提取图像的内容特征和风格特征，然后根据特征生成图像（初始化为一张随机噪声图）。通过构造损失函数，令模型学习生成的图像 G 具有 C 的内容和 S 的风格，训练完毕后给定任意两张图像都能生成它们的风格迁移图像。实验使用了迁移学习提取图像特征，首先在 ImageNet 上预训练了一个用于分类的 VGG-19 网络，然后直接应用过来提取图像特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = load_vgg_model(<span class="string">"pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br></pre></td></tr></table></figure><p>使用 <code>tf.assign</code> 函数为模型输入数据，获取模型中间隐藏层的输出如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model[<span class="string">"input"</span>].assign(image)</span><br><span class="line">sess.run(model[<span class="string">"conv4_2"</span>])</span><br></pre></td></tr></table></figure><h3 id="代价函数">代价函数</h3><p>风格迁移的代价函数分为两部分：内容代价函数 <span class="math inline">\(J_{content}(C,G)\)</span> 和风格代价函数 <span class="math inline">\(J_{style}(S,G)\)</span>。完整的代价函数为： <span class="math display">\[ J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G) \]</span></p><p>其中 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 是超参数，代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span>:</span></span><br><span class="line">    J = alpha * J_content + beta * J_style</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><h4 id="内容代价函数">内容代价函数</h4><p>由可视化可知，通常浅层的滤波器提取的特征都是一些简单的特征，例如边角和纹理；比较深的、靠近全连接层的滤波器提取的特征就比较高级，例如一些复杂的纹理或者对象的类别。因此我们需要将比较中间的卷积层的输出作为图像的内容特征，假设选择的层数为 <span class="math inline">\(l\)</span>，图像 C 经过该层激活函数后的输出为 <span class="math inline">\(a^{[l](C)}\)</span>，为了表示方便，后续内容将省略层数，用 <span class="math inline">\(a^{(C)}\)</span> 表示图像 C 的内容特征，同时后续内容实验会测试不同 <span class="math inline">\(l\)</span> 取值的影响。</p><p>那么如何衡量生成的图像 G 和 C 之间的内容匹配了多少？内容代价函数比较简单，就是计算 C 和 G 的内容特征图每个像素点的差异，然后进行归一化。计算公式如下所示： <span class="math display">\[ J_{content}(C,G) = \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2 \]</span> 其中 <span class="math inline">\(n_H\)</span>、<span class="math inline">\(n_W\)</span> 和 <span class="math inline">\(n_C\)</span> 分别表示特征图的高、宽和通道数。为了<strong>便于理解</strong>，将 3 维的特征图展开成两维，如下所示：</p><p><img src="/2019/01/14/neural-style-transfer/reshape_loss.png"></p><p>由于 <code>reshape</code> 只是修改维度，而不改变填充顺序，因此需要先使用 <code>transpose</code> 对矩阵进行转置。使用 Tensorflow 实现内容代码函数分为以下三个步骤：</p><ol type="1"><li>获取图像维度</li><li>展开 <span class="math inline">\(a_C\)</span> 和 <span class="math inline">\(a_G\)</span></li><li>计算内容损失</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span><span class="params">(a_C, a_G)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    <span class="comment"># Reshape a_C and a_G (≈2 lines)</span></span><br><span class="line">    a_C_unrolled = tf.reshape(tf.transpose(a_C, [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]), [n_C, n_H * n_W, m])</span><br><span class="line">    a_G_unrolled = tf.reshape(tf.transpose(a_G, [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]), [n_C, n_H * n_W, m])</span><br><span class="line">    <span class="comment"># compute the cost with tensorflow (≈1 line)</span></span><br><span class="line">    J_content = (<span class="number">1</span>/ (<span class="number">4</span>* n_H * n_W * n_C)) * tf.reduce_sum(tf.pow((a_G_unrolled - a_C_unrolled), <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_content</span><br></pre></td></tr></table></figure><p>计算过程中展开和不展开并不会影响矩阵元素之间的计算，而且 <code>transpose</code> 函数默认的参数 <code>perm</code> 可以省略。</p><h4 id="风格代价函数">风格代价函数</h4><p>图像的风格定义为 <span class="math inline">\(l\)</span> 层中各个通道之间激活项的相关系数，即风格矩阵（也叫 Gram 矩阵）。这里有个小问题就是风格矩阵用 <span class="math inline">\(G\)</span> 表示，生成的图像也是用 <span class="math inline">\(G\)</span> 表示。</p><h5 id="gram-矩阵">Gram 矩阵</h5><p>给定展开成两维的特征图矩阵，其由 <span class="math inline">\(n_C\)</span> 个横向量<span class="math inline">\((v_{1},\dots ,v_{n_H\times n_W})\)</span> 组成。根据定义，Gram 矩阵中每个元素的值 <span class="math inline">\({\displaystyle G_{ij} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j}) }\)</span>，即 <span class="math inline">\(G_{ij}\)</span> 衡量滤波器 <span class="math inline">\(i\)</span> 的激活值 <span class="math inline">\(v_i\)</span> 和滤波器 <span class="math inline">\(j\)</span> 的激活值 <span class="math inline">\(v_j\)</span> 的相似性，如下图所示：</p><p><img src="/2019/01/14/neural-style-transfer/NST_GM.png"></p><p>输出的 Gram 矩阵的维度为 <span class="math inline">\((n_C, n_C)\)</span>，值得注意的是 <span class="math inline">\(G_{ii} = v_{i}^T v_{i}\)</span> 衡量的是图像中滤波器 <span class="math inline">\(i\)</span> 对应的特征的活跃性。假设 <span class="math inline">\(i\)</span> 对应水平纹理，<span class="math inline">\(G_{ii}\)</span> 的值越大就表示图像中水平纹理越多。通过计算各种特征之间的 <span class="math inline">\(G_{ij}\)</span> 即这些特征同时出现的可能性，就可以衡量一张图像的风格。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(A)</span>:</span></span><br><span class="line">    GA = tf.matmul(A, tf.transpose(A))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> GA</span><br></pre></td></tr></table></figure><h5 id="风格代价">风格代价</h5><p>我们的目标是最小化风格图像 S 和生成图像 G 之间的 Gram 矩阵的距离，这里只考虑第 <span class="math inline">\(l\)</span> 个隐藏层的风格（考虑的层数越多，风格越相似），其对应的风格代价计算公式如下所示： <span class="math display">\[ J_{style}^{[l]}(S,G)=\frac{1}{4\times {n_C}^2\times (n_H\times n_W)^2}\sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}\_{ij}-G^{(G)}\_{ij})^2 \]</span> 计算过程分为四个步骤：</p><ol type="1"><li>获取风格矩阵的维度</li><li>展开 <span class="math inline">\(a_S\)</span> 和 <span class="math inline">\(a_G\)</span></li><li>计算 S 和 G 的风格矩阵</li><li>计算风格代价</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span><span class="params">(a_S, a_G)</span>:</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape the images to have them of shape (n_H*n_W, n_C) (≈2 lines)</span></span><br><span class="line">    a_S = tf.transpose(tf.reshape(a_S, [n_H*n_W, n_C]))</span><br><span class="line">    a_G = tf.transpose(tf.reshape(a_G, [n_H*n_W, n_C]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing gram_matrices for both images S and G (≈2 lines)</span></span><br><span class="line">    GS = gram_matrix(a_S)</span><br><span class="line">    GG = gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing the loss (≈1 line)</span></span><br><span class="line">    J_style_layer = (<span class="number">1.</span>/(<span class="number">4</span> * n_C**<span class="number">2</span> * (n_H*n_W)**<span class="number">2</span>)) * tf.reduce_sum(tf.pow((GS - GG), <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_style_layer</span><br></pre></td></tr></table></figure><h5 id="风格权值">风格权值</h5><p>综合考虑每个隐藏层的风格会令实验效果更好，因此对每个隐藏层的风格代价一个权值，进行加权平均：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">STYLE_LAYERS = [</span><br><span class="line">    (<span class="string">'conv1_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv2_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv3_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv4_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv5_1'</span>, <span class="number">0.2</span>)]</span><br></pre></td></tr></table></figure><p>整体的风格代价函数为： <span class="math display">\[ J_{style}(S,G) = \sum_{l} \lambda^{[l]} J^{[l]}_{style}(S,G) \]</span> 其中 <span class="math inline">\(\lambda^{[l]}\)</span> 就是给定的 <code>STYLE_LAYERS[l]</code>。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_style_cost</span><span class="params">(model, STYLE_LAYERS)</span>:</span></span><br><span class="line">    <span class="comment"># initialize the overall style cost</span></span><br><span class="line">    J_style = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> layer_name, coeff <span class="keyword">in</span> STYLE_LAYERS:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select the output tensor of the currently selected layer</span></span><br><span class="line">        out = model[layer_name]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out</span></span><br><span class="line">        a_S = sess.run(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name] and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that when we run the session, this will be the activations drawn from the appropriate layer, with G as input.</span></span><br><span class="line">        a_G = out</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute style_cost for the current layer</span></span><br><span class="line">        J_style_layer = compute_layer_style_cost(a_S, a_G)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add coeff * J_style_layer of this layer to overall style cost</span></span><br><span class="line">        J_style += coeff * J_style_layer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> J_style</span><br></pre></td></tr></table></figure><p>在循环中 <code>a_S</code> 和 <code>a_G</code> 都是选择同一隐藏层的激活值，但是前者使用了 <code>sess.run</code> 而后者没有。因此后续需要将生成的图像 G 作为输入，然后运行对话才可以得到具体 <code>a_G</code> 的值。</p><h3 id="解决优化问题">解决优化问题</h3><p>最后需要结合上述代码，实现风格迁移。实验分为以下几个步骤：</p><ol type="1"><li><p>创建交互式会话</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reset the graph</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="comment"># Start interactive session</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure></li><li><p>载入 VGG19 模型、内容图像和风格图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = load_vgg_model(<span class="string">"pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br><span class="line"></span><br><span class="line">content_image = scipy.misc.imread(<span class="string">"images/louvre_small.jpg"</span>)</span><br><span class="line">content_image = reshape_and_normalize_image(content_image)</span><br><span class="line">style_image = scipy.misc.imread(<span class="string">"images/monet.jpg"</span>)</span><br><span class="line">style_image = reshape_and_normalize_image(style_image)</span><br></pre></td></tr></table></figure></li><li><p>随机初始化生成图像（通过对内容图像添加大量噪声而不是完全随机，可以让生成的图像内容快速匹配）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">generated_image = generate_noise_image(content_image)</span><br><span class="line">imshow(generated_image[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></li><li><p>构建 Tensorflow 图模型</p><ul><li><p>通过 VGG19 模型运行内容图像，计算内容代价</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assign the content image to be the input of the VGG model.  </span></span><br><span class="line">sess.run(model[<span class="string">'input'</span>].assign(content_image))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select the output tensor of layer conv4_2</span></span><br><span class="line">out = model[<span class="string">'conv4_2'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a_C to be the hidden layer activation from the layer we have selected</span></span><br><span class="line">a_C = sess.run(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that when we run the session, this will be the activations drawn from the appropriate layer, with G as input.</span></span><br><span class="line">a_G = out</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the content cost</span></span><br><span class="line">J_content = compute_content_cost(a_C, a_G)</span><br></pre></td></tr></table></figure></li><li><p>通过 VGG19 模型运行风格图像，计算风格代价</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assign the input of the model to be the "style" image </span></span><br><span class="line">sess.run(model[<span class="string">'input'</span>].assign(style_image))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the style cost</span></span><br><span class="line">J_style = compute_style_cost(model, STYLE_LAYERS)</span><br></pre></td></tr></table></figure></li><li><p>计算整体代价、定义优化器和学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">J = total_cost(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span><br><span class="line">optimizer = tf.train.AdamOptimizer(<span class="number">2.0</span>)</span><br><span class="line">train_step = optimizer.minimize(J)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>初始化图模型，迭代输入<strong>生成的图像</strong>，更新生成的图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span><span class="params">(sess, input_image, num_iterations = <span class="number">200</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize global variables (you need to run the session on the initializer)</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the noisy input image (initial generated image) through the model. Use assign().</span></span><br><span class="line">    sess.run(model[<span class="string">'input'</span>].assign(input_image))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        <span class="comment"># Run the session on the train_step to minimize the total cost</span></span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the generated image by running the session on the current model['input']</span></span><br><span class="line">        generated_image = sess.run(model[<span class="string">'input'</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print every 20 iteration.</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            Jt, Jc, Js = sess.run([J, J_content, J_style])</span><br><span class="line">            print(<span class="string">"Iteration "</span> + str(i) + <span class="string">" :"</span>)</span><br><span class="line">            print(<span class="string">"total cost = "</span> + str(Jt))</span><br><span class="line">            print(<span class="string">"content cost = "</span> + str(Jc))</span><br><span class="line">            print(<span class="string">"style cost = "</span> + str(Js))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save current generated image in the "/output" directory</span></span><br><span class="line">            save_image(<span class="string">"output/"</span> + str(i) + <span class="string">".png"</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save last generated image</span></span><br><span class="line">    save_image(<span class="string">'output/generated_image.jpg'</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_image</span><br></pre></td></tr></table></figure></li></ol><p>运行模型 <code>model_nn(sess, generated_image)</code> 后即可得到保存在输出文件夹中的生成图像，实验为了节省时间直接设定好了所有超参数，例如风格权值 <code>STYLE_LAYERS</code>、迭代的次数和 <span class="math inline">\((\alpha, \beta)\)</span>。</p><h2 id="总结">总结</h2><p>深度学习具有各种各样的模型，这次实验是首次对图像的像素值进行更新优化而不是权值，由于不需要手动实现反向传播所以不算很难，但是还需要多了解 Tensorflow 的文档。收获比较大的就是将直观感觉用数学语言描述出来，即如何表示一张图像的内容和风格！然后才能设计合适的代价函数，让模型学习出我们想要的内容。</p><h2 id="参考文献">参考文献</h2><ol type="1"><li>吴恩达. DeepLearning.</li><li>Matthew D Zeiler, Rob Fergus, (2013). Visualizing and Understanding Convolutional Networks(https://arxiv.org/abs/1311.2901)</li><li>Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015). A Neural Algorithm of Artistic Style (<a href="https://arxiv.org/abs/1508.06576" class="uri" target="_blank" rel="noopener">https://arxiv.org/abs/1508.06576</a>)</li></ol></div><div class="reward-container"><div>疏影横斜水清浅，暗香浮动月黄昏</div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.png" alt="Randy Peng 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.png" alt="Randy Peng 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Randy Peng</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://pengzhendong.cn/2019/01/14/neural-style-transfer/" title="风格迁移">https://pengzhendong.cn/2019/01/14/neural-style-transfer/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><center><br><div class="addthis_inline_share_toolbox"><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5aa9d6309315fb5e" async="async"></script></div></center><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2019/01/12/face-recognition/" rel="prev" title="人脸识别"><i class="fa fa-chevron-left"></i> 人脸识别</a></div><div class="post-nav-item"> <a href="/2019/04/08/java-basis/" rel="next" title="Java 基础总结">Java 基础总结<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#可视化"><span class="nav-number">2.</span> <span class="nav-text">可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#风格迁移"><span class="nav-number">3.</span> <span class="nav-text">风格迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#迁移学习"><span class="nav-number">3.1.</span> <span class="nav-text">迁移学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数"><span class="nav-number">3.2.</span> <span class="nav-text">代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#内容代价函数"><span class="nav-number">3.2.1.</span> <span class="nav-text">内容代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#风格代价函数"><span class="nav-number">3.2.2.</span> <span class="nav-text">风格代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#gram-矩阵"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">Gram 矩阵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#风格代价"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">风格代价</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#风格权值"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">风格权值</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决优化问题"><span class="nav-number">3.3.</span> <span class="nav-text">解决优化问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">5.</span> <span class="nav-text">参考文献</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Randy Peng" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">Randy Peng</p><div class="site-description" itemprop="description">路漫漫其修远兮 吾将上下而求索</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/pengzhendong" title="GitHub → https://github.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://twitter.com/pengzhendong" title="Twitter → https://twitter.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i> Twitter</a></span><span class="links-of-author-item"><a href="mailto:275331498@qq.com" title="E-Mail → mailto:275331498@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="https://www.facebook.com/pengzhendong" title="FaceBook → https://www.facebook.com/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i> FaceBook</a></span><span class="links-of-author-item"><a href="https://t.me/pengzhendong" title="Telegram → https://t.me/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-telegram fa-fw"></i> Telegram</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/pengzhendong" title="知乎 → https://www.zhihu.com/people/pengzhendong" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://weibo.com/qq275331498" title="微博 → https://weibo.com/qq275331498" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i> 微博</a></span><span class="links-of-author-item"><a href="/about" target="_self" title="关于 → /about" target="_self"><i class="fa fa-user fa-fw"></i> 关于</a></span></div><hr style="margin-top:20px;margin-bottom:20px"> <img src="/images/wechat.png"></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 – <span itemprop="copyrightYear">2021</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Randy Peng</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span title="站点总字数">258k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span title="站点阅读时长">3:55</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"YHMwvrTgcfDjOXmiGY3jQ2r5-gzGzoHsz","app_key":"JRfKfM8mRPgxMB9GOSAnix9W","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var o,n,e=document.getElementsByTagName("link");if(0<e.length)for(i=0;i<e.length;i++)"canonical"==e[i].rel.toLowerCase()&&e[i].href&&(o=e[i].href);n=o?o.split(":")[0]:window.location.protocol.split(":")[0],o||(o=window.location.href),function(){var e=o,i=document.referrer;if(!/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(e)){var t="https"===String(n).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";i?(t+="?r="+encodeURIComponent(document.referrer),e&&(t+="&l="+e)):e&&(t+="?l="+e),(new Image).src=t}}(window)}()</script><script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/algolia-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script></body></html>